\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{2010}{-}% support older LaTeX versions
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{tikz-qtree}
%\usepackage{subfigure}
\usepackage{graphicx}
\usepackage[numbers]{natbib}

% paper1
\usepackage{amsthm,amssymb,amsfonts}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{algpseudocode}
\usepackage{parskip}
\usepackage{subcaption}
%\usepackage{subfig}

%% paragraph
%\usepackage{titlesec}
\setcounter{secnumdepth}{4}


\graphicspath{
	{./figs/}
}

\hypersetup{letterpaper,bookmarksopen,bookmarksnumbered,
pdfpagemode=UseOutlines,
colorlinks=true,
linkcolor=blue,
anchorcolor=blue,
citecolor=blue,
filecolor=blue,
menucolor=blue,
urlcolor=blue
}

\DeclareMathOperator*{\argmax}{arg\,max}

%opening
\title{Provably Constant-time Motion Planning}
\author{Fahad Islam\\
The Robotics Institute\\
Carnegie Mellon University\\
\texttt{fi@andrew.cmu.edu}}

\input{macros1.tex}
\input{macros2.tex}
\begin{document}

\maketitle

\begin{center}
\Large{Thesis Proposal}
\vspace{40mm}

\large{
\textbf{Thesis Committee:}

Maxim Likhachev (Chair)\\
Chris Atkeson\\
Oliver Kroemer\\
Siddhartha Srinivasa (UW)\\
Oren Salzman (Technion)\\
}

\vspace{10mm}
\textit{Submitted in partial fulfillment of the requirements
for the degree of Doctor of Philosophy in Robotics.}

\vspace{10mm}
Copyright~$\copyright$ 2020 Fahad Islam. All rights reserved.
\end{center}
\newpage

\begin{abstract}
In manufacturing and warehouse scenarios, robots often perform recurring manipulation tasks in structured environments. Fast and reliable motion planning is one of the key elements that ensure efficient operations in such environments. A very common example scenario is of manipulators working at conveyor belts, where they have limited time to pick moving objects and if the planner exceeds a certain time threshold, they would fail to pick the objects up. Similar scenarios are encountered in automated assembly lines. Such time-critical applications spur the need for planners which are \emph{guaranteed} to be fast. To this end we introduce and formalize the concept of Constant-time Motion Planning (CTMP); namely, the ability to provably guarantee to generate a motion plan within a (small) constant time. We then develop several algorithms that fall into the class of CTMP algorithms.

Specifically, up to now we have developed constant-time motion planning algorithms for two domains; (1) Manipulation for repetitive tasks in static environments (2) Manipulation for the task of picking up moving objects (of known models) off a conveyor belt. For the latter, the robot typically perceives a rough pose estimate viewing from a distance, but it must start moving early on (relying on that rough estimate) to be able to reach the object in time and then adjust its motion in real time as it gets improved estimates.
Our key insight is that since these domains are fairly repetitive, the space in which the robot operates is a very small subset of its configuration space, which allows us to preprocess it exhaustively. The preprocessing step generates a representative set of paths that can be used by search at query time in a way that assures small constant-time planning.
%
For the former domain, we evaluate our algorithm for a mail sorting task in simulation on PR2 and also tested the algorithm on a real truck-unloading robot. For the latter domain, we perform real robot experiments on PR2 working at a conveyor belt.

For the remainder of this thesis we propose the following work. First, we propose to further study and formalise what CTMP implies in practice and what underlying assumptions it entails. Second, we propose to extend our  planning framework to semi-static environments (containting some movable obstacles). Third, we aim to boost the capability of the conveyor pick-up task by having multiple robot arms simultaneously picking up objects, while still maintaining strong theoretical guarantees on the planning side.

%For the remainder of this thesis we propose the following work. First, we propose to further study and formally define what constant-time planning implies in practice and what underlying assumptions it entails. Second, we propose to extend our  planning framework to semi-static environments (containting some movable obstacles). Third, we aim to boost the capability of the conveyor pick-up task by having multiple robot arms simultaneously picking up objects, while still maintaining strong theoretical guarantees on the planning side.
%This introduces new algorithmic challenges including decision making about which object to assign to which arm and motion synchronization between the arms.
\end{abstract}
\newpage

\tableofcontents
\newpage

\section{Introduction}
\subsection{Motivation}
In industrial settings such as manufacturing and warehouse environments, the robots typically operate in structured environments and perform repetitive tasks. Despite significant advancements in the field of motion planning in the past several decades, a large percentage of warehousing and manufacturing industry still uses robots that run hardcoded routines to perform very specific tasks. The lack of penetration of modern motion planning algorithms at scale can be attributed to the fact that the industry needs systems which are \emph{guaranteed} to be fast and reliable, even at the cost of flexibility that these systems could potentially provide.

In warehouses, robots are widely deployed at fast moving conveyor belts to perform repetitive pick and place or sorting tasks, which gives them a very short time budget to plan their motion. Failing to plan within that time, a robot would skip objects and affect the throughput of the system. Similarly in assembly lines where multiple robots operate in their respective work stations, an overhead caused by the motion planner at one station can slow down the entire chain. This thesis focuses on such time critical applications and introduces the notion of (small) Constant-time Motion Planning (CTMP). CTMP implies generating motion plans in provably-bounded short planning times.

This thesis is motivated by the following key observations from the aforementioned domains; (1) The tasks are highly repetitious (2) The  environments are fairly structured. (1) gives us an insight that the operational space of the robot is a very small as compared to its full C-space and (2) implies that the environment model is known in advance. These two aspects allow us to fully preprocess the part of the C-space that the robot operates in while accounting for the known objects that exists in its surroundings.

A naive way of providing constant-time guarantees would be to precompute paths for all possible start and goal states that the planner could be queried for and use a simple hash lookup at query time (assuming the lookup time is constant). However, as the set of start or goal states increase, this approach quickly becomes intractable, memory and precomputation time-wise. Our method provides a compression scheme and precomputes only a small set of representative paths which can still ``cover" the robot's operational space. Namely, using this set of paths, at query time, our method can solve any query within the robot's operational space in constant time.

\subsection{Approach}
\label{sec:approach}
So far, we have developed CTMP algorithms for two domains.
\subsubsection{Manipulation for repetitive tasks in static environments}
In this work, we consider the specific case where the start state is fixed and there exists a ``goal region" which contains all possible goals. Consider for example, a typical mailroom scenario where the robot has to pick up mail from a fixed location and sort them in cubby shelves. The start corresponds to the pickup location for the packages and the goal region corresponds to all possible goal poses within the cubby shelves. Note that the cubbies reduce the robot's operational space drastically which makes the preprocessing tractable for our method.

Our key insight is that given an ``attractor state'' in the goal region, typically there is a large region of states around it for which a greedy search (a search following an attractive potential function) towards the attractor state is collision free. We show that such so called ``attractor regions" can be efficiently computed by using dynamic programming.
Importantly, the runtime-complexity of such a greedy search is bounded and there is no need to perform computationally-complex collision-detection operations. 
This insight allows us to generate in an offline phase a small set of these attractor regions together with a path between each attractor state and the start, ensuring that together these attractor regions completely cover the full goal region.
In the query phase, a path is generated by performing a greedy search from the goal to an attractor state (the one which contains the goal) followed by the precomputed path from the start. The approach is described in detail in chapter~\ref{chap:icaps}.

\subsubsection{Manipulation for picking up moving objects off a conveyor belt}
In this domain, the task for the robot to reliably and safely pickup objects coming on a conveyor belt. We will refer to this problem as ``conveyor-pickup task". We assume that the geometric models of the target objects are known in advance. The success of manipulation tasks relies heavily on the accuracy of the perception system which often is noisy, especially if the target objects are perceived from a distance. For fast moving conveyor belts, the robot cannot wait for a perfect estimate before it starts execution. In order to be able to reach the object in time it must start moving early on (relying on the initial noisy estimates) and adjust its motion on-the-fly in response to the pose updates from perception. We developed an approach that meets these requirements by providing provable constant-time planning and replanning guarantees.

Again, with the same insight, in the first step we precompute a small set of paths from a fixed start (say drop off location) that can cover the goal region which in this domain is defined in the space of object poses. The goal can be any arbitrary object pose~$(x,y,yaw)$ of the objects. In the second step, we uniformly discretize these paths in time to get a set of states that we call ``replannable'' states. To handle pose updates online, we may need to replan from any of these states to all the goals in the goal region. The algorithm then goes back to the first step treating all the replannable states as new starts and this recursive process continues until all replannable states are taken care of. While the approach has exponential complexity in the number of timesteps from the start to the goal, we drastically save on computation and memory by reusing the first set of paths (from the drop off location to the goal region). Our algorithm guarantees that for any replannable state and a goal, a plan is be computed in constant time. Experimentally we observe that the robot fails most of the time without replanning i.e. in case it relies on the first estimate or if it waits for an accurate estimate before starting planning. The details of the framework is provided in chapter~\ref{sec:rss}.

\subsection{Expected Contribution}
We expect to make the following contributions in this thesis.
\begin{itemize}
	\item Introduce and formalize the concept of Constant-time Motion Planning (CTMP)
	\item Develop CTMP algorithm for domains with repetitive manipulation tasks in static environments
	\item Develop CTMP algorithm for the conveyor-pickup task
	\item Extend CTMP framework to semi-static environments
	\item Enhance the conveyor pick task using multiple robot arms, simultaneously picking up multiple objects
\end{itemize}

\newpage
\section{Background}
\subsection{Configuration Space and Motion Planning}
Motion planning algorithms operate in the state space often also referred to as the \emph{configuration space} (C-space) or~$\calC$~\cite{lozano1990spatial}. In this space a state or a configuration of the robot can be uniquely represented as a point which makes it convenient for the motion planners to operate. It can be the~($x,y$) position of the robot or~($x,y,yaw$) if the orientation also matters. For a robot arm the configuration can be composed of all the joint positions. If planning with dynamics, the velocities and times may also be added to the configuration. Similarly higher order derivatives can also be added as per the requirements of the domain. The number of variables in the configuration represents the \emph{dimension} of the C-space. For example, for a 7DoF robot arm, for only considering the joint positions, the dimension of the C-space is seven. The free space~$\Cfree$ contains all the configurations which are valid with respect to some state validity criterion such as violation of obstacle collision constraints or kinematic constraints etc. The obstacle space~$\Cobs$ contains all the configurations which are invalid and so~$\Cobs = \calC \backslash \Cfree$.

For the general motion planning problem we define a start configuration~$\Sstart$ and a goal set~$\Sgoal$. The goal can be under-specified as the pose of the robot end-effector or the pose of the target object. For instance, given a target object that the robot needs to grasp, the goal can be the grasp pose (i.e. position and orientation of the end-effector) or it can be even more under-specified as the pose of the object, giving the flexiblity to select different grasp poses. The motion planning problem is defined as finding a continuous path~$\tau : [0:1] \rightarrow \Cfree$ s.t. $\tau(0) = \Sstart$ and $\tau(1) \in \Sgoal $. Typically in practice most motion planning algorithms find a path of the form~$[s_0, s_1, s_2,...,s_n] \in \Cfree$ s.t. $s_0 = \Sstart$ and $s_n \in \Sgoal$ assuming that the consecutive states i.e $s_i, s_{i+1}$ can be connected via some simple interpolation scheme.

Computing~$\Cfree$ is extremely hard, specially in higher dimensions. Instead of doing that, most motion planners generally sample in~$\Cfree$ and use a collision checker to identify the validity of a configuration. These planners typically sample states in~$\Cfree$ using rejection sampling, connect those samples to construct a graph (or a tree) and then use graph search to find the path from a start to goal. These algorithms are called sampling-based algorithms. A motion planner is referred to as \emph{complete} if it guarantees to find a path if one exists, and otherwise returns failure. Sampling-based planners are \emph{probablisitically complete}, meaning that in the limit of the number of samples they are guaranteed to find a solution if one exists.

\subsection{C-space representation for Search-based Planning}
\label{sec:sbp}
Search-based planning methods discretize the C-space into cells. This discretization generally relies on a grid-based or lattice-based structure. A cell is the smallest unit of this discrete space and represents a small volume of C-space states that lie within it. A representative state within a cell, commonly its geometric center is picked to denote a vertex for that cell. All the vertices~$\calV$ connect to their neighboring vertices through edges~$\calE$ to construct a graph~$\calG = (\calV, \calE)$. These edges may have associated costs. Only those vertices and edges are added to~$\calG$ that are valid. The motion planning problem is thus turned into a graph search problem which can be solved using any graph search algorithim. The search is done on an implicit graph as opposed to explicit graph. Namely, instead of precomputing the entire graph which is infeasible for large domains, the graph is constructed and evaluated on-the-fly as the search progresses.

The C-space discretization is done at a certain resolution which is a domain dependent parameter. The resolution determines how course or fine the discretization is. Search-based algorithms are attributed as \emph{resolution complete} which means that they will return a path if one exists for the given resolution. Note that it is possible that a solution exists in the C-space and the search-based planner fails to find it because the resolution is not fine enough for that problem.

\subsection{Computational Complexity in Motion Planning}
The general motion planning problem was shown by Reif to be PSPACE-hard~\cite{reif1979complexity}. Later it was shown to be PSPACE-complete by Canny~\cite{canny1988complexity}. Their analysis was based on finding the \emph{exact} solutions. The class of algorithms that go for finding the exact solutions are called combinatorial algorithms or \emph{exact} algorithms~\cite{lavalle2006planning}. These algorithms do not rely on any approximation of the C-space and operate in the continuous C-space. They are also complete with respect to the continuous C-space which is a stronger notion of completeness compared to sampling-based planners or search-based planners which are probablisitically complete and resolution complete respectively~\citep{lavalle2006planning}.

The dimension~$d$ of the C-space is the most crucial element in determining the complexity of the motion planning problem. It has been shown that for the general motion planning problem the computational complexity grows exponentially with~$d$~\cite{chazelle1991singly}.

The exact motion planning algorithms do not scale to more complex problems as they make certain limiting assumptions about the geometry of the robot and the obstacles. In contrast, the modern approaches use approximations of the general motion planning problem by relaxing the completeness guarantees. For example the sampling-based algorithms approximate the problem structure via random sampling in the C-space and so they relax the completeness guarantee to probabilistic completeness. Search-based algorithms approximate the problem via space discretization with a certain resolution, hence they are resolution complete.
Since the exact methods are no more used in modern days, the computational complexity analysis for the general motion planning problem is of less relevance now and the analysis for the approximate methods is based on the underlying approximation. For instance, for sampling-based methods the complexity is analysed in the number of samples and for search-based methods it is analysed in the size of the grid or the lattice structure.


\newpage
\section{Related Work}
\label{sec:rel}
Our approach leverages preprocessing in order to speed up online planning. In this respect, first we cover some popoluar preprocessing-based approaches. Second, we review techniques that reuse previously generated plans or experiences to accelerate planning. Our method also bears resemblence with these methods as it also uses paths that are generated in the preprocessing phase in the search. Third, we cover real-time heuristic search and moving target search methods as they also provide constant-time guarantees, the key proporty that our approach also has. We then review some work from the controls communitee that explored similar high level ideas that our method builds upon. Finally, we cover some of the motion planning approaches particalurly used for the conveyor-pickup task.

\subsection{Preprocessing-based Methods}
Preprocessing-based motion planners often prove beneficial for real-time planning. They analyse the configuration space offline to generate some auxiliary information that can be used online to speed up planning. 

The most reknowned example is the Probablistic Roadmap Method (PRM)~\cite{kavraki1996probabilistic}. They have a preprocessing and a query phase. In the preprocessing phase a roadmap is constructed in $\Cfree$ by randomly sampling valid states in the C-space and connecting them to their neighboring states if the edges making the connections are valid. In the query phase the start and goal states are connected to their neighboring states in the roadmap (if valid connections are possible) and then any search algorithm like A* search is used to find the path. PRM methods are categorized as multi-query methods meaning that once the roadmap is constructed it can be used to answer multiple queries for the same environment. Query times can be significantly sped up by further preprocessing the roadmaps using landmarks~\cite{paden2017landmark}. Recently,the repetition roadmap~\cite{LA18} was suggested as a way to extend the PRM for the case of multiple highly-similar scenarios. Some of these methods have been integrated with sparse motion-planning roadmaps (see e.g.,~\cite{SSAH14,DB14}) to reduce the memory footprint of the algorithm. All these methods only provide probablistic completeness guarantees.

Our work bares resemblance to previous work on 
subgoal graphs~\cite{UK17,UK18}.
Subgoal graphs is a search-based method that preprocesses the entire C-space to generate a sparse overlay graph comprised of vertices called sub-goals. The pair of sub-goals connected by an edge are \emph{reachable} and thus searching on this overlay graph is significantly faster than searching on the original graph. The method assures that the path found on this subgoal graph has a path on the original graph and can be quickly computed at query time. As this method requires preprocessing the full C-space it is expensive and only is applicable to lower dimensional planning problems.

While all these preprocessing-based approaches speedup planning times compared to planning from scratch by using the preprocessed information, they do not provide constant-time bounds on the planning times.


\subsection{Motion Planning with Reuse}
An alternative approach to address our problem is to precompute a set of complete paths into a library and given a query, attempt to match complete paths from the library to the new query~\cite{berenson2012robot,jetchev2013fast}.
Using paths from previous search episodes (also known as using experience) has also been an active line of work~\cite{PCCL12,PDCL13,BAG12,CSMOC15}. E-graphs method~\cite{PCCL12} computes a heuristic function to guide the search in such a way as to reuse search efforts from previously generated paths. The Lightning framework~\cite{BAG12} runs two modules in parallel, a module that runs the planner from scratch and a module that retrieves and repairs path stored in a library. The Thunder framework~\cite{CSMOC15} also leverages previous experiences and the experiences are generated using probabilistic sampling. The experiences are stored as sparse roadmap spanner~(SPARS). 

These methods also provide significant speedups in planning times, yet they do not provide constant-time planning guarantees which our method aims for.

\subsection{Real-time Motion Planning}
\label{rel:real}
In practice, the only class of algorithms that provide constant-time planning guarantees is of real-time algorithms~\cite{KL06,KS09,K90,bjornsson2009tba}.
To provide guarantees on planning time the search only looks at a finite horizon, generating a partial plan, and interleaves planning and execution. In planning cycles, the robot may re-expand states multiple times. In addition, during execution, the robot is likely to visit the same states multiple times before it reaches the goal resulting in highly suboptimal behavior specially in higher dimensional problems. These algorithms run repeated A* searches and update the heuristic values in the local search spaces while maintaining the admissibility of the heuristics. The heuristis get more informed as the search is run repeatedly. By doing so they provide completeness guarantees, meaning that the robot will eventually arrive at the goal.

As opposed to these finite horizon planning algorithms, our methods either plan for indefinite horizons i.e. all the way from the start to the goal while providing constant-time planning guarantees or do limited horizon planning (interleaving planning and execution) while assuring that the planner never re-expands a state nor the robot never revisits the same state during execution.


\subsection{Global Control using Local Potential Functions}
Finally, our notion of ``attractor regions" is similar (in spirit) to control-based methods that  ensure safe operation over local regions of the free configuration space~\cite{CRC03,CCR06,tedrake2010lqr}. Metaphorically they name these regions as ``funnels" that collapse a large set of input conditions to a single policy.
These regions are then used within a high-level motion planner to compute collision-free paths. For instance, LQR-trees~\cite{tedrake2010lqr} computes local stability regions to build a sparse tree of LQR-stabilized trajectories. The union of these local regions covers the entire state space ensuring that all initial conditions that are capable of reaching the goal will reach the goal.


\subsection{Online Replanning: Moving Target Search}
The conveyor-pickup task~(chapter~\ref{sec:rss}) can be modelled as a Moving Target Search problem (MTS) as the robot has to plan a motion to reach a moving target, that is the object coming on a conveyor belt. MTS is a widely-studied topic in the graph search-based planning literature~\cite{ishida1991moving,ishida1995moving,koenig2007speeding,sun2010moving}. 
These approaches interleave planning and execution and incrementally update the heuristic values of the state space to improve the distance estimates to the moving target. Unfortunately, in high-dimensional planning problems, this process is computationally expensive which is why these approaches are typically used for two-dimensional grid problem such as those encountered in video games.

\subsection{Motion Planning for Conveyor-pickup Task}
Existing work on picking moving objects has focused on different aspects of the problem ranging from closed-loop controls to object perception and pose estimation, motion planning and others~\cite{allen1993automated, han2019toward, stogl2017tracking, zhang2018gilbreth}. 
%
Here, we focus on motion-planning related work. Graph-searched based approaches have been used for the motion-planning problem of the conveyor-pickup task~\cite{cowley2013perception, menon2014motion}. The former uses a kinodynamic motion planner to smoothly pick up moving objects i.e., without an impactful contact. A heuristic search-based motion planner that plans with dynamics and could generate optimal trajectories with respect to the time of execution was used. While this planner provides strong optimality guarantees, it is not real-time and thus cannot be used online.
%
The latter work was demonstrated in the real world showing real-time planning capability. While the planner is fast, it does  pure kinematic planning and does not require collision checking with the target object. The approach plans to a pregrasp pose and relies on Cartesian-space controllers to perform the pick up. The usage of the Cartesian controller limits the types of objects that the robot can grasp.

\newpage
\section{Constant-time Motion Planning (CTMP)}
\label{sec:ctmp}
In this section we introduce and formalise Constant-time Motion Planning (CTMP). CTMP implies providing constant-time planning guarantees. Namely we aim to provide bounds on the planning times independently of the underlying approximation of the original planning problem. In this section we detail the properties and introduce a new notion of completeness for this new class of motion planning algorithms. 
%In our problem setting our input to the algorithm is the full configuration space of the robot~$\calC$. The size of~$\calC$ by definition is unbounded since it is a continuous space. We will also consider the case in section~\ref{sec:conveyor} where we consider a discretized search space as highlighted in section~\ref{sec:sbp}. In that case the input is finite is defined by the size of the underlying implicit graph~$\calG$. 

\subsection{Background: Constant-time Complexity}
To prove that the algorithm has a constant-time complexity, we need to show that its computation time is bounded and the bound is independent of the input size. The input size depends on the abstraction used for motion planning (e.g. a roadmap or a grid etc.). More formally an algorithm with input size~$n$ has a constant-time complexity if its processing time~$T(n)$ is bounded by a value that does not depend on~$n$.
Note that not the running time itself but its upper bound has to be bounded independently of~$n$. In Big O notation, a constant-time algorithm has an asymptotic complexity denoted as~$O(1)$.

In our work, besides assuring~$O(1)$ complexity, we also aim to achieve real-time behavior. In other words, we aim that~$T(n)$ of the algorithm is small.

\subsection{Overview of CTMP Approach}
This thesis focusses on environments that involve repetitive tasks. Two instances of such tasks were briefly described in section~\ref{sec:approach}. Specifically we consider the problem setting where we have a fixed start state(s)~$\Sstart$ and corresponding goal region(s)~$G$ which are defined specific to the task. $G$ could be defined as a region of fully or under-specified goals depending on the task. We descritize~$G$ to get a finite set of goals. In the query phase, the algorithm gets a query ($\sStart, g \in G$) and needs to compute a collision free path in constant time.


In this section we provide sketches of a straw man approach and our approach for constant-time motion planning for repetitive tasks that will serve as the conceptual foundation for the following chapters.

\subsubsection{A Straw Man Constant-time Algorithm}
Given ~$\sStart$ and ~$G$ with a finite set of goals, the naive approach would be to precompute and store paths to each $g \in G$ in a preprocessing phase, construct a hash table mapping goals to paths and then use it to answer any query goal~$g \in G$ in the query phase.
With perfect hashing~\cite{czech1997perfect}, the hash lookup is a constant-time operation and hence the algorithm will have a constant-time complexity. However, this approach becomes intractable when the number of queries becomes large. This is because the memory requirement or the precomputation time becomes very large for practical computational resources.

\subsubsection{Proposed Approach}
The naive approach puts all the computation burden on the proprocessing phase and requires minimum computation time but a large memory during the query phase. On the other end of the spectrum we can just plan from scratch online every time and eliminate the preprocessing stage, shifting all the burden to the query phase. This would require minimal memory but high computational complexity. We propose a middle ground where we can balance the offline-online burden in such a way that we can provide provable (small) constant-time planning guarantees at query time while still requiring a small memory footprint. Instead of having to precompute paths to all the goals in~$G$, we propose compression schemes to precompute and store only a small set of representative paths that can be used in query phase in such a way that assures (small) constant-time planning. In the following two chapters we will theoretically show that our algorithms run in constant-time. Additionally we experimentally show that the planning times are small.

\subsection{CTMP algorithm}
\label{subsec:ctmp_alg}

We now formally define a CTMP algorithm.
%\vspace{2mm}
%\begin{definition}
%An indefinite-horizon CTMP algorithm finds a full path from a start to goal in (small)~$O(1)$ time.
%\end{definition}
%
%\vspace{2mm}
%\begin{definition}
%\label{ctmp:finite}
%A finite-horizon CTMP algorithm requires (small)~$O(1)$ time delibration for each action, while assuring no state re-expansion or revisitation.
%\end{definition}

\vspace{2mm}
\begin{definition}
\label{ctmp:def}
A CTMP algorithm can plan for a fixed-length partial path from the current state within a user-controlled time bound~$\Tbound$ along a path that is guaranteed to have no cycles, under the condition that~$\Tbound > \Tconst$ where~$\Tconst$ is a small time constant whose value is independent of the size and complexity of the motion planning problem.
%A CTMP algorithm requires (small)~$O(1)$ time delibration to plan the next action along a path from a start to goal, which is guaranteed to have no cycles
\end{definition}

%The time budget~$\Tbound$ is a \emph{user-specified} input to the algorithm. It should be greater than a minimum required time~$T_\textrm{const}$ for the CTMP algorithm. However,~$T_\textrm{const}$ is upper bounded by a small value that is independent of the size of the input to the algorithm (hence the term constant-time).

Additionally, it is assured that the complete path, if reconstructed does not contain cycles. It is an important property that distinguishes our approach from the conventional limited horizon planning methods described in Sec.~\ref{rel:real}.

\subsubsection{Time Complexity}
We analyse the CTMP algorithms for the worst-case complexity. To that end we have the following two properties:

\vspace{2mm}
\begin{property}
\label{ctmp:prop1}
A CTMP algorithm requires~$O(1)$ time to plan the next action along a path from $\sStart$ to $g$
\end{property}

\vspace{2mm}
\begin{property}
\label{ctmp:prop2}
A CTMP algorithm requires~$O(l)$ time to find a full path from $\sStart$ to $g$, where~$l$ is the length of the path
\end{property}

Resultantly, we can use CTMP algorithms in two modes; (1) if a robot interleaves planning and execution, (2) if the robot requires full path (e.g. for post-processing) from $\sStart$ to $g$ before execution. For the former, each next action requires constant-time planning. Although the latter requires more delibration before the robot can start executing motion, the computation is only linear in the length of the path. Moreover, the path is guaranteed to have no cycles.

The time bound~$\Tbound$ is a tunable input that can be used to tradeoff query time with the memory footprint of the preprocessed data or the planning lookahead (for the case of interleaving planning and execution). We will examine the instantiations of these tradeoffs in the following chapters.
%
%Note the distinction that we make in Def.~\ref{ctmp:finite} between CTMP and real-time heuristic search algorithms~(section~\ref{rel:real}). Namely, we enforce additional requirements of not re-expanding or re-visiting a state during the planning and execution respectively.

%In the following two chapters we will propose CTMP algorithms that respect at least one of the above two definitions.

\subsubsection{CTMP-Completeness}
The CTMP algorithms make use of an offline motion planner~$\calP$ in the preprocessing phase to compute the representative set of paths that are then used at query time. We introduce a new notion of completeness for CTMP algorithms. We first define some preliminaries. 

\vspace{2mm}
\begin{definition}[Reachability]
\label{def:reachable}
A goal is reachable for a $\Sstart$  if~$~\calP$ can find a path to it within a (large) time bound~$T_\textrm{\calP}$ 
\end{definition}

\vspace{2mm}
\begin{definition}[Coverage]
\label{def:covered}
    A reachable goal $g \in G$ is said to be covered by~$\sStart$ if the CTMP algorithm (in the query phase) can plan from~$\sStart$ to $g$ while satisfying Def.~\ref{ctmp:def}.
\end{definition}

\vspace{2mm}
\begin{definition}[CTMP-Completeness]
\label{def:complete}
    An algorithm is CTMP-complete if it covers all the reachable goals $g \in G$ by $\Sstart$
\end{definition}


Namely, CTMP-Completeness implies that the algorithm is guaranteed to plan to any goal in $G$ while satisfying the CTMP properities, if~$\calP$ can find a path to it within the time~$T_\textrm{\calP}$. It is important to note that~$T_\textrm{const} \ll T_\textrm{\calP}$ (assuming that we provide fairly long preprocessing times).The performance and completeness quality of CTMP algorithms largely depends upon the performance and completeness guarantees of~$\calP$. For instance if all goals in $G$ are reachable using~$\calP$, the CTMP algorithms provide the guarantees for the entire~$G$.

\subsubsection{CTMP for Non-stop Execution}
The concept of non-stop execution is relevant for the mode of interleaving planning and execution. Non-stop execution implies that the robot does not come to rest (to plan for the next partial path) once it starts moving until it reaches the goal.

\vspace{2mm}
\begin{definition}
A CTMP algorithm can be used for non-stop execution under the condition~$\Tconst < \Texec$ where~$\Texec$ is the duration of execution of the partial path, given that the planning and execution are parallelized. 
\end{definition}
%
In the following two chapters, we describe two specific instances of the CTMP algorithms that we developed in this thesis so far.

\newpage
\section{CTMP for Static Environments}
\label{chap:icaps}
\subsection{Overview}
%1. What is the problem?
We consider the problem of planning robot motions for highly-repetitive tasks while ensuring constant query-time complexity. There exists manipulation domains where the planning takes non-trivial amount of time, despite the fact that the scenarios are well-structured. Specifically we consider the settings where the environment does not change, the start and goal in each task are similar, yet not identical to the start and goal in previous tasks.

%2. Why is it relevant?
As a running example, consider the problem of mail-sorting where a robot has to put envelopes into appropriate bins (see Fig.~\ref{fig:PR2}). The newly-inserted envelopes do not present any new clutter, and therefore the domain is really static. This domain is being actively pursued in both industry (see for example~\cite{DBot}) and in academia (e.g.,~\cite{hwang2015lazy}).

%Another well-suited domain for our planner is a manipulator working at a conveyor belt where it has to pick up objects (arbitrarily positioned and oriented) coming on the conveyor (or drop them off onto a conveyor). In such a setting the robot has to deal with one object at a time and other objects do not cause obstructions. As a result, the domain is also static but start/goal configurations may change. An autonomous robot working at a conveyor is also actively being pursued in industry (see again~\cite{DBot}) and in academia (see \cite{cowley2013perception,menon2014motion}).


%3. Why is it hard?
Clearly, every time a task is presented to the robot, it can compute a desired path.
However, this may incur large online planning times that may be unacceptable in many settings.
%
Alternatively, we could attempt to precompute for each start and goal pair a robot path.
However, as the set of possible start and goal locations may be large, caching pre-computed paths for all these queries in advance is unmanageable in high-dimensional configuration spaces\footnote{
A robot configuration is a $d$-dimensional point describing the position of each one of the robot's $d$ joints.
The configuration space of a robot is the $d$-dimensional space of all robot configurations.}.
Thus, we need to balance memory constraints while providing provably fast query times.

%4. What have others done?
As detailed in chapter.~\ref{sec:rel}, there has been intensive work 
for fast online planning~\cite{LA18} and 
for learning from experience in known environments~\cite{PCCL12,PDCL13,berenson2012robot,CSMOC15}.
Similarly, compressing precomputed data structures in the context of motion-planning is a well-studied problem with efficient algorithms~\cite{SSAH14,DB14}.
%5. What's missing?
However, to the extent of our knowledge, there is no approach that can \emph{provably guarantee} that a solution will be found to \emph{any} query in constant-time and using a small memory footprint.

%\vspace{8 mm}

%6. What is our ONE key insight?
In this work, we consider the specific case where the start is fixed. Returning to our running example, this corresponds to having a fixed pickup location above the cart (see Fig.~\ref{fig:PR2}).
%
Our key insight is that given any state $s$, we can efficiently compute a set of states for which a greedy search (to be defined formally in Sec.~\ref{sec:alg}) towards~$s$ is collision free.
Importantly, the runtime-complexity of such a greedy search is bounded and there is no need to perform computationally-complex collision-detection operations. 
This insight allows us to generate in an offline phase a small set of so-called ``attractor vertices'' together with a path between each attractor vertex and the start.
In the query phase, a path is generated by performing a greedy search from the goal to an attractor state followed by the precomputed path from the start.
We describe our approach in Sec.~\ref{sec:alg} and analyze it in Sec.~\ref{sec:analysis1}.


%7. How do we compare against the state of the art?
We evaluate our approach in Sec.~\ref{sec:eval} in simulation on the PR2 robot\footnote{~\url{http://www.willowgarage.com/pages/pr2/overview}} (see Fig.~\ref{fig:PR2}).
We demonstrate a speedup of over tenfold in query time when compared to the PRM algorithm with a memory footprint of less than 8 Mb while guaranteeing a maximal query time of less than 3 milliseconds (on our machine).

%8. What are our contributions?
%9. What are our limitations?

\begin{figure}[tb]
  \centering
    \includegraphics[width=0.6\textwidth]{pr2.png}
    % \vspace{-2mm}
  \caption{
  Motivating scenario---a robot (PR2) picking up objects from shelves and placing them into a bin.
}
    \label{fig:PR2}
% \vspace{-6mm}
\end{figure}

\subsection{Algorithm Framework}
\label{sec:alg}
In this section we describe our algorithmic framework. We start (Sec.~\ref{sec:pdef}) by formally defining our problem and continue (Sec.~\ref{sec:key}) by describing the key idea that enables our approach.
We then proceed (Sec.~\ref{subsec:alg}) to detail our algorithm and conclude with implementation details (Sec.~\ref{subsec:impl}).

\subsubsection{Problem formulation and assumptions}
\label{sec:pdef}
Let $\calC$ be the configuration space of a robot operating in a static environment containing obstacles.
We say that a configuration is valid (invalid) if the robot, placed in that configuration does not (does) collide with obstacles, respectively.
We are given in advance a start configuration~$\Sstart \in \calC$ and some goal region~$G \subset \calC$.
We emphasize that the goal region may contain invalid configurations.
In the query phase we are given multiple queries $(\Sstart, s_{\text{goal}})$ where $s_{\rm goal} \in G$ is a valid configuration (fully defined goal) and for each query, we need to compute a collision-free path connecting $\Sstart$ to $s_{\text{goal}}$.

Coming back to our motivating example of mail sorting---the start configuration would be some predefined configuration above the cart where the robot can pick up the envelopes from and the goal region would comprise of all possible placements of the robot's end effector in the cubbies. The environment is static as the only obstacles in the environment are the shelves and the cart which remain stationary in between queries.

We discretize $\calC$ into a state lattice $\calS$ such that any state~$s \in \calS$ is connected to a set of successors and predecessors via a mapping Succs/Preds: $\calS \rightarrow 2^\calS$.
Define $G_\calS := \calS \cap G$ to be the states that reside in the goal region. Note that although our approach is applicable to general graphs (directed or undirected), to be able to reuse the planned path in the reverse direction (e.g. in our motivating example for the motion from the shelve to the start configuration) the graph needs to be undirected.
We make the following assumptions:

%\begin{enumerate}[label={\textbf{A\arabic*}}]
\vspace{2mm}\begin{assumption} \label{asm:1:1} $G_\calS$ is a relatively small subset of $S$. Namely, it is feasible to exhaustively iterate over all states in $G_\calS$.
However, storing a path from $\Sstart$ to each state in $G_\calS$ is infeasible.
\end{assumption}
  
\vspace{2mm}\begin{assumption} \label{asm:1:2} The planner has access to a heuristic function $h: \calS \times \calS \rightarrow \mathbb{R}$ which can estimate the distance between any two states in $G_\calS$. Moreover, 
 \begin{itemize}
  \item The heuristic function should be \textit{weakly-monotone} with respect to $G_\calS$, meaning that $\forall s_1, s_2  \in G_\calS$ where $s_1 \neq s_2 $, it holds that,
  \begin{center}
    $h(s_1, s_2) \geq \min\limits_{s_1' \in \text{Preds}(s_1)} h(s_1', s_2)$.
  \end{center}

  \item The heuristic function $h$ should induce that the goal region is \emph{convex} with respect to $h$, meaning that $\forall s_1, s_2  \in G_\calS$ where $s_1 \neq s_2 $, it holds that,
  \begin{center}
     $\argmin\limits_{s_1' \in \text{Preds}(s_1)} h(s_1', s_2) \in G_\calS$.
  \end{center}

 \end{itemize}
 Namely, for any distinct pair of states ($s_1, s_2$) in $G_\calS$, at least one of $s_1$'s predecessors has a heuristic value less than or equal to its heuristic value.
 Moreover, the predecessor with minimal heuristic value lies in the goal region.
\end{assumption}

\vspace{2mm}\begin{assumption} \label{asm:1:3} The planner has access to a tie-breaking rule that can be used to define a total order \footnote{A total order is a binary relation on some set which is anti-symmetric, transitive, and a convex relation.} over all states with the same heuristic value.
  % to a given state.
\end{assumption}

These assumptions allow us to establish strong theoretical properties regarding the efficiency of our planner. Namely, that
within a known bounded time, we can compute a collision-free path from $\Sstart$ to any state in $G_\calS$. 


Assumption~\ref{asm:1:2} may seem too restrictive (especially convexity), imposing that the goal region cannot be of arbitrary structure. However, after we detail our algorithm (Sec.~\ref{subsec:alg}) and analyze its theoretical properties (Sec.~\ref{sec:analysis1}), we sketch how we can relax this assumption to be less restrictive.


 
\subsubsection{Key idea}
\label{sec:key}
Our algorithm relies heavily on the notion of a greedy search.  Thus, before we describe of our algorithm, we formally define the terms greedy predecessor and greedy search.

\vspace{2mm}
\begin{definition}
\label{def:greedy-suc}
  Let $s$ be some state and $h(\cdot)$ be some heuristic function.
  A state $s' \in \text{Preds}(s)$ is said to be a \emph{greedy} predecessor of $s$ according to $h$ if it has the minimal $h$-value among all of $s$'s predecessors.
\end{definition}
Note that 
if $h$ is weakly monotone with respect to $G_\calS$
(Assumption~\ref{asm:1:2}) 
and we have some tie-breaking rule
(Assumption~\ref{asm:1:3}), then every state has a greedy predecessor in the $G_\calS$ and it is unique.
In the rest of the text, when we use the term greedy predecessor, we assume that it is unique and in $G_\calS$.

\vspace{2mm}
\begin{definition}
  Given a heuristic function $h(\cdot)$,
  an algorithm is said to be a \emph{greedy search}  with respect to $h$ if for every state it returns its greedy predecessor according to $h$.
\end{definition}
Note that we define the greedy search in terms of the predecessors and not the successors to account for the directionality of the graph. This will become more clear in Sec.~\ref{subsec:alg}.

\textbf{Remark:} Now that we have the notion of greedy search, we can better explain our definition of convexity with respect to~$h$ (Assumption~\ref{asm:1:2});
this assumption ensures that a greedy search between any pair of states lies within the goal region, analogously to the standard notion of a convex region where for every pair of points within the region, every point on the straight line segment that joins the pair of points is also within the region.

Our key insight is to precompute in an offline phase subregions within the goal region where a greedy search to a certain (``attractor'') state is guaranteed to be collision free and use these subregions in the query phase.
Specifically, in the preprocessing phase, $G_\calS$ is decomposed into a finite  set of (possibly overlapping) subregions $\calR$.
Each subregion $R_i \in \calR$ is a hyper-ball defined using a center which we refer to as the ``attractor state''~
\sAttract and a radius $r_i$.
These subregions, which may contain invalid states, are constructed in such a way that the following two properties hold
\begin{enumerate}[label={\textbf{P\arabic*}}]
  \item \label{property:1} For any valid goal state $s_{\text{goal}} \in R_i \cap G_\calS$, a greedy search with respect to $h(s, \sAttract)$ over $\calS$ starting at $\sGoal$ will result in a collision-free path to \sAttract.
  \item \label{property:2} The union of all the subregions completely cover the valid states in $G_\calS$. 
      Namely, $\forall s \in G_\calS~\text{s.t.}~s~\text{is valid}, \exists R \in \calR \ s.t. \ s \in R$.
\end{enumerate}

In the preprocessing stage, we precompute a library of collision-free paths $\calL$ which includes a path from $\Sstart$ to each attractor state. 
In the query phase, given a query~\sGoal, we 
(i)~identify a subregion $R_i$ such that $\sGoal \in R_i$ (using the precomputed radii~$r_i$),
(ii)~run a greedy search towards~\sAttract by greedily choosing at every point the predecessor that minimizes~$h$ and
(iii)~append this path with the precomputed path in $\calL$ to $\Sstart$ to obtain the complete plan.
For a visualization of our algorithm, see Fig.~\ref{fig:approach}.


\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{approach.pdf}
    % \vspace{-2mm}
  \caption{
  Visualization of approach. Subregions are depicted in green, 
  goal region~$G$ is depicted in tiled gray  containing obstacles (red).
  Precomputed paths from $s_{\rm{start}}$ to attractor states are depicted in blue.
 Given a query $(s_{\rm{start}}, s_{\rm{goal}})$, the returned path is given by $\pi_i$ appended with a greedy path ~$\pi_g$ from $s_{\rm{goal}}$ to \sAttract (reversed), which is depicted in purple.
}
    \label{fig:approach}
% \vspace{-6mm}
\end{figure}

\subsubsection{Algorithm}
\label{subsec:alg}
\paragraph{Preprocessing Phase}
The preprocessing phase of our algorithm, detailed in Alg.~\ref{alg:1}, takes as input the start state~$\Sstart$, the goal region~$G_\calS$ and a conventional motion planner $\calP$, and outputs a set of subregions~$\calR$ and the corresponding library of paths~$\calL$ from~\Sstart to each~\sAttract. 

The algorithm covers~$G_\calS$ by iteratively finding a state $s$ not covered\footnote{Here, a state $s$ is said to be covered if there exists some subregion $R \in \calR$ such that $s \in R$.} by any subregion and computing a new subregion centered at $s$.
To ensure that $G_\calS$ is completely covered (Property~\ref{property:2}) we maintain a set~$V$ of valid (collision free) and a set $I$ of invalid (in collision) states called \emph{frontier states} (lines~\ref{alg:1:v} and~\ref{alg:1:i}, respectively).
We also construct subregions centered around invalid states~$\hat{\calR}$ to efficiently store which invalid states have been considered.
We start by initializing~$V$ with some random state in $G_\calS$ and iterate until both~$V$ and~$I$ are empty, which will ensure that $G_\calS$ is indeed covered even if $G_\calS$ is not fully connected.

At every iteration, we pop a state from $V$ (line~\ref{alg:1:pop}), and if there is no subregion covering it, we add it as a new attractor state and compute a path $\pi_i$ from $\Sstart$ (line~\ref{alg:1:path}) using the planner $\calP$.
We then compute the corresponding subregion (line~\ref{alg:1:cr} and Alg.~\ref{alg:2}).

As we will see shortly, computing a subregion corresponds to a Dijkstra-like search centered at the attractor state.
The search terminates with the subregion's radius $r_i$ and a list of frontier states that comprise of the subregion's boundary.
The valid and invalid frontier states are then added to $V$ and $I$, respectively (lines~\ref{alg:1:insert_v} and~\ref{alg:1:insert_i}).


Once $V$ gets empty the algorithm starts to search for states which are valid and yet uncovered by growing subregions around invalid states popped from~$I$ (lines~\ref{alg:1:iv_loop}-\ref{alg:1:iv_region} and detailed in Alg.~\ref{alg:3b}). If a valid and uncovered state is found, it is added to $V$ and the algorithm goes back to computing subregions centered at valid states (lines~\ref{alg:1:x_states}-\ref{alg:1:break}), otherwise if $I$ also gets empty, the algorithm terminates and it is guaranteed that each valid state contained in $G_\calS$ is covered by at least one subregion.

\begin{algorithm}[t]
%\footnotesize
\hspace*{\algorithmicindent} \textbf{Inputs:} $G_\calS$, $\Sstart$, $\calP$
\Comment{goal region, start state and a planner} 

\hspace*{\algorithmicindent} \textbf{Outputs:} 
$\calR, \calL$
%$R_i = (\sAttract, r_i)$, $\pi_i$,
\Comment{subregions and corresponding paths to $\Sstart$}

%\hspace*{\algorithmicindent}
% \textbf{Parameters:} $r_{\text{max}}$   \Comment{maximum radius of a region}
\caption{Goal Region Preprocessing}\label{alg:1}
\begin{algorithmic}[1]
\Procedure{PreprocessRegion}{$G_\calS$}
%\Procedure{PreprocessRegion}{$G, r_{\text{max}}$}
  \State $s \leftarrow$\textsc{ SampleValidState}($G_\calS$)
  \State $V \leftarrow \{ s \}$   \Comment{valid frontier states initialized to random {state}} \label{alg:1:v}
  \State $I$ = $\emptyset$   \Comment{invalid frontier states} \label{alg:1:i}
  \State $ i \leftarrow 0$
       \hspace{2mm} 
       $\calL = \emptyset$
       \hspace{2mm} 
       $\calR = \emptyset$
       \hspace{2mm} 
       $\hat{\calR} = \emptyset$
       
  \vspace{2mm}
    \While {$V$ and $I$ are not empty}
        \While {$V$ is not empty}
          \State $s \leftarrow V.\text{pop}()$ \label{alg:1:pop}
            \If {$\nexists R \in \calR$  s.t. $s \in R$ }  \label{alg:1:discard}
            \Comment{$s$ is not covered}
        \State $\sAttract \leftarrow s$               
        \label{alg:1:attract} 
                \State $\pi_i$ = $\calP.$\textsc{PlanPath}($\Sstart, \sAttract$); \label{alg:1:pp}
                \hspace{2mm }
                $\calL \leftarrow \calL \cup \{ \pi_i \}$  \label{alg:1:path}
                \State $(\text{OPEN}, r_i) \leftarrow$ \textsc{ComputeCoverage}($\sAttract$) \label{alg:1:cr}
%                \State $(\text{OPEN}, r_i) \leftarrow$ \textsc{ComputeCoverage}($\sAttract, r_{\text{max}}$) \label{alg:1:cr}
                \State insert Valid(OPEN) in $V$  \label{alg:1:insert_v}
                \State insert Invalid(OPEN) in $I$   \label{alg:1:insert_i}
                \State $R_i$ $\leftarrow$ $(\sAttract, r_i)$; 
                \hspace{2mm} $ i \leftarrow i+1$        \hspace{2mm }
                $\calR \leftarrow \calR \cup \{ R_i \}$
                            \EndIf
        \EndWhile

\vspace{2mm}        
        
        \While {$I$ is not empty} \label{alg:1:iv_loop}
            \State $s$ $\leftarrow$ $I.pop()$
      \If {$\nexists R \in \calR \cup \hat{\calR}$ s.t. $s \in R$ }      \Comment{$s$ is not covered}
\State $(X, r)$ $\leftarrow$ \textsc{FindValidUncoveredState}($s$)
%                \State $(X, r)$ $\leftarrow$ \textsc{FindValidUncoveredState}($s, r_{\text{max}}$)
                \State $\hat{R}$ $\leftarrow$ $(s,r)$;
        \hspace{2mm}
        $\hat{\calR} \leftarrow \hat{\calR} \cup \{ \hat{R} \}$ \Comment{invalid subregion}  \label{alg:1:iv_region}
                \If {$X$ is not empty}  \Comment{valid state found}  \label{alg:1:x_states}
                    \State insert $X$ in $V$
                    \State \textbf{break} \label{alg:1:break}
                \EndIf
            \EndIf
        \EndWhile
    \EndWhile

  \vspace{2mm}

  \State \Return $\calR, \calL$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{figure}[tb]
  \centering
%    \includegraphics[width=0.3\textwidth]{alg2.pdf}
    % \vspace{-2mm}
  \caption{
  Visualization of Alg~\ref{alg:2}. Subregion $R_i$ (green) grown from $\sAttract$ in a goal region~$G$ (tiled grey) containing an obstacle (red).
  Frontier states and first state not in $R_i$ are depicted by circles and a cross, respectively.
}
    \label{fig:alg2}
% \vspace{-6mm}
\end{figure}

\paragraph{ Search}
The core of our planner lies in the way we compute the subregions (Alg.~\ref{alg:2} and Fig.~\ref{fig:alg2}) which we call a ``Coverage Search''. The algorithm maintains a set of \emph{covered} states $S_{\text{covered}}$ for which Property~\ref{property:1} holds.
As we will see, this will ensure that in the query phase, we can run a greedy search from any covered state $s \in S_{\text{covered}}$ and it will terminate in the attractor state. 
%
The following recursive definition formally captures the notion of a covered state.

\vspace{2mm}
\begin{definition}
  Given some attractor state \sAttract, we say that a state $s \in G_\calS$ is covered under some function $h(\cdot)$ by to \sAttract if either
  (i)~$s = \sAttract$ or
  (ii)~the greedy predecessor of $s$ with respect to $h(s,\sAttract)$ is covered.
\end{definition}

The algorithm computes a subregion that covers the maximum number of states that can fit into a hyper-ball defined by $h(s,\sAttract)$. 
The search maintains a priority queue OPEN ordered according to $h(s,\sAttract)$. Initially, the successors of $\sAttract$ are inserted in the OPEN (line~\ref{alg:2:OPEN}). For each expanded successor, if its valid greedy predecessor is in $S_{\text{covered}}$, then the successor is also labeled as covered (lines~\ref{alg:2:crit} and~\ref{alg:2:set}). 

The algorithm terminates when the search pops a state which is valid but does not have a greedy predecessor state in $S_{\text{covered}}$ (line~\ref{alg:2:terminate}). Intuitively, this corresponds to  the condition when the coverage search exits an obstacle (see Fig.~\ref{fig:alg2}).
At termination, all the states within the boundary of radius $r_i$ (excluding the boundary) are covered.


\begin{algorithm}[t]
%\footnotesize
\caption{Coverage Search}\label{alg:2}

\begin{algorithmic}[1]
\Procedure{ComputeCoverage}{$\sAttract$}
%\Procedure{ComputeCoverage}{$\sAttract, r_{\text{max}}$}
\State $S_{\text{covered}} \leftarrow \{\sAttract\}$ \Comment{covered set} \label{alg:2:covered}
\State OPEN $\leftarrow \{$Succs($\sAttract$)$\}$  \Comment{key: $h(s,\sAttract)$} \label{alg:2:OPEN}
\State CLOSED $\leftarrow \emptyset$
\State $r_i \leftarrow 0$

%\While {$r_i \leq r_{\text{max}}$}
\While{OPEN$\neq \emptyset$}
    \State $s \leftarrow$ OPEN.pop()
    \State insert $s$ in CLOSED
    \State $s'_g \leftarrow \argmin\limits_{s' \in \text{Preds}(s)} h(s', \sAttract)$ 
%    \State $s'_g \leftarrow$ GreedySucc($s$)
  \label{alg:2:greedy} 
 \Comment{greedy predecessor}
 % acc. to some tie-breaking criteria}
    \If {$s'_g$ $\in$ $S_{\text{covered}}$ and Valid(edge(s,$s'_g$))}  \label{alg:2:crit}
        \State $S_{\text{covered}} \leftarrow S_{\text{covered}} \cup \{s\}$  \Comment{$s$ is greedy} \label{alg:2:set}
    \ElsIf {Valid($s$)} \label{alg:2:terminate}
        \State $r_i \leftarrow h(s, \sAttract)$ \label{alg:2:rad}
        \State \Return (OPEN, $r_i$)
%        \State break
    \EndIf
    \For {each $s' \in \text{Succs}(s) \cap G_\calS$} \label{alg:2:prun}
        \If {$s' \notin$ CLOSED}
            \State insert $s'$ in OPEN with priority $h(s', \sAttract)$
        \EndIf
    \EndFor
\EndWhile
\State $r_i \leftarrow h(s, \sAttract) + \epsilon$    \Comment{$\epsilon$ is a small positive constant}
\State \Return (OPEN, $r_i$)

\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
%\footnotesize
\caption{Find valid uncovered state}\label{alg:3b}

\begin{algorithmic}[1]
\Procedure{FindValidUncoveredState}{$\hat{s}$}
\State OPEN $\leftarrow \{\hat{s}\}$
\While{OPEN$\neq \emptyset$}
  \State $s \leftarrow$ OPEN.pop()
  \State insert $s$ in CLOSED
  \If {$\nexists R \in \calR$  s.t. $s \in R$ and Valid(s)}
  % \Comment{$s$ is not covered and $s$ is valid}
    \State \Return ($\{s\}$, $h$($s$, $\hat{s}$))
  \EndIf
  \For {each $s' \in \{\text{Succs}(s) \cup \text{Preds}(s)\} \cap G_\calS$}
    \If {$s' \notin$ CLOSED}
        \State insert $s'$ in OPEN with priority $h(s', \sAttract)$
    \EndIf
  \EndFor
\EndWhile
\State $r = h$($s$, $\hat{s})) + \epsilon$    \Comment{$\epsilon$ is a small positive constant}
\State \Return ($\emptyset$, $r$))
\EndProcedure
\end{algorithmic}
\end{algorithm}


\paragraph{Query Phase}
Given a query goal state $s_{\text{goal}} \in G_\calS$ 
our algorithm, detailed in Alg.~\ref{alg:3}, starts by finding a subregion $R_i \in \calR$ which covers it (line.~\ref{alg:3:covers}). Namely, a subregion~$R_i$ for which 
$h(s_{\text{goal}}, \sAttract) < r_i$.
We then run a greedy search starting from $s_{\text{goal}}$ by iteratively finding for each state $s$ the predecessor with the minimum heuristic $h(s, \sAttract)$ value until the search reaches \sAttract (lines~\ref{alg:3:greedy-call} and~\ref{alg:3:greedy-call-start}-~\ref{alg:3:greedy-call-end}). 
The greedy path~$\pi_g$ is then appended to the corresponding precomputed path~$\pi_i \in \calL$ (line~\ref{alg:3:return}). 
Note that at no point  do we need to perform collision checking in the query phase (given the fact that the environment is static).

\begin{algorithm}[t]
\hspace*{\algorithmicindent} \textbf{Inputs:} $G_\calS, \calL, \calR$

\hspace*{\algorithmicindent} \textbf{Output:} $\pi$	\Comment{full path from \sStart to \sGoal}
%\footnotesize
\caption{Query}\label{alg:3}

\begin{algorithmic}[1]
\Procedure{FindGreedyPath}{$s_1, s_2$}
  \label{alg:3:greedy-call-start}
  \State $s_{\text{curr}} \leftarrow s_2$; \hspace{2mm} $\pi \leftarrow \emptyset$
  \While{$s_{\text{curr}} \neq s_1$} \label{alg:3:while}
    \State $\pi \leftarrow \pi \cdot s_{\text{curr}}$
    \Comment{append current state to path}
      \State $s_{\text{curr}} \leftarrow \argmin\limits_{s \in \text{Preds}(s_{\text{curr}})} h(s, s_1)$  \Comment{greedy predecessor}
    \EndWhile
    \State \Return \textsc{Reverse} ($\pi$)   \Comment{reverse to get a path from $s_1$ to $s_2$}
\EndProcedure
\label{alg:3:greedy-call-end}

\vspace{2mm}

\Procedure{ComputePath}{$\sGoal$}
  \For {each $R_i \in \calR$}
    \If {$h(s_{\text{goal}}, \sAttract) < r_i$}
    \Comment{$R_i$ covers $\sGoal$}
    \label{alg:3:covers}
      \State $\pi_g \leftarrow$ \textsc{FindGreedyPath} ($\sAttract, \sGoal$)
      \label{alg:3:greedy-call}
      \State \Return $\pi_i \cdot \pi_g$  \Comment{append $\pi_g$ to~$\pi_i \in \calL$}
      \label{alg:3:return}
    \EndIf
  \EndFor

\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Analysis}
\label{sec:analysis1}
In this section we formally prove that 
our algorithm is correct (Sec.~\ref{subsec:correct}) and 
analyze its computational complexity and bound on solution quality (Sec.~\ref{subsec:complexity} and ~\ref{subsec:quality} respectively).

\subsubsection{Correctness}
\label{subsec:correct}
To prove that our algorithm is correct, we show that indeed all states of every subregion are covered and we can identify if a state belongs to a subregion using its associated radius.
Furthermore, we show that a path obtained by a greedy search within any subregion is valid and that all states in $G_\calS$ are covered by some subregion.
These notions are captured by the following set of lemmas.

\vspace{2mm}
\begin{lemma}
\label{lemma:covered-1}
Let $S_{\text{covered}}$ be the set of states computed by Alg.~\ref{alg:2} for some attractor vertex \sAttract.
%
Every state $s \in S_{\text{covered}}$ is covered by \sAttract.
\end{lemma}
%
\begin{proof}
The proof is constructed by an induction over the states added to $S_{\text{covered}}$.
The base of the induction is trivial as the first state added to $S_{\text{covered}}$  is \sAttract (line~\ref{alg:2:covered}) which, by definition, is covered by \sAttract.
%
A state $s$ is added to $S_{\text{covered}}$ only if its greedy predecessor is in $S_{\text{covered}}$ (line~\ref{alg:2:greedy}) which by the induction hypothesis is covered by \sAttract.
This implies by definition that $s$ is covered by \sAttract.
%
Note that this argument is true because the greedy predecessor of every state is unique (Assumption~\ref{asm:1:3}).
\end{proof}

\begin{lemma}
\label{lemma:covered-2}
Let $S_{\text{covered}}$ be the set of states computed by Alg.~\ref{alg:2} for some attractor vertex \sAttract.
%
A state $s$ is in $S_{\text{covered}}$ iff $h(s, \sAttract) < r_i$.
\end{lemma}

\begin{proof}
Alg.~\ref{alg:2} orders the nodes to be inserted to $S_{\text{covered}}$ according to $h(s, \sAttract)$ (line~\ref{alg:2:OPEN}).
As our heuristic function is weakly monotonic (Assumption~\ref{asm:1:2}), the value of $r_i$ monotonically increases as the algorithm adds states to $S_{\text{covered}}$ (line~\ref{alg:2:rad}).
Thus, for every state $s \in S_{\text{covered}}$, we have that $h(s, \sAttract) < r_i$.

For the opposite direction, assume that there exists a state $s \notin S_{\text{covered}}$ such that $h(s, \sAttract) < r_i$.
This may be because  Alg.~\ref{alg:2} terminated due to a node $s'$ that was popped from the open list with 
$h(s', \sAttract) \leq h(s, \sAttract)$.
However, using the fact that our heuristic function is weakly monotonic (Assumption~\ref{asm:1:2}) we get a contradiction to the fact that $h(s, \sAttract) < r_i$.
Alternatively, this may be because we prune away states that are in $G_\calS$  (Alg.~\ref{alg:2} line~\ref{alg:2:prun}).
However, using the fact that our goal region is convex with respect to $h$ (Assumption~~\ref{asm:1:2}), this cannot hold.
\end{proof}

\begin{lemma}
\label{lemma:greedy}
Let $R_i \in \calR$ be a subregion computed by Alg.~\ref{alg:2}.
for some attractor vertex \sAttract.
% 
A greedy search with respect to $h(s, \sAttract)$  starting from any valid state $s \in R_i$ is complete and valid.
\end{lemma}

\begin{proof}
Given a state $s \in R_i$, we know that $s \in S_{\text{covered}}$ (Lemma~\ref{lemma:covered-2})
and that it is covered by \sAttract (Lemma~\ref{lemma:covered-1}).
%
It is easy to show (by induction) that any greedy search starting at a state $S_{\text{covered}}$ will only output states in $S_{\text{covered}}$.
Furthermore, a state is added to $S_{\text{covered}}$ only if the edge connecting to its greedy predecessor is valid (line~\ref{alg:2:crit}).
Thus, if $s\in S_{\text{covered}}$ is valid, the greedy search with respect to $h(s, \sAttract)$  starting from $s$ is complete and valid.
\end{proof}

\begin{lemma}
\label{lemma:coverage}
At the end of Alg.~\ref{alg:1}, every state $s \in G_\calS$ is covered by at least one subregion $R \in \calR$.
\end{lemma}
\begin{proof}
Assume that this does not hold and let $s \in G_\calS$ be a state that is not covered by any subregion but has a neighbor (valid or invalid) that is covered.
%
If $s$ is valid, then it would have been in the valid frontier states $V$ and either been picked to be an attractor state (line~\ref{alg:1:attract}) or covered by an existing subregion (Alg.~\ref{alg:2}).
%
A similar argument holds if $s$ is not valid.
\end{proof}

From the above we can immediately deduce the following corollary:

\vspace{2mm}

\begin{cor}
  After preprocessing the goal region~$G_\calS$ (Alg.~\ref{alg:1} and~\ref{alg:2}), in the query phase we can compute a valid path for any valid state $s \in G_\calS$ using Alg.~\ref{alg:3}.
\end{cor}


\textbf{Remark}
We can relax Assumption~\ref{asm:1:2} in two ways.
The first is by explicitly tracking states not in the goal region instead of pruning them away (Alg.~\ref{alg:2} line~\ref{alg:2:prun}).
Unfortunately, this may require the algorithm to store many states not in $G_\calS$  which may be impractical (recall that Assumption~\ref{asm:1:1} only states that we can exhaustively store in memory the states in the goal region).
An alternative, more practical way, to relax Assumption~\ref{asm:1:2}  is by terminating the search when we  encounter a state not in the goal region.
This may cause the algorithm to generate much more subregions (with smaller radii) which may increase the memory footprint and the preprocessing times.


\subsubsection{Complexity}
In this section we will do an asymptotic analysis of the time and memory complexities of our algorithm for the worst-case.

The complexity analysis makes the following global assumptions.
\vspace{2mm}\begin{assumption}
\label{asm:1:4} $|\calR| \ll n_{G_{\calS}}$ where~$n_{G_{\calS}}$ is the total number of goals in~$G_\calS$
\end{assumption}
\vspace{2mm}\begin{assumption}
\label{asm:1:5} $G_\calS$ is fragmented such that $|\calR|$ is upper-bounded by a constant
\end{assumption}
%\vspace{2mm}\begin{assumption}
%\label{assum:8} The time it takes to expand a state~(single iteration of loop at Alg.~\ref{alg:3}, line~\ref{alg:3:while}) is less than the time it takes for a robot to execute one action.
%\end{assumption}

\label{subsec:complexity}
\paragraph{Time Complexity (Query phase)}
We define the variable~$\calD$ as depth (maximal number of expansions of a greedy search) of the largest subregion. We can measure the depth of each subregion in Alg.~\ref{alg:2} by keeping track of the depth of each expanded state from the root i.e., $\sAttract$.
%
\vspace{2mm}
\begin{lemma}
\label{lemma:unboundedD}
Given the number of subregions~$|\calR|$ and the depth of the largest subregion~$\calD$, full path can be found in~$O(\calD)$ time
\end{lemma}

\begin{proof}
The query time comprises of 
(i)~finding the containing subregion $R_i$
(ii)~running the greedy search to $\sAttract$.
Step~(i) requires iterating over all subregions (in the worst case) which takes $O(|\calR|)$ steps while 
step~(ii) requires expanding the states along the path from $\sGoal$ to $\sAttract$ which requires~$O(\calD)$ expansions.
For each expansion we need to find the greedy predecessor, considering at most $b$ predecessors, where $b$ is the maximal branching factor of our graph.
Hence, overall the query phase takes $O(|\calR| + \calD \cdot b)$ operations. Since~$|\calR|$ is upper bounded by a constant~(from assumption~\ref{asm:1:5}) and~$b$ is a constant factor, the time complexity thus becomes~$O(\calD)$.
\end{proof}

Note that we can also bound the number of expansions required for the query phase by bounding the maximum depth of the subregions~$\calD$. We can do that by terminating Alg.~\ref{alg:2} when the $R_i$ reaches the depth bound or if the existing termination condition (line~\ref{alg:2:terminate}) is satisfied. Having said that, this will come at the price of increasing the number of subregions and the assumption~\ref{asm:1:5} will also not hold anymore. To give an intuition, we can draw an analogy with the sphere packing problem~\cite{hales1992sphere}; for a required volume to be covered, if the radii of the spheres is fixed, then the number of spheres needed to cover the volume will not be bounded by a constant but will be a function of the size of the volume.

\vspace{2mm}
\begin{lemma}
\label{lemma:OrderR}
	Given the number of subregions~$|\calR|$ and bounded~$\calD$, full path can be found in~$O(|\calR|)$ time
\end{lemma}

Now we consider a special case for which the algorithm has~$O(1)$ complexity.

\vspace{2mm}
\begin{lemma}
\label{lemma:o1}
	For the case when ~$|\pi_g| \leq c*|\pi_i|$, where~$c \geq 1$ is a constant integer, each action requires~$O(1)$ time delibration
\end{lemma}

\begin{proof}
	Here $|\cdot|$ denotes the length of the paths. This lemma can be trivally proved from assumptions~\ref{asm:1:5} and by interleaving planning and execution. From assumption~\ref{asm:1:5}, the time required to find the containing subregion~$R_i$ is bounded by a constant. Once~$R_i$ is found, the robot can find the greedy path~$\pi_g$ while it executes~$\pi_i$ by interleaving planning and execution.
	Since~$|\pi_g|$ is bounded by ~$c*|\pi_i|$, expanding~$c$ states~($O(1)$ operation) for each step traversal along~$|\pi_i|$, the greedy search will find~$\pi_g$ by the time robot arrives at~$\sAttract$.	
%	
%If~$|\pi_i| > |\pi_g|$ and the assumption~\ref{assum:8} holds, by the time the robot reaches the~$\sAttract$,~$\pi_g$ will be computed and can be executed by the robot.
\end{proof}

%$\Tbound$ and ~$T_\textrm{const}$
\textbf{CTMP Criteria:} We now analyse the algorithm under the CTMP criteria described in chapter~\ref{sec:ctmp} for the special case of Lemma~\ref{lemma:o1}. Since the algorithm requires~$O(1)$ time to plan the next action, $T_\textrm{const}$ is constant-time bounded and can be deduced from the empirical analysis of the algorithm.
%
The constant~$c$ can be deduced from the specified time budget~$\Tbound (> T_\textrm{const})$. The larger the~$\Tbound$, the larger the number of actions $c$, that the algorithm can plan per action execution.
%
Furthermore, Lemma~\ref{lemma:greedy} assures that the greedy path~$\pi_g$ does not contain cycles, otherwise the greedy search would be incomplete. Hence from lemmas~\ref{lemma:o1} and~\ref{lemma:greedy} and by Definition.~\ref{def:complete} we have the following theorem.

\vspace{2mm}
\begin{theorem}
	The algorithm is CTMP-Complete.
%	By Def.~\ref{ctmp:def} and from lemmas~\ref{lemma:o1} and~\ref{lemma:greedy}, the algorithm is a CTMP algorithm.
\end{theorem}

We also have the following corollary.

\vspace{2mm}
\begin{cor}
	The algorithm can be used for non-stop execution under the condition~$\Tconst < \Texec$
\end{cor}
%
%\textbf{Constant-time Complexity}
%We need to show that $O(|\calR| + \calD \cdot b) \rightarrow O(1)$. While~$\calD$ and~$b$ are constants,~$|\calR|$ is not a constant and does not have a bound and so the complexity of the query phase as is is~$O(|\calR|$ which is linear in~$|\calR|$.
%Here we propose a modification or extension of the full algorithm (both preprocessing and query phases) which is detailed in section~\ref{sec:extension} which will make the complexity constant time. The extension allows us to specify a constant upper bound on~$|\calR|$.  
%Now recall the definition of the constant-time algorithm from section~\ref{sec:ctmp}. Irrespective of the size of input to the algorithm, since ~$|\calR|$ has a constant upper bound and $\calD$ and~$b$ are constants, the query phase has constant-time complexity i.e.~$O(1)$.
%
%Note that in~$O(|\calR| + \calD \cdot b)$,~$\calD$ and~$b$ are trivially bounded; both are constants. The third term~$|\calR|$ however depends on goal region~$G_\calS$. Roughly speaking,~$|\calR|$ depends on three things~(1) how cluttered~$G_\calS$ is with obstacles, the more cluttered it is the more subregions we will get~(2) the size of~$G_\calS$; a bigger goal region will render more subregions and~(3) the parameter~$\calD$. We make the assumption that for a choice of~$\calD$~(large enough), for certain degree of obstacle clutter in C-space~(less enough) and by our original problem formulation that the goal region is a small subset of the C-space, the upper bound on~$|\calR|$ is bounded independently of the size of~$G_\calS$. Under this analysis we can conclude that the algorithm has a constant time complexity i.e~$O(1)$.
%
\paragraph{Space Complexity}
This section analyses the worst-case space complexity of the metadata generated after the preprocessing phase that is then used in the query phase.
\vspace{2mm}
\begin{lemma}
	Given the number of subregions~$|\calR|$ and unbounded~$\calD$, the algorithm requires~$O(|\pi_{\text{max}}|)$ space.
\end{lemma}
\begin{proof}
	The preprocessing phase generates metadata of size~$O(|\pi_{\text{max}}| \cdot |\calR|)$ where $|\pi_{\text{max}}|$ is the length of the longest precomputed path in~$\calL$. Since~$\calR$ is upper bounded by a constant~(assumption~\ref{asm:1:5}) and also because~$|\calR| \ll n$~(assumption~\ref{asm:1:4}) the algorithm requires~$O(|\pi_{\text{max}}|)$ space.
\end{proof}

\vspace{2mm}
\begin{lemma}
	Given the number of subregions~$|\calR|$ and bounded~$\calD$, the algorithm requires~$O(|\pi_{\text{max}}| \cdot |\calR|)$ space.
\end{lemma}
\begin{proof}
	Enforcing a bound on~$\calD$ breaks assumption~\ref{asm:1:5} as explained earlier in the analysis. This introduces~$|\calR|$ as a factor in the space complexity.
\end{proof}


%\subsubsection{Algorithm Completeness}
%\label{subsec:completeness}
%Our method generates plans by stitching together a path from the library $\calL$ (computed offline using some planner~$\calP$), and a path computed online which is returned by the greedy search on a discretized graph $G_\calS$. For the former, our method simply inherits the completeness properties of the planner $\calP$, whereas for the latter, our method is resolution complete; it follows from the correctness discussion (Section~\ref{subsec:correct}).

\subsubsection{Bound on Solution Quality}
\label{subsec:quality}
Let the function $c(\cdot)$ denote the cost of a path and $c^*(\cdot)$ denote the cost of an optimal path. Assuming that we precompute each path $\pi_i \in \calL$ with the optimal cost $c^*(\pi_i)$, from the triangle inequality it can be trivially shown that the quality of complete path $\pi$ computed by our method has an additive suboptimality bound; i.e.,
 % \begin{center}
 $c(\pi) - c^*(\pi) < 2 * c(\pi_g)$,
 % \end{center}
where $c(\pi_g)$ is the cost of the greedy path from the attractor to the goal state.

%\ignore{
%\subsection{Algorithm Extension}
%\label{sec:extension}
%In section~\ref{sec:analysis}, we enforced a bound on the number of subregions to guarantee~$O(1)$ complexity. If we do so Alg.~\ref{alg:1} may terminate without covering the~$G$ fully.
%The key idea of the following algorithm extension is to change the method of finding the attractor given a goal from geometric checks~(using radii of subregions) to a hash lookup which is a constant time operation. For doing so, the algorithm needs to pay an extra price in terms of memory, since now it has to store the goals in~$G_\calS$ and associated attractors in a lookup table. The new method differs both in preprocessing and query phases and is explained in this section.
%
%\subsubsection{Preprocessing Phase and Coverage Search}
%In case the upper bound on the number of subregions is reached in Alg.~\ref{alg:1} before termination, the algorithm switches to Alg.~\ref{alg:grp2}. The key difference is that instead of computing new geometric subregions~$R_i$, the algorithm maintains a lookup table~$\calM$ mapping the goals to their corresponding attractors computed during the coverage search. The modified coverage search is algorithm is detailed in Alg.~\ref{alg:cover2}. The two main differences in the coverage search are that OPEN need not be a priority queue; it can be an unordered queue and the termination condition that was there in Alg.~\ref{alg:2} is removed.
%
%\subsubsection{Query phase}
%
%
%%% Main Alg
%
%\begin{algorithm}[t]
%%\footnotesize
%\hspace*{\algorithmicindent} \textbf{Inputs:} $G_\calS$, $\Sstart$, $\calP$
%\Comment{goal region, start state and a planner} 
%
%\hspace*{\algorithmicindent} \textbf{Outputs:} 
%$\calL$,
%$\calM$
%%$R_i = (\sAttract, r_i)$, $\pi_i$,
%\Comment{Library of paths to $\Sstart$ and a lookup table}
%\caption{Goal Region Preprocessing}\label{alg:grp2}
%\begin{algorithmic}[1]
%\Procedure{PreprocessRegion}{$G_\calS$}
%  \State $V \leftarrow$\textsc{ GetAllValidStates}($G_\calS$)
%  \State $ i \leftarrow 0$
%       \hspace{2mm} 
%       $\calL = \emptyset$
%       \hspace{2mm} 
%       $\calM = \emptyset$
%       
%  \vspace{2mm}
%    %\While {$V$ and $I$ are not empty}
%        \While {$V$ is not empty}
%          \State $s \leftarrow V.\text{pop}()$ %\label{alg:1:pop}
%            %\If {$s$ is not covered}  %\label{alg:1:discard}
%           % \Comment{$s$ is not covered}
%        \State $\sAttract \leftarrow s$               
%        %\label{alg:1:attract} 
%                \State $\pi_i$ = $\calP.$\textsc{PlanPath}($\Sstart, \sAttract$); %\label{alg:1:pp}
%                \hspace{2mm }
%                $\calL \leftarrow \calL \cup \{ \pi_i \}$  %\label{alg:1:path}
%                \State $S_{\text{covered}} \leftarrow$ \textsc{ComputeCoverage}($\sAttract$) %\label{alg:1:cr}
%                \State insert lookup enteries $S_{\text{covered}} \rightarrow \sAttract$ in~$\calM$
%                \State $V \leftarrow V \backslash S_{\text{covered}}$;
%                \hspace{2mm} $ i \leftarrow i+1$
%
%        \EndWhile
%
%  \vspace{2mm}
%
%  \State \Return $\calL, \calM$
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{algorithm}[t]
%%\footnotesize
%\caption{Coverage Search}\label{alg:cover2}
%
%\begin{algorithmic}[1]
%\Procedure{ComputeCoverage}{$\sAttract$}
%%\Procedure{ComputeCoverage}{$\sAttract, r_{\text{max}}$}
%\State $S_{\text{covered}} \leftarrow \{\sAttract\}$ \Comment{covered set} %\label{alg:2:covered}
%\State OPEN $\leftarrow \{$Succs($\sAttract$)$\}$  %\Comment{key: $h(s,\sAttract)$}% \label{alg:2:OPEN}
%\State CLOSED $\leftarrow \emptyset$
%\State $r_i \leftarrow 0$
%
%%\While {$r_i \leq r_{\text{max}}$}
%\While{OPEN$\neq \emptyset$}
%    \State $s \leftarrow$ OPEN.pop()
%    \State insert $s$ in CLOSED
%    \State $s'_g \leftarrow \argmin\limits_{s' \in \text{Preds}(s)} h(s', \sAttract)$ 
%%    \State $s'_g \leftarrow$ GreedySucc($s$)
%  %\label{alg:2:greedy} 
% \Comment{greedy predecessor}
% % acc. to some tie-breaking criteria}
%    \If {$s'_g$ $\in$ $S_{\text{covered}}$ and Valid(edge(s,$s'_g$))} % \label{alg:2:crit}
%        \State $S_{\text{covered}} \leftarrow S_{\text{covered}} \cup \{s\}$  \Comment{$s$ is greedy}% \label{alg:2:set}
%    \EndIf
%    \For {each $s' \in \text{Succs}(s) \cap G_\calS$}% \label{alg:2:prun}
%        \If {$s' \notin$ CLOSED}
%            \State insert $s'$ in OPEN with priority $h(s', \sAttract)$
%        \EndIf
%    \EndFor
%\EndWhile
%%\State $r_i \leftarrow h(s, \sAttract) + \epsilon$    \Comment{$\epsilon$ is a small positive constant}
%\State \Return $S_{\text{covered}}$
%
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{algorithm}[t]
%\hspace*{\algorithmicindent} \textbf{Inputs:} $G_\calS$, $\calM$, $\calL$
%
%\hspace*{\algorithmicindent} \textbf{Output:} $\pi$	\Comment{full path from \sStart to \sGoal}
%%\footnotesize
%\caption{Query}\label{alg:3}
%
%\begin{algorithmic}[1]
%
%%\Procedure{FindGreedyPath}{$s_1, s_2$}
%%  \label{alg:3:greedy-call-start}
%%  \State $s_{\text{curr}} \leftarrow s_2$; \hspace{2mm} $\pi \leftarrow \emptyset$
%%  \While{$s_{\text{curr}} \neq s_1$}
%%    \State $\pi \leftarrow \pi \cdot s_{\text{curr}}$
%%    \Comment{append current state to path}
%%      \State $s_{\text{curr}} \leftarrow \argmin\limits_{s \in \text{Preds}(s_{\text{curr}})} h(s, s_1)$  \Comment{greedy predecessor}
%%    \EndWhile
%%    \State \Return \textsc{Reverse} ($\pi$)   \Comment{reverse to get a path from $s_1$ to $s_2$}
%%\EndProcedure
%%\label{alg:3:greedy-call-end}
%%
%%\vspace{2mm}
%
%\Procedure{ComputePath}{$\sGoal$}
% % \For {each $R_i \in \calR$}
%    %\If {$h(s_{\text{goal}}, \sAttract) < r_i$}
%    %\Comment{$R_i$ covers $\sGoal$}
%    \State $\sAttract \leftarrow \calM$.\textsc{LookupAttractor}($\sGoal$)
%    %\label{alg:3:covers}
%      \State $\pi_g \leftarrow$ \textsc{FindGreedyPath} ($\sAttract, \sGoal$)
%     % \label{alg:3:greedy-call}
%      \State \Return $\pi_i \cdot \pi_g$  \Comment{append $\pi_g$ to~$\pi_i \in \calL$}
%     % \label{alg:3:return}
%    %\EndIf
%  %\EndFor
%
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}
%}

\subsection{Implementation details}
\label{subsec:impl}

\subsubsection{Ordering subregions for faster queries}
Recall that in the query phase we iterate over all subregions to find one that covers \sGoal. 
In the worst case we will have to go over all subregions.
However,  our algorithm typically covers most of the goal region $G_\calS$ using a few very large subregions (namely, with a large radii~$r_i$) and the rest of $G_\calS$ is covered by a number of very small subregions.

Thus, if we order our subregions (offline) according to their corresponding radii, there is a higher chance of finding a covering subregion faster. While this optimization does not change our worst-case analysis (Sec.~\ref{sec:analysis}), it speeds up the query time in case the number of subregions is very large.

\subsubsection{Using Hash Table for Singletons}
In experiments we notice that the distribution of the subregions as a function of the radii is skewed towards smaller regions and a significant number of regions are merely singletons i.e. regions having a single state. In order to decrease the query time we can store all these states in a hash table mapping each singleton to the precomputed path~$\pi_i \in \calL$.  In the query phase, this lookup table can be queried before doing the linear search through all the subregions or the vice versa depending on which operation is faster experimentally. The lookup can be performed in constant time~(worst-case)~\cite{czech1997perfect}. Complementing the preprocessed data with a lookup table improves the average query times.

\subsubsection{Using Efficient Nearest Neighbor Datastructures}
Using an efficient nearest neighbor data structure to store the subregions like kdtrees~\cite{bentley1975multidimensional} or cover trees~\cite{beygelzimer2006cover} etc. can reduce the time complexity of the query phase. The radii of the subregions can be included as an extra dimension to the states and specialised metric for the nearest neighbor data structure can be used to find the subregion containing the state.


\subsubsection{Efficiently constructing $\calL$}
Constructing paths to the attractor states (Alg.~\ref{alg:1}, line~\ref{alg:1:pp}) can be done using any motion planning algorithm $\calP$.
In our implementation we chose to use RRT-Connect~\cite{KL00}.
Interestingly, this step dominates the running time of the preprocessing step.
%
To improve the preprocessing time, we initially set a small timeout for RRT-Connect to compute a path from an attractor state $\Sstart$ to $\sAttract$.
Attractor states for which RRT-Connect fails to find a path to $\sAttract$ are marked as \textit{bad} attractors and we do not grow the subregions from them. 
These, so-called \textit{bad} attractors are discarded when other subregions cover them.

When Alg.~\ref{alg:1} terminates, we reload the valid list $V$ with the remaining bad attractors and rerun Alg.~\ref{alg:1} but this time with a large timeout for RRT-Connect. 
%
We can also increase the timeout with smaller increments and run Alg.~\ref{alg:1} iteratively until there are no more bad attractors (assuming that there exists a solution for each goal state $\in$ $G_\calS$).


\subsubsection{Pruning redundant subregions}
To reduce the number of precomputed subregions we remove redundant ones after the Alg.~\ref{alg:1} terminates. 
In order to do that, we iterate through all the subregions and remove the ones which are fully contained within any other subregion. 
%Note that this is possible the way Alg.~\ref{alg:1} is designed. 
This step reduces both the query complexity (see Sec.~\ref{sec:analysis}) as well as the memory consumption.

\subsubsection{Interleaving Planning and Execution}
\label{subsubsec:interleave}
We know that in the query phase we have access to the precomputed library of paths~$\calL$, that stores paths from~$\sStart$ to all the attractors. Instead of computing the full path from~$\sStart$ from~$\sGoal$ and then executing it, the robot can start executing the path~$\pi_i$ (path from~$\sStart$ to~$\sAttract$) and run Alg.~\ref{alg:1:attract} in parallel. If the execution time of path~$\pi_i$ is greater than that of running Alg.~\ref{alg:1:attract}, the planning time will reduce to merely the time it takes to identify the subregion~$\R_i$.


\subsection{Experimental Results}
\label{sec:eval}

\subsubsection{Mail Sorting Task}

We evaluated our algorithm on the PR2 robot for a single-arm (7-DOF) motion-planning problem. The illustrated task here is to pick up envelopes from a cart and put them in the cubby shelves (see Fig.~\ref{fig:PR2}). Such settings are common in mailroom environments where the robot may have to encounter the same scenario over and over again. The start state is a fixed state corresponding to the pickup location, whereas the task-relevant goal region~$G$ is specified by bounding the position and orientation of the end effector. For this domain, we define~$G$ as a bounding box covering all possible positions and orientations of the end effector within the cubby shelf.

The search is done on an implicit graph $G_\calS$ constructed using $\textit{motion primitives}$ which are small kinematically feasible motions. We define the primitives in the task-space respresentation as small motions for the robot end effector in position axes ($\textit{x, y, z}$) and orientation axes of Euler angles ($\textit{roll, pitch, yaw}$), and a joint-angle motion for the redundant joint of the 7-DOF arm. 
The heurstic function is the Euclidean distance in these seven dimensions. The discretization we use for the graph $G_\calS$ is 2~cm for the position axes, 10 degrees for the Euler axes and 5 degrees for the redundant joint. 
For this domain, we keep the $\textit{pitch}$ and $\textit{roll}$ of the end effector fixed and allow motion primitives along the remaining five dimensions i.e. $\textit{x, y, z, yaw}$ and the reduntant joint angle. We also limit the $\textit{yaw}$ to be between -30 and 30 degrees. Note that the specification of the $G_\calS$ is purely task specific and we exploit the task constraints to limit the size of $G_\calS$ which makes our preprocessing step tractable (Assumption~\ref{asm:1:1}).

We compared our approach with different single- and multi-query planners in terms of planning times, success rates and memory consumption (see Table~\ref{tab:stats}) for 200 uniformly sampled goal states from $G$. Among the multi-query planners, we implemented PRM, a multi-query version of RRT which we name MQ-RRT and the E-graph planner. 
For MQ-RRT, we precompute an RRT tree rooted at  $\Sstart$ offline (similar to PRM) and query it by trying to connect $\sGoal$ to the nearest nodes of the precomputed tree. We use the same connection strategy for MQ-RRT as the one that the asymptotically-optimal version of PRM uses\footnote{In order for the quality of paths obtained the PRM to converge to the quality of the optimal solution, a query should be connected to its $k$ nearest neighbors where 
$k = e(1+1/d)\log(n)$.
Here $n$ is the number of nodes in the tree/roadmap and $d$ is the dimensionality of the configuration space~\cite{karaman2011sampling,SSH16}. 
}.
For PRM, we precomputed the paths from all the nodes in~$G$ to~$s_{\text{start}}$ (this is analogous to our library~$\calL$). 
The query stage thus only required the connect operation (i.e. attempting to connect to $k$ nearest neighbors of $\sGoal$). For both of these planners we also added a goal-region bias by directly sampling from~$G$, five percent of the time\footnote{We used OMPL~\cite{SMK12} for comparisons with the sampling-based planners and modified the implementations as per needed.}. 

For single-query planning, we only report the results for RRT-Connect as it has the fastest run times from our experience. For PRM and MQ-RRT, if the connect operation fails for a query, we considered that case as a failure. For RRT-Connect, we set a timeout of 10 seconds.

For our method, preprocessing (Alg.~\ref{alg:1}) took 1,445 seconds and returned 1,390 subregions. For precomputing paths (Alg.~\ref{alg:1}, line~\ref{alg:1:pp}), we use RRT-Connect. For the first run of Alg.~\ref{alg:1}, we set the timeout to be 10 seconds and for the second run (after reloading $V$ with the bad attractors), we set the timeout to be 60 seconds. By doing so the algorithm finishes successfully with having no remaining bad attractors.

Table~\ref{tab:stats} shows the numerical results of our experiments. Our method being provably guaranteed to find a plan in bounded time, shows a success rate of 100 percent. For the two preprocessing-based planners PRM and MQ-RRT, we report the results for a preprocessing time of 4T (T being the time consumed by our method in preprocessing). The E-graph planner is bootstrapped with a hundred paths precomputed for uniformly sampled goals in $G_S$. For each of these three multi-query planners while running the experiments, the newly-created edges were appended to the auxilary data (tree, roadmap or E-graph) to be used for the subsequent queries.

Among other planners, PRM shows the highest success rate but at the cost of a large memory footprint. The E-graph planner has a small memory footprint but it shows significantly longer planning times. RRT-Connect being a single-query planner happens to be the slowest. Our method shows a speedup of over tenfold in query time as compared to PRM and MQ-RRT and about three orders of magnitude speedup over the E-graph planner and RRT-Connect. The plots in Fig.~\ref{fig:plots} show how the success rate and the memory footprints of PRM and MQ-RRT vary as a function of the preprocessing time. For our domain the PRM seems to saturate in terms of the success rate after T time whereas MQ-RRT continues to improve provided more preprocessing time. In terms of memory usage, PRM's memory footprint grows more rapidly than MQ-RRT.

\begin{table*}[t]
\centering
     \resizebox{\columnwidth}{!}{%
    % \resizebox{1.4\textwidth}{!}{\begin{minipage}{\textwidth}
        \begin{tabular}{ l | c c c c c}
           & PRM (4T) & MQ-RRT (4T) & E-graph & RRT-Connect & \textbf{Our method} \\
         \hline
         Planning time [ms]& 21.7 (59.6) & 21.2 (35.5) & 497.8 (9678.5) & 1960 (9652) & \textbf{1.0 (1.6)}\\
         Success rate [$\%$]& 86 & 69.75 & 76.5 & 83.8 & \textbf{100}\\
         Memory usage [Mb] & 1,828 & 225.75 & \textbf{2.0} & - & {7.8}
        \end{tabular}
    }
    \caption{Experimental results comparing our method with other single- and multi-query planners tested on Intel Core i75600U (2.6GHz) machine with 16GB RAM. The table shows the mean/worst-case planning times, success rates and memory usage for our method and for other multi-query planners preprocessed with quadruple the time that our method takes in precomputation (T = 1,445 seconds). Note that the worst-case time for our method shown in these results ($\sim$1.6 millisecond) is the empirical one and not the computed provable time bound which is 3 milliseconds (on our machine) for this environment.
    Results of sampling-based planners are averaged over 200 uniformly sampled queries. For the sampling-based planners the results were averaged over 4 trials (for the same set of 200 queries) with different random number generation seeds.}
    \label{tab:stats}
  % \end{center}
% \vspace{-6mm}
\end{table*}


\begin{figure}
\begin{subfigure}{0.5\textwidth}
\centering
  \includegraphics[width=0.9\linewidth]{success.png}
  \caption{}
  \label{fig:success}
\end{subfigure}
\hfill
\begin{subfigure}{0.5\textwidth}
\centering
  \includegraphics[width=0.9\linewidth]{memory.png}
  \caption{}
  \label{fig:memory}
\end{subfigure}
    \caption{Preprocessing time vs~(\subref{fig:success}) the success rates and~(\subref{fig:memory}) memory useage for the 200 queries averaged over 4 trials with different random number generation seeds for the PRM and the MQ-RRT algorithms. The results were computed at the intervals which are multiple of the time T = 1,455 seconds, that our method takes for precomputation. The green cross shows our method for reference.}
    \label{fig:plots}
\end{figure}

\subsubsection{Motion Planning for Truck-unloading Robot}
We tested our algorithm on an industrial truck-unloading robot~(see Fig.~\ref{fig:gmt}). The robot has a mobile omnidirectional base referred to as \textit{base} and two articulated mechanisms--a manipulator-like tool with suction grippers and a  scooper-like tool with conveyor belts, referred to as \textit{arm} and \textit{nose}, respectively. The general task of this robot is to unload boxes from delivery trucks as quickly and efficiently as possible and that is why it requires fast and reliable motion planning.

The robot's arm is 4-Dof and the nose has 3-Dof. We restrict the base motion to be one-dimensional and only allow the base to move either forward or backward. The robot can be controlled in different modes of operation and we tested our planner for each of these modes~(see table~\ref{tab:gmt}). The start state of the robot for each of these mode is a fixed configuration where the robot drops off the boxes or resets. Similar to the mail-sorting domain, the goal region is defined in the task space of the robot. For this robot we define two goal regions~$G_{\textrm{arm}}$ and~$G_{\textrm{nose}}$ for the arm and nose respectively. The discretization of~$G$ along position axes~($x,y,z$) is~0.05m and rotations axes~($\textit{roll, pitch, yaw}$) is 5 degrees. The sizes of the goal regions are selected as per the requirement of the task.
We compare our results with Weighted A* (WA*)algorithm. The results are summarized in Table~\ref{tab:gmt}. We observe our algorithm shows over an order of magnitude improvements in terms of planning times over the WA*. For a timeout of 5 seconds both the planners showed 100 percent success.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{gmt.JPG}
    % \vspace{-2mm}
  \caption{
  Truck-unloader robot with its manipulator-like (arm) and scooper-like (nose) end effectors
}
    \label{fig:gmt}
% \vspace{-6mm}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{gmt_sim.png}
    % \vspace{-2mm}
  \caption{
  A simulated real world truck-unloading scenario
}
    \label{fig:gmt_sim}
% \vspace{-6mm}
\end{figure}

\begin{table*}[ht]
\centering
     \resizebox{0.7\columnwidth}{!}{%
    % \resizebox{1.4\textwidth}{!}{\begin{minipage}{\textwidth}
        \begin{tabular}{ l | c c c c}  
           & arm & nose & arm+base & nose+base\\
         \hline
         WA* 		& 12.8 & 7.2 & 7.5 & 4.4 \\
         Our Method & 0.69 & 0.29 & 0.5 & 0.47 \\
        \end{tabular}
    }
    \caption{Comparison of mean planning times~[ms] averaged over 100 randomized queries for different modes of operation of the truck-unloading robot. The number of subregions preprocessed for each of these modes~(left to right) are~101,~7,~85 and~308.}
    \label{tab:gmt}
  % \end{center}
% \vspace{-6mm}
\end{table*}

As this algorithm is only applicable to static (known) environments, we only preprocessed it with collision checking against the walls of the truck and with self collision checking and not with the boxes inside the truck. We also tested our planner in a simulated real world scenario~(see Fig.~\ref{fig:gmt_sim}) in conjunction with the WA* planner. For each query, we first ran our planner and validated the computed path against non-static obstacles i.e the boxes. We employed WA* as a fallback planner in case the first path is invalid. We observed that for all the queries ran for this scene, the fallback planner was never invoked. This experiment gave us an interesting use case for our planner to be employed in conjunction with conventional planners in the environments that are partially static.

\subsection{Algorithm Variant}
In this section we introduce a new variant of our algorithm and discuss its properties. The variant provides stronger constant-time guarantee for the case where the robot interleaves planning and execution. Specifically, it provides the same guarantee as lemma~\ref{lemma:o1} but without the included condition. We make the following high-level modifications to the previous algorithm to construct the new variant. 

\begin{enumerate}[label={\textbf{M\arabic*}},leftmargin=0.75cm]
\item \label{mod:flip} Flip the problem definition described in section~\ref{sec:pdef}. Namely instead of have a fixed start and a goal region, we have a fixed goal~$\sGoal$ and a start region~$S$~(see Fig.~\ref{fig:approach_flip}).
\item \label{mod:interleave} Interleave planning and execution and expand a constant number of states per action execution.
%\item \label{item:bound} Remove the bound~$\calD$ on the depth of the subregions.
\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{approach_flip.pdf}
    % \vspace{-2mm}
  \caption{
  Visualization of approach. Subregions are depicted in green, 
  start region~$S$ is depicted in tiled gray  containing obstacles (red).
  Precomputed paths from attractor states to~$\sGoal$ are depicted in blue.
 Given a query $(s_{\rm{start}}, s_{\rm{goal}})$, the returned path is given by the greedy path $\pi_g$~(depicted in purple) appended with the path~$\pi_i$ from~$\sAttract$ to $\sGoal$.
}
    \label{fig:approach_flip}
% \vspace{-6mm}
\end{figure}

The modifications result in the following lemma:
\vspace{2mm}
\begin{lemma}
\label{lemma:var:o1}
	Given the number of subregions~$|\calR|$ and the depth of the largest subregion~$\calD$, each action requires~$O(1)$ time delibration.
\end{lemma}

\begin{proof}
We consider the algorithm setting where~$|\calR|$ is bounded~(Assumption~\ref{asm:1:5}) but~$\calD$ is unbounded. From lemma~\ref{lemma:unboundedD} thereby, the query phase can find path in~$O(|\calR|)$ time.
%
However by modification~\ref{mod:interleave}, by interleaving planning and execution, the algorithm requires~$O(1)$ planning time per action.
%The completeness guarantee follows from section~\ref{sec:analysis} and by the construction of the algorithm, it is guaranteed that a state is never re-expanded or re-visited by the robot.
Once the robot reaches~$\sAttract$, it can then continue to follow the pre-stored path~$\pi_i$. 
\end{proof}

\textbf{CTMP Criteria:} We now analyse the algorithm variant under the CTMP criteria described in chapter~\ref{sec:ctmp}. Since the algorithm requires~$O(1)$ time to plan the next action, $T_\textrm{const} (< \Tbound)$ is constant-time bounded.
%
Furthermore, Lemma~\ref{lemma:greedy} assures that the greedy path~$\pi_g$ does not contain cycles, otherwise the greedy search would be incomplete. Hence from lemmas~\ref{lemma:var:o1} and~\ref{lemma:greedy} and by Def.~\ref{def:complete} we have the following theorem:

\vspace{2mm}
\begin{theorem}
	The algorithm variant is CTMP-Complete.
%	By Def.~\ref{ctmp:def} and from lemmas~\ref{lemma:o1} and~\ref{lemma:greedy}, the algorithm is a CTMP algorithm.
\end{theorem}

We also have the following corollary.

\vspace{2mm}
\begin{cor}
	The algorithm variant can be used for non-stop execution under the condition~$\Tconst < \Texec$
\end{cor}

Note that flipping the problem definition~(\ref{mod:flip}) will change the set of domains that our approach is applicable to. The experimental evaluation of this algorithm variant is outside the scope of this thesis and left as future work.

\newpage
\section{CTMP for Conveyor-pickup Task}
\label{sec:rss}
\subsection{Overview}
%% motivation
Conveyor belts are widely used in automated distribution, warehousing, as well as for manufacturing and production facilities. In the modern times robotic manipulators are being deployed extensively at the conveyor belts for automation and faster operations~\cite{zhang2018gilbreth}. In order to maintain a high-distribution throughput, manipulators must pick up moving objects without having to stop the conveyor for every grasp. In this work, we consider the problem of motion planning for grasping moving objects off a conveyor. An object in motion imposes a requirement that it should be picked up in a short window of time. The motion planner for the arm, therefore, must compute a path within a bounded time frame to be able to successfully perform this task.


%% why replanning?
    Manipulation relies on high quality detection and localization of moving objects. When the object first enters the field of view of the robot, the initial perception estimates of the object's pose are often inaccurate. Consider the example of an object (sugar box) moving along the conveyor towards the robot in Fig.~\ref{fig:intro_pic}, shown through an image sequence as captured by the robot's Kinect camera in Fig.~\ref{fig:pose_sequence}. In order to understand how pose estimation error varies as the object gets closer, we filter the input point cloud and remove all points lying outside the conveyor. We then compute the pair-wise minimum distance between the input point cloud and a predicted point cloud that corresponds to the object's 3D model transformed with the predicted pose through ICP~\cite{besl1992method} refinement. The plot in Fig.~\ref{fig:pose_sequence} shows the variation of the average pairwise minimum distance between the two point clouds as the object gets closer. We can observe that the distance decreases as the object moves closer, indicating that the point clouds overlap more closely due to more accurate pose estimates closer to the camera.
    
% The black color denotes points corresponding to the object in the observed scene after filtering out all points lying outside the conveyor. The white point cloud shows the object's 3D model transformed with the predicted 3-Degree of Freedom pose obtained by using Iterative Closest Point (ICP)~\cite{besl1992method} refinement. Upon observation of the overlap between the point clouds, we see that they begin to overlap more closely as the object moves towards the robot, indicating that the pose estimate is becoming increasingly accurate. 
However, if the robot waits too long to get an accurate estimate of the object pose, the delay in starting plan execution could cause the robot to miss the object. The likelihood of this occurring increases proportionately with the speed of the conveyor. Therefore, the robot should start executing a plan computed for the initial pose and as it gets better estimates, it should repeatedly replan for the new goals. However, for every replanning query, the time window for the pickup shrinks. This makes the planner's job difficult to support real-time planning.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\textwidth]{conveyor_pr2}
    \caption{
    \CaptionTextSize
    A scene demonstrating the PR2 robot picking up a moving object (sugar box) off a conveyor belt.}
    \label{fig:intro_pic}
   % \vspace{-7.5mm}
\end{figure}


%% why problem is hard
Furthermore, the planning problem is challenging because the motion planner has to account for the dynamic object and thus plan with time as one of the planning dimension. It should generate a valid trajectory that avoids collision with the environment around it and also with the target object to ensure that it does not damage or topple it during the grasp. Avoiding collisions with the object requires precise geometric collision checking between the object geometry and the geometry of the manipulator. The resulting complexity of the planning problem makes it infeasible to plan online for this task.

%%
Motivated by these challenges, we propose a planning framework that leverages offline preprocessing to provide bounds on the planning time when the planner is invoked online. Our key insight is that in our domain the manipulation task is highly repetitive---even for different object poses, the computed paths are quite similar and can be efficiently reused to speed up online planning. Based on this insight, we derive a method that precomputes a representative set of paths offline with some auxiliary datastructures and uses them online to plan in constant time. Here, we assume that the models of the target objects are known. Namely, the planner is provided with the geometric model of the target object apriori.
%To the best of our knowledge, our approach is the first to provide provable constant-time planning guarantees on generating motions all the way to the goal for dynamic environments.

%
We experimentally show that constant-time planning and replanning capability is necessary for a successful conveyor-pickup task. Specifically if we only perform one-time planning, (namely, either following the plan for the initial noisy pose estimate or from a delayed but accurate pose estimate) we frequently fail to pick the object.
\begin{figure}[t]
    \centering
    \begin{subfigure}{.49\textwidth}
    %   \centering
        \includegraphics[width=1.0\textwidth]{object_blur}
        \caption{}
        \label{fig:obj1}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
    %   \centering
        \includegraphics[width=1.0\textwidth]{pose_error_f}
        \caption{}
        \label{fig:obj2}
    \end{subfigure}
    \caption{
    \CaptionTextSize
    (\subref{fig:obj1})~Depiction of an object moving along a conveyor towards the robot.
    %
    (\subref{fig:obj2})~Pose error as a function of the distance from the conveyor's start.
    %, calculated as the average pair wise minimum distance between the input point cloud and the point cloud corresponding to the predicted pose.
    }
    \label{fig:pose_sequence}
    %\vspace{-7.5mm}
\end{figure}

\subsection{Algorithmic framework}
\subsubsection{Problem formulation and assumptions}
Our system is comprised of 
a robot manipulator,
a conveyor belt moving at some known velocity,
a set of known objects~$\calO$ that need to be grasped and 
a perception system that is able to estimate the type of object and its location on the conveyor belt.

Given a pose $g$ of an object $o \in \calO$, our task is to plan the motion of the robot such that it will be able to pick~$o$ from the conveyor belt at some future time.
%
Unfortunately, the perception system may give inaccurate object poses.
Thus, the pose $g$ will be updated by the perception system as the robot is executing its motion. 
To allow for the robot to move towards the updated pose in real time, we introduce the additional requirement that planning should be done within a predefined time bound~\Tbound. \Tbound, however, is lower bounded by a constant time $T_\textrm{const}$ (see chapter~\ref{sec:ctmp} for details of these terms).
%
For ease of exposition, when we say that we plan to a pose $g$ of $o$ that is given by the perception system, 
we mean that we plan the motion of the robot such that it will be able to pick~$o$ from the conveyor belt at some future time. 
This is explained in detail in Sec.~\ref{sec:eval} and in Fig.~\ref{fig:pe}.

%
We define a goal region $\Gfull$ comprised of the discrete set of initial object poses on the conveyor belt that the perception system can perceive.
%
Finally, we assume that the robot has an initial configuration \Shome from which it will start its motion to grasp any object.


Roughly speaking, the objective, following the set of assumptions we will shortly state, is to enable planning to any goal pose $ g \in \Gfull$ in bounded time~\Tbound regardless of the robot's current configuration.
%More specifically, the perception system is setup such that it sends updated pose estimates on the fly as the object moves along the conveyor belt and as the robot approaches the object.   
To formalize this idea, we will use the notion of \emph{reachable} and \emph{covered} goals that we introduced in Sec.~\ref{subsec:ctmp_alg}.
%\vspace{2mm}
%\begin{definition}
%    A goal pose $g \in \Gfull$ is said to be \emph{reachable} from a state $s$ if there exists a path from $s$ to $g$ and it can be computed in finite time.
%\end{definition}
Given a state $s$ we denote the set of all goals that are reachable from $s$ as $G^{\rm reach}(s)$ and we say that $G^{\rm reach}(s)$ is \emph{reachable} from $s$.

%\vspace{2mm}
%\begin{definition}
%    A reachable pose $g \in \Gfull$ is said to be \emph{covered} by a state $s$ if 
%    the system can plan a path from $s$ to $g$ within time~\Tbound.
%\end{definition}

Thus, we wish to build a system such that 
for any state $s$ that the system can be in 
and every reachable goal pose $g \in \Gfull$ updated by the perception system,
$g$ is covered by $s$.


We are now ready to state the assumptions for which we can solve the problem defined.
%\begin{definition}
%    A goal region $G \subseteq \Gfull$ is said to be \emph{reachable} from a state $s$ if any state $g \in G$ is reachable from $s$.
%\end{definition}

We make the following assumptions about the system.
%\begin{enumerate}[label={\textbf{A\arabic*}},leftmargin=0.75cm]
    % \item \label{asm:1:2} The goal set \Gfull is reachable from the start state~\Shome. Namely,  $G^{\rm reach}(\Shome) = \Gfull$.
    
\ignore{
    \item \label{asm:2:2} Given a path 
    $\Pi = \{s_0, \ldots, s_k \}$ 
    s.t. $s_0 = \Shome$ and $s_k \in \Gfull$, 
    we have that $G^{\rm reach}(s_{i+1}) \subset G^{\rm reach}(s_{i})$.
    Namely, the reachable set of goals for a state on the path is a subset of the reachable set of every other state on that path that exists before it.
    \os{Oren - Didn't we agree that this is not an assumption but a property?}
}

%    \item \label{assum:2:1} \Gfull would accommodate for an error $\epsilon$ in the initial pose estimate $g_{\textrm{init}}$ of the perception system. Each subsequent estimate $g_{\textrm{next}}$ will be within the $\epsilon$ window around $g_{\textrm{init}}$ (in retrospect because the conveyor is moving).
\vspace{2mm}
\begin{assumption} \label{assum:2:2} There exists a replan cutoff time \Trc from when the robot starts moving, after which the planner does not replan and continues to execute the last planned path.
\end{assumption}
\vspace{2mm}
\begin{assumption} \label{asassum:2:3} If the robot starts moving at $t = 0$ then for any time $t < \Trc$, the environment is static. Namely, objects on the conveyor belt cannot collide with the robot during that time.
\end{assumption}
\vspace{2mm}
\begin{assumption} \label{assum:2:1} Given an initial goal pose $g_{\textrm{init}} \in \Gfull$ by the perception system, any subsequent pose $g_{\textrm{new}} \in \Gfull$ is at most $\varepsilon_\calP$ distance away from $g_{\textrm{init}}$ (after accounting for the fact that the object moved along the conveyor belt).
\end{assumption}    

%\end{enumerate}

Assumptions~\ref{assum:2:2}-\ref{asassum:2:3} enforce a requirement that the perception system must provide an accurate estimate $g$ while $o$ is at a safe distance from the robot.
%
Assumption~\ref{assum:2:1} corresponds to the error tolerance of the perception system. This is explained in detail in Sec.~\ref{sec:eval} and in Fig.~\ref{fig:pe}
% Assumption~\ref{asm:1:2} corresponds to properties of the environment \emph{without} taking the objects on the conveyor belt into account 
% while


% Assumptions~\ref{assum:2:1}-\ref{asassum:2:3} correspond to properties of the environment that account for the perception system and the objects on the conveyor belt.


Our approach for bounded-time planning relies on a \emph{preprocessing} stage that allows to efficiently compute paths in a \emph{query} stage to any goal state (under Assumptions~\ref{assum:2:2}-\ref{assum:2:1}). 
%
Before we describe our approach, we start by describing a \naive method that solves the aforementioned problem but requires a prohibitive amount of memory.
%
This can be seen as a warmup before describing our algorithm which exhibits the same traits but doing so in a memory-efficient manner.

\subsubsection{Straw man approach}
\label{subsec:strawman}
\begin{figure}[t]
    \centering
    \begin{subfigure}{.49\textwidth}
    %   \centering
        \includegraphics[width=1.0\textwidth]{naive1}
        \caption{}
        \label{fig:naive1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
    %   \centering
        \includegraphics[width=1.0\textwidth]{naive2}
        \caption{}
        \label{fig:naive2}
    \end{subfigure}
    \caption{
    \CaptionTextSize
    The two steps of the preprocessing stage of the straw man algorithm. In both figures, paths that are very similar (and their corresponding goal states) are depicted using the same color.
    (\subref{fig:naive1})~First step---the algorithm computes a path from \Shome to every state in \Gfull.
    %
    (\subref{fig:naive2})~Second step---the algorithm computes for each path a new path to each goal state for every state along the path. Here, one representative path is depicted together with three different goal states. 
    This is repeated recursively for the new paths but not visualized.}
    \label{fig:naive}
    \vspace{-5mm}
\end{figure}

We divide the preprocessing stage into two steps.
In the first step, we compute from \Shome a path $\Pi_g$ to every reachable $g \in \Gfull$. 
%(such a path exists following \ref{asm:1:2}).
. Thus, all goal states are covered by \Shome and this allows us to start executing a path once the perception system gives its initial pose estimate.
However, we need to allow for updated pose estimations while executing~$\Pi_g$. 

%
Following \ref{assum:2:2} and \ref{asassum:2:3}, this only needs to be done up until time~\Trc.
Thus, we discretize each path such that consecutive states along the path are no more than $\delta _t$ time apart. As we will see, this will allow the system to start executing a new path within $\Tbound + \delta_t$ after a new pose estimation is obtained from the perception system.
%
We call all states that are less than \Trc time from \Shome \emph{replanable states}.


In the second step of the preprocessing stage, for every replanable state along each path $\Pi_g$, we compute a new path to all goal states. 
%\bound{that are $\varepsilon_\calP$ away from $g$}.
See Fig.~\ref{fig:naive}.
%
This will ensure that all goal states are covered by the replanabale states that were introduced in the first step of the preprocessing stage. Namely, it will allow to immediately start executing a new path once the goal location is updated by the perception system.
%
Unfortunately, the perception system may update the goal location more than once. Thus, this process needs to be performed recursively for the new paths as well.


The outcome of the preprocessing stage is a set of precomputed collision-free paths starting at states that are at most $\Trc$ from \Shome and end at goal states.
The paths are stored in a lookup table that can be queried in constant time $T_\textrm{const} < \Tbound$.

In the query stage we obtain an estimation $g_1$ of the goal pose by the perception system. 
The algorithm then retrieves the path~$\Pi_1(\Shome,g_1)$ from~\Shome to~$g_1$ and the robot starts executing~$\Pi_1(\Shome,g_1)$.
%
For every new estimation $g_i$ of the goal pose obtained from the perception system  while the system is executing path $\Pi_{i-1}(s,g_{i-1})$, the algorithm retrieves from the lookup table the path $\Pi_i(s',g_i)$ from the first state~$s'$ along $\Pi_{i-1}(s,g_{i-1})$ that is $\Tbound + \delta_t$ time from~$s$. The robot will then start executing~$\Pi_i(s',g_i)$ once it reaches~$s'$.

Clearly, every state is covered by this brute-force approach, however it requires a massive amount of memory.
Let $n_{\rm goal} = \vert \Gfull \vert$ be the number of goal states and
$\ell$ be the number of states between \Shome and the state that is \Trc time away.
This approach requires precomputing and storing $O(n_{\rm goal}^\ell)$ paths which is clearly infeasible.
In the next sections, we show how we can dramatically reduce the memory footprint of the approach without compromising on the system's capabilities.

\subsubsection{Key Idea}
While the straw man algorithm presented  allows for planning to any goal pose $ g \in \Gfull$ in bounded time~\Tbound, its memory footprint is prohibitively large.
%
We suggest to reduce the memory footprint by building on the observation that many paths to close-by goals traverse very similar parts of the configurations space (see Fig.~\ref{fig:naive}).

Similar to the straw man algorithm, our preprocessing stage runs in two steps.
In the first step we  compute from \Shome a path $\Pi_g$ to every goal state $ g \in \Gfull$. However, this is done by computing a set of so-called ``root paths'' $\{\Pi_1, \ldots, \Pi_k \}$ from \Shome to a subset of goals in \Gfull (here we will have that $k \ll n_{\rm goal})$. 
As we will see, these paths will allow to efficiently compute paths to every goal state in the query stage and the system only needs to explicitly store these root paths in memory (and not a path to every goal state as in the straw man algorithm).
%
In the second step of the preprocessing stage, the algorithm recursively computes for all replanabale states along all root paths a new path to each goal state. However, this is done by attempting to re-use previously-computed root paths which, in turn, allows for a very low memory footprint.
%
It is important to note that replanable states are not only the ones that lie on the root paths from \Shome---whenever the recursion happens we compute a new set of root paths and generate new replanable states
%
The remainder of this section formalizes these ideas.

\subsubsection{Algorithmic building blocks}
We start by introducing the algorithmic building blocks that we use.
Specifically, we start by describing the motion planner that is used to compute the root paths 
and then continue to describe how they can be used as \emph{experiences} to efficiently compute paths to near-by goals.

\paragraph{Motion planner $\calP$}
We use a heuristic search-based planning approach with motion primitives (see, e.g,~\cite{CCL10,CSCL11,LF09})
as it allows for deterministic planning time which is key in our domain.
Moreover, such planners can easily handle under-defined goals as we have in our setting---we define a goal as a six DoF grasp pose for the goal object while our robot has seven DoFs and we need to acount for the grasping time.

\textbf{State space and graph construction.}
We define a state $s$ as a pair $(q,t)$ where $q = (\theta_1, ..., \theta_n)$ is a configuration represented by the joint angles for an $n$-DOF robot arm (in our setting $n=7$) and $t$ is the time associated with $q$.
%
Given a state $s$ we define two types of motion primitives which are short kinodynamically feasible motions that the robot can execute. The first, which we term \emph{predefined} primitives are used to move to a pre-grasp position while the second, termed \emph{dynamic} primitives are used to perform grasping after the robot reached a pre-grasp position.
%

The predefined primitives are small individual joint movements in either direction as well as wait actions.
For each motion primitive, we compute its duration by using a nominal constant velocity profile for the  joint that is moved.
%\footnote{Computing a plan using these motion primitives is not guaranteed to respect the acceleration limits of the robot. However, in practice this seldom occurs and we elaborate in Sec.~\ref{sec:eval} how we handle these execution errors.}.
%

The dynamic primitives are generated by using a Jacobian pseudo inverse-based control law similar to what~\cite{menon2014motion} uses. 
The velocity of the end effector is computed such that the end-effector minimizes the distance to the grasp pose. Once the gripper encloses the object, it moves along with the object until the gripper is closed.
Additional details omitted due to lack of space. 

\textbf{Graph search}
The states and the transitions implicitly define a graph $\calG = (S,E)$ where $S$ is the set of all states and~$E$ is the set of all transitions defined by the motion primitives. We use Weighted \astar (\wastar)~\cite{pohl1970heuristic} to find a path in $\calG$ from a given state~$s$ to a goal state $g$. 
\wastar is a suboptimal heursitic search algorithm that allows a tradeoff between optimality and greediness by inflating the heuristic function by a given weight~$w$. 
The search is guided by an efficient and fast-to-compute heuristic function which in our case has two components.
The first component drives the search to intercept the object at the right time and 
the second component guides the search to correct the orientation of the end effector as it approaches the object. 
More formally, our heuristic function is given by
\vspace{-3mm}
$$
 h(s,g) = \max (w \cdot t(s,g), \textsc{AngleDiff}(s,g)).
$$
Here, $t(s,g)$ is the expected time to intercept the object which can be analytically computed from the velocities and positions of the target object and the end-effector and \textsc{AngleDiff}($s,g$) gives the magnitude of angular difference between the end-effector's current pose and target pose.

\paragraph{Planning using experiences}
We now show how previously computed paths which we call ``experiences" can be reused by $\calP$ in our framework. Given a heuristic function $h$ we define for each root path $\Pi$ and each goal state $g \in \Gfull$ the \emph{shortcut} state $\Ssc (\Pi,g)$ as the  state that is closest to~$g$ with respect~$h$.
Namely,
%\vspace{-3mm}
$$
\Ssc (\Pi,g) := \argmin\limits_{s_i \in \Pi} h(s_i, g).
$$
Now, when searching for a path to a goal state $g \in \Gfull$ we add $\Ssc (\Pi,g)$ as a successor for any state along $\Pi$
(subject to the constraint that the path along $\Pi$ to \Ssc is collision free). In this manner we reuse previous experience to quickly reach a state close to the $g$.

\ignore{
\paragraph{Planning using experiences}
We now show how \emph{experience graphs}~\cite{PCCL12} can be used in our framework.
%
Roughly speaking, experience graphs allow a planner  to accelerate its planning efforts whenever possible by using previously-computed paths. The planner gracefully degenerates to planning from scratch if no previous planning experiences can be reused.
%
The key idea is to bias the search efforts, using a specially-constructed heuristic function (called the ``e-graph heuristic''), towards finding a way to get onto the previously-computed paths and to remain on them rather than explore new regions as much as possible. 

In our setting, we use a simplified version of the aforementioned approach which is faster to compute.
%
The key insight is that in our setting, we always start at \Shome which is the first state on all root paths. Thus, we only need to bias the search to stay on a root path (and we don't need to bias the search efforts to get onto the previously-computed paths).
%
To this end, given a heuristic function $h$ we define for each root path $\Pi$ and each goal state $g \in \Gfull$ the \emph{shortcut} state $\Ssc (\Pi,g)$ as the   state that is closest to~$g$ with respect~$h$.
Namely,
$$
\Ssc (\Pi,g) := \argmin\limits_{s_i \in \Pi} h(s_i, g).
$$
Now, when searching for a path to a goal state $g \in \Gfull$ we
(i)~add $\Ssc (\Pi,g)$ as a successor for any state along $\Pi$
(subject to the constraint that the path along $\Pi$ to \Ssc is collision free)
and
(ii)~update our heuristic function to bias the search to use root paths. Specifically, for any state $s$ on the root path $\Pi$ the heuristic is given by
$$
h(s,g) = \min(h(s,\Ssc (\Pi,g)) + h(\Ssc (\Pi,g),g), \varepsilon \cdot h(s,g)).
$$
Here, $\varepsilon>1$ is a penalty term that biases the search to find a path via \Ssc.
}

\subsubsection{Algorithm}
We are finally ready to describe our algorithm describing first the preprocessing phase and then the query phase.

\paragraph{Preprocessing}
\begin{figure*}[t]
    \centering
    \begin{subfigure}{.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{2_compute_root_paths_1}
        \caption{}
        \label{fig:crp1}
    \end{subfigure}
    \hspace{4mm}
    \begin{subfigure}{0.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{2_compute_root_paths_2}
        \caption{}
        \label{fig:crp2}
    \end{subfigure}
    \hspace{4mm}
    \begin{subfigure}{0.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{2_compute_root_paths_3}
        \caption{}
        \label{fig:crp3}
    \end{subfigure}
    \caption{\CaptionTextSize
    First step of the preprocessing stage.
    (\subref{fig:crp1})~A goal state $g_1$ is sampled and the root path $\Pi_1$ is computed between \Shome and $g_1$.
    %
    (\subref{fig:crp2})~The set $G_1 \subset \Gfull$ of all states that can use $\Pi_1$ as an experience is computed and associated with $\Pi_1$.
    %
    (\subref{fig:crp3})~The goal region covered by four root paths from \Shome after the first step of the preprocessing stage terminates.
    }
    \label{fig:crp}
    \vspace{-5mm}
\end{figure*}
Preprocessing step uses $\calP$ both with or without using an experience. It starts by sampling a goal state~$g_1 \in \Gfull$ and computing a root path~$\Pi_1$ from~$\Shome$ to~$g_1$. We then associate with~$\Pi_1$ all goal states~$G_1 \subset \Gfull$ such that~$\Pi_1$ can be used as an experience in reaching any state in $g_1' \in G_1$ within time~$\Tbound^e =  \Tbound - T_\textrm{const}$. 
Thus, all goal states in~$G_1$ are covered by~\Shome.
%
We then repeat this process but instead of sampling  a goal state from \Gfull, we sample from $\Gfull \setminus \Gcov$, where~\Gcov is the set of goal states already covered by \Shome.
At the end of this step, we obtain a set of root paths. 
Each root path~$\Pi_i$ is associated with a goal set $G_i \subseteq \Gfull$ such that 
(i)~$\Pi_i$ can be used as an experience in reaching any state in $g_i' \in G_i$ in~$\Tbound^e$ and 
(ii)~$\bigcup_i G_i = \Gfull$.
%
See Fig.~\ref{fig:crp}.and  Alg.~\ref{alg:step1}, respectively.




\begin{algorithm}[t]
\caption{\textsc{ComputeRootPaths}($s_{\textrm{start}}, \Guncov$)}
\label{alg:step1}
    \AlgFontSize
\begin{algorithmic}[1]
\State $\Psi \leftarrow \emptyset$   \Comment{A list of pairs ($\Pi_i, G_i)$}
\State $\Guncovp \leftarrow \emptyset$; \hspace{3mm}
       $\Gcovp \leftarrow \emptyset$; \hspace{3mm}
       $i = 0$
\While{$\Guncov \neq \emptyset$}
        \Comment{Runs until all uncovered points are considered}
    \State $g_i \leftarrow$\textsc{Sample}($\Guncov$)
    \State $\Guncov \leftarrow \Gcov \setminus \{g_i\}$
    
    \State $\Pi_i \leftarrow$ \textsc{Plan}($s_{\textrm{start}}, g_i$)
        \Comment{Plans root path}
    \If {$\Pi_i = \emptyset$}  \Comment{Path does not exist}
        \State $\Guncovp \leftarrow \Guncovp \cup \{g_i\}$
    \Else
        \State $G_i \leftarrow \{ g_i \}$
        \For {\textbf{each} $g_j \in \Guncov$}
            \State $\pi_j \leftarrow$\textsc{Plan}($s_{\textrm{start}},g_j,\Pi_i$)
                \Comment{Plans using root path as experience}
            \If {$\pi_j \neq \emptyset$} \Comment{Planner succeeded}
                \State $G_i \leftarrow G_i \cup \{g_j\}$
                \State $\Guncov \leftarrow \Guncov \setminus \{g_j\}$
            \EndIf
        \EndFor
        \State $\Psi \leftarrow \Psi \cup \{ (\Pi_i, G_i)\}$
        \Comment{Insert $(\Pi_i, G_i)$ to $\Psi$}
        \State $i \leftarrow i + 1$
    \EndIf
\EndWhile
\State \textbf{return} $\Psi, \Guncovp$
\end{algorithmic}
\end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
\caption{\textsc{PreprocessMain(\Shome, \Gfull)}}\label{alg:all}
    \AlgFontSize
\begin{algorithmic}[1]
    \State \textsc{Preprocess}($\Shome, \Gfull$)
\end{algorithmic}
\end{algorithm}
%
\begin{algorithm}[t]
\caption{\textsc{Preprocess}($\Sstart,\Guncov,\Gcov $)}\label{alg:preprocess}
    \AlgFontSize
\begin{algorithmic}[1]
\State $\Psi_{\textrm{work}}, G'^{\textrm{uncov}}_{\textrm{work}} \leftarrow$ \textsc{ComputeRootPaths}($\Sstart,\Guncov$)

\If {$\Sstart = \Shome$}
    \State $\Psi_{\textrm{home}} = \Psi_{\textrm{work}}$
\EndIf

\State $G'^{\textrm{cov}} \leftarrow 
    \Gcov \cup (\Guncov \setminus G'^{\textrm{uncov}}_{\textrm{work}})$
\If{$t(s_{\textrm{start}}) \geq \Trc$}
    \State \textbf{return} $G'^{\textrm{uncov}}_{\textrm{work}}, G'^{\textrm{cov}}$
\EndIf
\For {\textbf{each} $(\Pi_i, G_i) \in \Psi_{\textrm{work}}$}
    \State $G_i^{\textrm{cov}} \leftarrow G_i$;
            \hspace{2mm}
           $G_i^{\textrm{uncov}} \leftarrow G'^{\textrm{cov}} \setminus G_i$;
            \hspace{2mm}
           $t = \Trc$
%    \State $t = \Trc$
%    \State $G_i^{\textrm{UNCOV}} \leftarrow G'^{\textrm{COV}} \setminus G_i$
%    \State $G_i^{\textrm{COV}} \leftarrow G_i$

    \While{$t \geq t(\Sstart)$}
        \State $s \leftarrow$ \textsc{GetState($\Pi_i, t$)}
        \For {\textbf{each} $(\Pi_j, G_j) \in \Psi_{\textrm{home}}$}
        \label{alg:preprocess:line:latch1a}
            \If{\textsc{CanLatch}($s,\Pi_j$)}
                \State $G_i^{\textrm{uncov}} \leftarrow G_i^{\textrm{uncov}} \setminus G_j$
                \State $G_i^{\textrm{cov}} \leftarrow G_i^{\textrm{cov}} \cup G_j$
                \label{alg:preprocess:line:latch1b}
            \EndIf
        \EndFor
        % }
        %%%%%%%%% LATCHING END
        \If{$G_i^{\textrm{uncov}} = \emptyset$}
            \State \textbf{break}
        \EndIf
        \State $G_i^{\textrm{uncov}},G_i^{\textrm{cov}} \leftarrow$ \textsc{Preprocess}($s,G_i^{\textrm{uncov}},G_i^{\textrm{cov}}$)
        \If{$G_i^{\textrm{uncov}} = \emptyset$}
            \State \textbf{break}
        \EndIf
        \State $t \leftarrow t - \delta_t$
    \EndWhile
    % \State $(\Pi_i,\Pi_j).s = s$ \Comment{replan state}
\EndFor
\State \textbf{return} $G'^{\textrm{uncov}}_{\textrm{work}}, G'^{\textrm{cov}}$

\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Equipped with a set of root paths and the corresponding goal regions they cover, we now need to allow for efficient replanning. This is done by using the following two observations:
\begin{enumerate}[label={\textbf{O\arabic*}},leftmargin=0.75cm]
    \item \label{obs:1} 
    Assume that the robot is executing a path $\Pi_i$ to $g_i \in G_i$ and that $\Pi_{s, i \rightarrow j}$ is a path starting at state $s \in \Pi_i$ and ending at goal $g_j \in \Gfull \setminus G_i$.
    If the last update given by the perception system happens $\Tbound + \delta_t$ before $s$ is reached,
    then the system can execute path $\Pi_i$ until $s$ and then continue executing $\Pi_{s, i \rightarrow j}$ to reach  $g_j$.

    \item \label{obs:2} 
    Assume that the robot is executing a path $\Pi_i$ to $g_i \in G_i$ and that $\pi_{s,s',i \rightarrow j}$ is a path connecting  $s \in \Pi_i$ to $s' \in \Pi_j$ (with~$\Pi_j$ the root path to $g_j \in G_j$).
    If the last update given by the perception system happens $\Tbound + \delta_t$ before~$s$ is reached,
    and the new goal is in $G_j$,
    then the system can execute path~$\Pi_i$ until~$s$, execute $\pi_{s,s',i \rightarrow j}$ and then continue executing~$\Pi_{j}$ from~$s'$ until the goal is reached.

    %\os{do we need to add a figure???}
\end{enumerate}

%\subsubsection{Procrastination is a bliss}
%Recall that in the straw man algorithm presented in Sec.~\ref{subsec:strawman}, given a path $\Pi$ connecting \Shome to some goal $g$, we computed a path to every goal state $\Gfull \setminus \{g\}$ from all replanable states.
%
\ref{obs:1} implies that if we can cover a goal state $g'$  by some state~$s$ along $\Pi_g$ (with $g \neq g'$), then we cover $g'$ by all states $\Pi$ that occur before~$s$.
%
\ref{obs:2} implies that if we can compute a path connecting one root path to some other root path, a process we term as ``latching'' on to the new path, then the new root path can be used to reach all its associated goal states.

We are now ready to describe the second step of our preprocessing stage.
%
For every root path $\Pi_i$ we look at the last replanning state $s_{\Pi_i, \Trc}$ (namely, the state that is $t=\Trc$ time from \Shome). For every other root path $\Pi_j$, we test if the motion (a dynamically generated primitive) connecting $s_{\Pi_i, \Trc}$ to $s_{\Pi_j, \Trc + \delta_t}$ (the state on $\Pi_j$ that is $\Trc+\delta_t$ away from \Shome) is valid (i.e. is collision free and reachable in time). 
%
If this is the case then all goal states in $G_j$ are covered by all replanning states along~$\Pi_i$
See Alg.~\ref{alg:preprocess} lines~\ref{alg:preprocess:line:latch1a}-\ref{alg:preprocess:line:latch1b}.
%

Let $\Guncov(\Trc)$ be all the states that are still uncovered after the above process. We recursively apply our algorithm to the setting where the start state is $s_{\Pi_i, \Trc}$ and the goal region is~$\Guncov(\Trc)$.
If all states are covered after this step, we terminate. 
If not, let $\Guncovp(\Trc)$ be all the states still uncovered.
We consider the state $s_{\Pi_i, \Trc-\delta_t}$ (the state on $\Pi_j$ that is $\Trc-\delta_t$ away from \Shome) and recursively run our algorithm where the start state is $s_{\Pi_i, \Trc-\delta_t}$ and the goal region is $\Guncovp(\Trc)$.
This process is repeated until either all states are covered or we backtracked to $s_{\Pi_i, 0}$ i.e the first state on $s_{\Pi_i}$.
See Fig.~\ref{fig:pl} and  Alg.~\ref{alg:all} and~\ref{alg:preprocess}, respectively.

Thus, the outcome of the preprocessing stage is map $\calM: S \times \Gfull \rightarrow \{ \Pi_1, \Pi_2, \ldots \}$ mapping which root path can be as an experience to reach a goal $g$ from a state $s$.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}{.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_1}
        \caption{}
        \label{fig:pl1}
    \end{subfigure}
    \hspace{1mm}
    \begin{subfigure}{0.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_2}
        \caption{}
        \label{fig:pl2}
    \end{subfigure} 
    \hspace{1mm}
    \begin{subfigure}{0.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_3}
        \caption{}
        \label{fig:pl3}
    \end{subfigure}
    %
    \hspace{1mm}
    \begin{subfigure}{.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_4}
        \caption{}
        \label{fig:pl4}
    \end{subfigure}
    %\hspace{2mm}
    \begin{subfigure}{0.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_5}
        \caption{}
        \label{fig:pl5}
    \end{subfigure}
    \hspace{1mm}
    \begin{subfigure}{0.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_6}
        \caption{}
        \label{fig:pl6}
    \end{subfigure}
    \hspace{1mm}
    \begin{subfigure}{0.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_7}
        \caption{}
        \label{fig:pl7}
    \end{subfigure}
    \caption{\CaptionTextSize
    Preprocess loop.
    (\subref{fig:pl1})~We start by trying to latch on to every other root path. 
    %
    (\subref{fig:pl2})~For successful latches, we add the corresponding goal states to our covered region.
    %
    (\subref{fig:pl3})~We then try to compute new root path with its corresponding goal states (see also Fig.~\ref{fig:crp2}).
    %
    (\subref{fig:pl4})-(\subref{fig:pl6})~This process is repeated by backtracking along the root path.
    %
    (\subref{fig:pl7})~Outcome of a preprocessing step for one path. \Gfull is covered either by using $\Pi_1$ as an experience, 
    latching on to $\Pi_2$ or  $\Pi_3$ (at different time steps)
    or by 
    using newly-computed root paths. 
    }
    \label{fig:pl}
    %\vspace{-5mm}
\end{figure*}

\paragraph{Query}
In the query stage, given an initial estimation $g_{\rm init}$ of the goal pose from the perception system, our algorithm queries $\calM$ to obtain the root path $\Pi$ from $\Shome$ to $g_{\rm init}$.
It then plans a path using $\Pi$ as an experience and starts executing this path.
%
Now, assume that the robot is executing path $\pi_{\rm curr}$ and a new estimation $g_{\rm new}$ of the goal pose is provided by the perception system.
%
We consider the state $s_{\pi_{\rm curr}, \Trc}$ along the path $\pi_{\rm curr}$ at time $\Trc$ and test if
(i)~there is a root path that can be used as an experience to reach $g_{\rm new}$ starting at $s_{\pi_{\rm curr}, \Trc}$ 
(Alg.~\ref{alg:query}, lines~\ref{alg:query:line:c2a}-\ref{alg:query:line:c2b})
or if 
(ii)~there is a root path starting at \Shome associated with~$g_{\rm new}$ that we can latch on to.
If the former holds, we run our experience-based planner to obtain a path $\pi(s_{\pi_{\rm curr}}, \Trc, g_{\rm new})$ starting at $s_{\pi_{\rm curr}},\Trc$ and ending at $g_{\rm new}$. We then execute $\pi_{\rm curr}$ until $s_{\pi_{\rm curr}, \Trc}$ and then continue by executing $\pi(s_{\pi_{\rm curr}}, \Trc, g_{\rm new})$.
%
If the latter holds, we run our experience-based planner to obtain a path $\pi_\textrm{home}$, the path starting from $\Shome$ and ending at $g_{\rm new}$. We then execute $\pi_{\rm curr}$ until $s_{\Pi_{\rm curr}, \Trc}$ and then continue by executing the motion that latches on to the state on $\pi_\textrm{home}$ at~$\Trc + \delta_t$ and finally executing~$\pi_\textrm{home}$ completely.
See Alg.~\ref{alg:query}, lines~\ref{alg:query:line:c3a}-\ref{alg:query:line:c3b}.
%
If neither hold, we consider the state $s_{\Pi_{\rm curr}, \Trc-\delta_t}$ and repeat this process.
%
For pseudocode describing our approach see~\ref{alg:query}.

\begin{algorithm}[t]
\caption{\textsc{Query}($g, \pi_{\textrm{curr}},s_{\textrm{start}}$)}\label{alg:query}  
    \AlgFontSize
\begin{algorithmic}[1]
%    \State $s_{\rm first} \leftarrow \textsc{Head}  \pi_{\textrm{curr}}$ 
%        \Comment{First state on $ \pi_{\textrm{curr}}$}
%        \label{alg:query:line:c1a}
%    \State $\Pi_{\textrm{curr}} \leftarrow$ $\calM(s_{\textrm{start}},g)$ 
%         \Comment{Lookup root path from $s_{\textrm{start}}$ to $g$}
%    \State $\pi \leftarrow$ \textsc{Plan}($s_{\textrm{start}},g,\Pi_{\textrm{curr}}$)
%        \Comment{Plan using root path as experience}
%    \If{$\pi \neq  = \emptyset$}
%        \State \textbf{return} $\pi$
%        \label{alg:query:line:c1b}
%    \EndIf
%\vspace{2mm}

\State $t \leftarrow \Trc$; \hspace{3mm} $t_{\textrm{curr}}\leftarrow \textsc{CurrentTime} + \Tbound$
\While{$t \geq t_{\textrm{curr}}$}
    \State $s \leftarrow$ \textsc{GetState}($\pi_{\textrm{curr}}, t$)
        \Comment{Get state on path at time $t$}
        \label{alg:query:line:c2a}
    \State $\Pi_{\textrm{next}} \leftarrow$ $\calM(s,g)$ 
         \Comment{Lookup root path from $s$ to $g$}
    \If{$\Pi_{\textrm{next}} \neq \emptyset$}
        \State $\pi_{\textrm{next}} \leftarrow$\textsc{Plan}($s_{\textrm{start}},g,\Pi_{\textrm{next}}$)
         \Comment{Plan using root path as experience}
        \State $\pi \leftarrow$ \textsc{MergePaths}($\pi_{\textrm{curr}},\pi_{\textrm{next}},t$)
        \State \textbf{return} $\pi$
        \label{alg:query:line:c2b}
    \EndIf
%%%%%%%%%%%%%%%%LATCHING 

\vspace{2mm}

%    {\color{blue}
    \State $\Pi_{\textrm{home}} \leftarrow \calM (\Shome,g)$
     \Comment{Lookup root path from  to $g$}
     \label{alg:query:line:c3a}
    \If{$\Pi_{\textrm{home}} \neq \emptyset$}
        \If{\textsc{CanLatch}($s,\Pi_{\textrm{home}}$)}
            \State $\pi_{\textrm{home}} \leftarrow$\textsc{Plan}($s_{\textrm{start}},g,\Pi_{\textrm{home}}$)
            \Comment{Plan using root path}
            \State $\pi \leftarrow$ \textsc{MergePathsByLatching}($\pi_{\textrm{curr}},\pi_{\textrm{home}}, t$)
            \State \textbf{return} $\pi$
            \label{alg:query:line:c3b}
        \EndIf
    \EndIf
 %  }
%%%%%%%%%%%
    \State $t \leftarrow t - \delta_t$
\EndWhile
\State \textbf{return failure}
\end{algorithmic}
\end{algorithm}




\subsection{Analysis}
In this section we prove that our algorithm is correct and present an asymptotic analysis of the time and space complexity of our algorithm for the worst-case and its completeness property.

\subsubsection{Correctness}
To show that our algorithm is correct, we prove the following lemma.

\vspace{2mm}
\begin{lemma}
For a start state~$s$ and a goal~$g$, if~$g$ is \emph{reachable} from~$s$ and~$t(s) \leq t_{rc}$, the algorithm is guaranteed to find a path from~$s$ to~$g$.
\end{lemma}

\begin{proof}
In order to prove it we show that (1) if~$g$ is reachable from~$s$, it is \emph{covered} by~$s$ in Alg. 3 and (2) if~$g$ is covered by~$s$, Alg. 4 is guaranteed to return a path.

\begin{enumerate}
    \item Alg. 3 starts by computing a set of \emph{root paths} from the robot's initial state that ensures that it covers all of its reachable goals. It then iterates over all states on these paths and adds additional root paths ensuring that these states also cover their reachable goals. It does it recursively until no start state is left with uncovered goals.
    
    In this manner, Alg. 3 ensures that $g$ is covered by $s$, provided that it is reachable from $s$ and $t(s) \leq t_{rc}$. It covers~$g$ via at least one state on~$\Pi$ between~$s$ and the state at~$t_{rc}$ (inclusive) (While loop at line 9).
    % \item Alg. 3, by construction, is guaranteed to cover~$g$ via at least one state on~$\Pi$ between~$s$ and the state at~$t_{rc}$ (inclusive) (While loop at line 9), provided~$g$ is reachable from~$s$ and~$t(s) \leq t_{rc}$.
    \item Alg. 4 iterates through all states on~$\Pi$  between~$s$ and the state at~$t_{rc}$ (inclusive) to identify the one that covers~$g$ (While loop at line 7). Since~$g$ is covered by at least one of these states by Alg. 3, Alg. 4 finds a path from $s$ to $g$ using the corresponding root path.

\end{enumerate}
\end{proof}

%%%%%%%%%%%%%%%%%%%%
%\label{lemma:completeness}
%Let $s$ be a state that the robot is at in the query phase and $g\in \Gfull$ is a goal state.
%If~$g$ is reachable from a state $s'$ that is (i)~reachable from $s$ and (ii)~is \Tbound away from $s$ then the planner will compute a path to $g$.
%\end{lemma}
%\begin{proof}
%To prove this lemma, we need to show that if~$g$ is reachable from~$s'$, it is ``covered" by~$s'$ in Alg. 7.
%
%We know that if~$g$ is covered by a state~$s$ on a root path~$\Pi_i$, it is also covered by all states before~$s$ on~$\Pi_i$. Hence, to show that~$g$ is covered by~$s'$, we can either show that it is covered by~$s'$ directly or it is covered via a state after~$s'$ on~$\Pi_i$. If either of these holds true, the loop at Alg. 8, line 7 will not terminate without returning the path. (Here Alg. 8 is called for~$(g, \pi_i,s')$).
%
%%Consider an iteration of loop at Alg. 7, line 7. The algorithm maintains the invariant that~$Guncov_i$ for~$\Pi_i$ is initialized with the reachable states of~$\sStart$ and therefore contains the reachable states for all states along~$\Pi_i$ (including~$g$).
%%
%Say~$s'$ lies on the root path~$\Pi_i$. To process~$\Pi_i$, Alg. 7 starts at the state at~$t_{rc} (\geq t(s') > t(\sStart))$ and loops backwards on~$\pi_{\text{curr}}$ until it reaches~$\sStart$ which is the first state on~$\Pi_i$ (Loop at line. 9). Consider the case when~$g$ was not covered via any of the states after~$s'$ on~$\pi_{\text{curr}}$. Then the iteration for which~$s = s'$ will cover~$g$ either by latching or by calling Preprocess(). The coverage is guaranteed by the call to Preprocess (if not covered via latching) because the ComputeRootPaths() function (called inside Preprocess()), by construction, guarantees coverage of all reachable states.
%\end{proof}

\subsubsection{Time Complexity (Query Phase)}


\label{para:time}
In this section we analyse the worst-case time complexity of the query phase.

\begin{lemma}
\label{lemma:o1_full}
	The algorithm can find full path from start to goal in~$O(1)$ time
\end{lemma}
\begin{proof}
	The number of times the algorithm queries for a root path (namely, computing $\calM()$ is bounded by $n_{\textrm{steps}} = \Trc/\delta_t$ which is the maximum number of time steps from $t = 0$ to $\Trc$ and is a constant for a given~$\delta_t$.
Quering a root path requires a hash table lookup. Standard hash tables have~$O(n)$ worst-case complexity. In case of a static input set (which holds true in our case), we can leverage perfect hashing~\cite{czech1997perfect} to achieve~$O(1)$ lookup complexity.
    The number of times the algorithm will attempt to latch on to a root path (namely, a call to \textsc{CanLatch}  which is a constant-time operation) is also bounded by $n_{\textrm{steps}}$. Finally, Alg.~\ref{alg:query} calls the \textsc{Plan} method only once.
    Since we are using a deterministic planner, the computation time is fixed $\Tbound^e$ (i.e. time bound of the experience-based planner).
    Hence the overall time complexity of the query phase is constant i.e~$O(1)$.
\end{proof}

\textbf{CTMP Criteria:} We now analyse the algorithm under the CTMP criteria described in chapter~\ref{sec:ctmp}. Since the algorithm requires~$O(1)$ time to plan the full path, it would also require~$O(1)$ time to get the next action along the path. Therefore, $T_\textrm{const} (< \Tbound)$ is constant-time bounded. Furthermore our experienced-based planner runs a wA* search which is guaranteed to return a path with no cycles. Hence from lemma~\ref{lemma:o1_full} and by Def.~\ref{def:complete} we have the following theorem.

\vspace{2mm}
\begin{theorem}
	The algorithm is CTMP-Complete.
%	By Def.~\ref{ctmp:def} and from lemmas~\ref{lemma:o1} and~\ref{lemma:greedy}, the algorithm is a CTMP algorithm.
\end{theorem}

%\vspace{2mm}
%\begin{cor}
%	By Def.~\ref{ctmp:def} and from lemma~\ref{lemma:o1_full}, the algorithm is a CTMP algorithm.
%\end{cor}
%
%\paragraph{Ammortized:} It is important to mention that the worst-case complexity for our algorithm is too pessimistic, since the hash tables have~$O(n)$ complexity when all the elements hash to the same bucket which is very unlikely if we choose a good hash function. The ammortized complexity of hash lookups, however is~$O(1)$. Since we have already showed that the other operations in the query phase are constant time, the overall ammortized complexity of query phase is~$O(1)$.


\ignore{
\begin{lemma}[Completeness]
\label{lemma:completeness}
Let $s$ be a state that the robot is at in the query phase and $g\in \Gfull$ is a goal state.
If~$g$ is reachable from a state $s'$ that is (i)~reachable from $s$ and (ii)~is \Tbound away from $s$ then the planner will compute a path to $g$.
\end{lemma}
%We omit the proof due to lack of space.

\begin{lemma}[Time boundedness]
\label{lemma:bounded_time}
Let $s$ be a replanable state and $g$ a goal state provided by the perception system.
If~$g$ is reachable, the planner is guaranteed to provide a solution in bounded time.
\end{lemma}

\begin{proof}
    We have to show that the query stage (Alg.~\ref{alg:query}) has a constant-time complexity. 
    The number of times the algorithm queries for a root path (namely, computing $\calM()$  which is a hash look-up that is a constant-time operation) is bounded by $n_{\textrm{steps}} = \Trc/\delta_t$ which is the maximum number of time steps from $t = 0$ to $\Trc$. 
    The number of times the algorithm will attempt to latch on to a root path (namely, a call to \textsc{CanLatch}  which is a constant-time operation) is also bounded by $n_{\textrm{steps}}$. Finally, Alg.~\ref{alg:query} calls the \textsc{Plan} method only once.
    Since we are using a deterministic planner, the computation time is constant. 
    Hence the overall complexity of Alg.~\ref{alg:query} is $O(1)$.
\end{proof}
}

The analysis provided in section~\ref{para:time} highlights the tradeoff our algorithm takes.
If we use a fine discretization of states along the path (namely, we choose a small value for $\delta_t$) then the guaranteed  planning time \Tbound is going to be high but there is a higher chance that any goal $g$ will be reachable.
On the other hand, if we use a course discretization (namely, we choose a large value for $\delta_t$) then we reduce~\Tbound at the price that some goal states may not be reachable.
%




\ignore{
Before describing how this is done, consider two root paths $\Pi_i$ and $\Pi_j$ with associated goal regions $G_i$ and~$G_j$, respectively.
Now, let $s_i$ and $s_j$ be states on $\Pi_i$ and $\Pi_j$, respectively, such that the timestamp associated with $s_j$ is $\delta_t$ time after the one associated with $s_i$. Furthermore, assume that the path between $s_i$ and $s_j$ is collision free. 
%
Now, assume that the robot is executing path $\Pi_i$ (targeting a goal state in $G_i$) and the perception system updates the goal state to be reached as $g_j \in G_j$. 
If the robot did not yet reach $s_i$ then it can reach $g_j$ by
(i)~continuing to follow $\Pi_i$ until $s_i$ is reached, 
(ii)~move to $s_j$ on $\Pi_j$ and 
(iii)~use $\Pi_j$ to reach $g_j$.
%
We term the process we just described of moving from one root path to another as ``latching'' on to a new root path.

Let $s_{\Pi_i, t}$ be the state that is $t$ time from \Shome on path $\Pi_i$.
If a collision-free path existed from $s_{\Pi_i, \Trc}$ to $s_{\Pi_j, \Trc+\delta_t}$ for every $i,j$ then we could latch on from any root path to any other root path. Moreover, following Assumption~\ref{assum:2:2}, $\Trc$ is the last time that the perception could update the goal location so no other replanning would be required.
%
Unfortunately, this may not be the case.
%
Thus, for every root path $\Pi_i$, we consider $s_{\Pi_i, \Trc}$ and check if we can latch on to all other root paths. 
If this can't be done, then we can tr

considering the last replanning state $s_{\Pi_i, \Trc}$ (namely, the state that is $t=\Trc$ time from \Shome). For every other root path $\Pi_j$, we test if the path connecting $s_{\Pi_i, \Trc}$ to $s_{\Pi_j, \Trc + \delta_t}$ (the state on $\Pi_j$ that is $\Trc+\delta_t$ away from \Shome) is collision free. 

In the straw man algorithm this was obtained by recursively computing a path for the replanning states along all the previously-computed paths.
%
Here, we attempt to re-use the previously-computed paths as much as possible by ``latching'' onto them.




More formally, for each root path~$\Pi_i$, we start by considering the last replanning state $s_{\Pi_i, \Trc}$ (namely, the state that is $t=\Trc$ time from \Shome). For every other root path $\Pi_j$, we test if the path connecting $s_{\Pi_i, \Trc}$ to $s_{\Pi_j, \Trc + \delta_t}$ (the state on $\Pi_j$ that is $\Trc+\delta_t$ away from \Shome) is collision free. 
If this is the case we know that any goal state in $G_j$ can be
}

\subsubsection{Space Complexity}
The space complexity analysis is done for the worst case. Although the preprocessing algorithm computes paths backwards for the replanable states i.e starting from the states at~$t_{rc}$, for the purpose of analysis we will follow the straw man approach~(detailed in section~\ref{subsec:strawman})) of considering replannable states in the forward order. The worst case complexity in both cases is the same.

\textbf{Assumption:}
We make an assumption that for each start~$\Sstart$ and goal region~$\Guncov$,~$k \ll n_{\text{goal}}$ where~$k$ is the number of root paths computed by Alg.~\ref{alg:step1} for~$\Guncov$ containing~$n_{goal}$ goals.


\vspace{2mm}
\begin{lemma}
	The algorithm requires~$O(k_{\text{max}}^{n_{\text{steps}}})$ space
\end{lemma}
\begin{proof}
Let~$k_{\text{max}}$ be the upper bound on the number of paths required to cover~$\Gfull$ from any state the robot can be at. First, the algorithm stores~$k_{\text{max}}$ paths~(worst case) from~$\sStart$ to~$\Gfull$ consuming~$O(k)$ space. Second, for each of the~$k_{\text{max}}$ paths, from the first replannable state it finds~$k_{\text{max}}$ paths consuming a commulative~$O(k_{\text{max}}^2)$ space. Similarly for all replannable states at~$t=t_{rc}$ it takes~$O(k_{\text{max}}^{n_{\text{steps}}})$ space. Adding the space required for replannable states at each time step gives~$O(k + O(k_{\text{max}}^2 + ... +O(k_{\text{max}}^{n_{\text{steps}}})$ complexity. For Big O notation we can ignore the lower exponent terms. Hence the final space complexity is~$O(k_{\text{max}}^{n_{\text{steps}}})$.
\end{proof}

Now we will analyse the space complexity for two special cases:

\begin{enumerate}[label={\textbf{C\arabic*}},leftmargin=0.75cm]
\item \label{c1} When the latching operation is always successful (Alg.~\ref{alg:preprocess} line~\ref{alg:preprocess:line:latch1b})
\item \label{c2} For all replannable states at~$t_{rc}$, the call to Alg.~\ref{alg:preprocess} returns~$G_i^{\textrm{uncov}} = \emptyset$

\end{enumerate}
The analysis is based on Alg.~\ref{alg:preprocess}

\vspace{2mm}
\begin{lemma}
	For case~\ref{c1}, the algorithm requires~$O(k)$ space
\end{lemma}
In this case the algorithm only computes~$k$ paths from~$\sStart = \Shome$ to~$\Gfull$.

\vspace{2mm}
\begin{lemma}
	For case~\ref{c2}, the algorithm requires~$O(k^2)$ space
\end{lemma}
\begin{proof}
	For this case in addition to $O(k)$ space as required by case~\ref{c1}, the algorithm needs to compute~$k$ paths from each replannable state at~$t=t_{rc}$. Since there are~$k$ such states, it requires~$O(k^2)$ space. For big O notation we can ignore the first term and the space complexity is given as~$O(k^2)$.
\end{proof}

The two special cases give a intuitive measure of the amount of memory and precomputation we save by the compression schemes that we utilise in preprocessing. Also note that our original assumption i.e.~$k_{\text{max}} \ll n_{\text{goal}}$, allows us to manage the memory requirement for the exponential space complexity.  

%\ignore{
%\subsubsection{CTMP-Completeness}
%We consider that the planner receives a re-planning request to plan from a state~$s$ to a goal~$g$ while the robot is executing a path~$\Pi$. The algorithm is designed such that it attempts to find the path to~$g$ via any state between~$s$ and the state at $t_{rc}$ on~$\Pi$ (inclusively).
%
%\begin{proof}
%We can prove CTMP-completeness (see definition in Sec.~\ref{sec:ctmp}) by showing that (1) if~$g$ is reachable from~$s$, it is \emph{covered} by~$s$ in Alg. 3. (2) if~$g$ is covered by~$s$, Alg. 4 is guaranteed to return a path within~$\Tbound$.
%
%\begin{enumerate}
%    \item Alg. 3 starts by computing a set of \emph{root paths} from the robot's initial state that ensures that it covers all of its reachable goals. It then iterates over all states on these paths and adds additional root paths ensuring that these states also cover their reachable goals. It does it recursively until no start state is left with uncovered goals.
%    
%    In this manner, Alg. 3 ensures that $g$ is covered by $s$, provided that it is reachable from $s$ and $t(s) \leq t_{rc}$. It covers~$g$ via at least one state on~$\Pi$ between~$s$ and the state at~$t_{rc}$ (inclusive) (While loop at line 9).
%    % \item Alg. 3, by construction, is guaranteed to cover~$g$ via at least one state on~$\Pi$ between~$s$ and the state at~$t_{rc}$ (inclusive) (While loop at line 9), provided~$g$ is reachable from~$s$ and~$t(s) \leq t_{rc}$.
%    \item Alg. 4 iterates through all states on~$\Pi$  between~$s$ and the state at~$t_{rc}$ (inclusive) to identify the one that covers~$g$ (While loop at line 7). Since~$g$ is covered by at least one of these states by Alg. 3, Alg. 4 finds a path from $s$ to $g$ in $T_{\textrm{bound}}$ using the corresponding root path.
%
%\end{enumerate}
%\end{proof}
%}

\subsection{Experimental Results}
\label{sec:eval}

We evaluated our algorithm in simulation and on a real robot. The conveyor speed that we used for all of our results is $0.2m/s$. We used Willow Garage's PR2 robot in our experiments using its 7-DOF arm. The additional time dimension makes the planning problem eight dimensional.


\begin{table*}[t]
\centering
\resizebox{0.75\width}{!}{
\begin{tabular}{|c||c||c|c|c|}
\hline
   & \textbf{Our Method} 
   & \multicolumn{3}{c|}{\wastar}
%   & \multicolumn{3}{c|}{\textsf{E-Graph}}
%   & \multicolumn{3}{c|}{\rrt}
   \\ \hline
   & $T_{b}$ = 0.2 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0
   \\ \hline
Pickup success rate [\%]                   
& 92.0 & 0.0 & 0.0 & 18.0 \\ \hline
Planning success rate [\%]                  
& 94.7 & 4.0 & 17.0 & 19.0 \\ \hline
Planning time [s]
& 0.069 & 0.433 & 0.628 & 0.824 \\ \hline
Planning episodes per pickup 
& 3 & 2 & 2 & 2 \\ \hline
Path cost [s]                         & 10.11        & 8.19          & 8.28          & 7.60         \\ \hline
\end{tabular}}
%\caption{\CaptionTextSize Simulation results. Here $T_b$ denotes the (possibly arbitrary) timebound that the algorithm uses. Note that for our method $T_b = \Tbound$ is the time bound that the algorithm is ensured to compute a plan.}
\label{tab:sim_results}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{0.75\width}{!}{
\begin{tabular}{|c||c|c|c||c|c|c|}
\hline
%   & \textbf{Our Method} 
%   & \multicolumn{3}{c|}{\wastar}
   & \multicolumn{3}{c|}{\textsf{E-Graph}}
   & \multicolumn{3}{c|}{\rrt}
   \\ \hline
%   & $T_{b}$ = 0.2 
%   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   \\ \hline
Pickup success rate [\%]                   
& 0.0 & 0.0 & 80.0 & 0.0 & 0.0 & 18.0 \\ \hline
Planning success rate [\%]                  
& 31.0 & 80.0 & 90.0 & 12.0 & 9.0 & 13.0 \\ \hline
Planning time [s]
& 0.283 & 0.419 & 0.311 & 0.279 & 0.252& 0.197\\ \hline
Planning episodes per pickup 
& 2 & 2 & 2 & 2 & 2 & 2 \\ \hline
Path cost [s]                         & 8.54          & 8.22          & 7.90          & 9.68          & 8.96          & 8.04          \\ \hline
%Expansions                        & 17.57        & 315.50        & 392.81        & 495.78        & 188.24        & 202.63        & 137.02        & 146.54        & 104.75        & 78.64         \\ \hline
\end{tabular}}
\caption{\CaptionTextSize Simulation results. Here $T_b$ denotes the (possibly arbitrary) timebound that the algorithm uses. Note that for our method $T_b = \Tbound$ is the time bound that the algorithm is ensured to compute a plan.}
\label{tab:sim_results}
\end{table*}



\subsubsection{Experimental setup}
\paragraph{Perception system}
In order to pick a known object $o$ moving object along the conveyor the conveyor belt, we need a method to estimate its 3-DoF pose at various locations across the conveyor belt. Thus, we created an ICP-based pose estimation strategy that uses the known 3D model of~$o$ and the initial pose estimate obtained after pre-processing the input point cloud. 
%
The following process is performed at every frame obtained by the perception system. 
We start by removing all points that lie outside $\calB$ which yields only points corresponding to the object of interest. 
This is followed by statistical outlier removal in order to remove any points that have been left in scene after the first step. 
In the final step, we compute the mean of the points left and use that as an initial translation estimate for ICP. 
The rotation estimate is applied in a way that it places the object parallel to its direction of motion along the conveyor. The object's 3D model is transformed with this initial estimate, creating a point cloud and ICP refinement is performed between this point and the pre-processed observed point cloud. The resulting transform is concatenated with the initial estimate to return the final predicted pose. For speeding up ICP, we downsample all point clouds to a leaf size of 15mm.

\paragraph{Sense-plan-act cycle}
We follow the classical sense-plan-act cycle as depicted in Fig.~\ref{fig:tl}.
%
Specifically, 
the perception system captures an image (point cloud) of the object~$o$ at time~$t_{\textrm{img}}$
followed by a period of duration $T_{\textrm{perception}}$ in which the the perception system estimates the pose of $o$.
At time $t_{\textrm{msg}} = t_{\textrm{img}} + T_{\textrm{perception}}$, planning starts for a period of $T_{\textrm{planning}}$ which is guaranteed to be less than~$\Tbound$.
Thus, at $t_{\textrm{plan}} = t_{\textrm{msg}} + T_{\rm planning}$ the planner waits for an additional duration of $T_{\rm wait} = \Tbound - T_{\rm planning}$.
Finally, at~$t_{\textrm{exec}} = t_{\textrm{plan}} + T_{\rm wait}$, the robot starts executing the plan. Note that the goal $g$ that the planner plans for is not for the object pose at $t_{\textrm{img}}$ but its forward projection in time to $t_{\textrm{exec}}$ to account for $T_{\textrm{perception}}$ and $T_{\textrm{bound}}$.
While executing the plan, if we obtain an updated pose estimate, the execution is preempted and the cycle repeats.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/timeline2.pdf}
    \caption{\CaptionTextSize Timeline of the sense-plan-act cycle.}
    \label{fig:tl}
    %\vspace{-5mm}
\end{figure}

\paragraph{Goal region specification}
To define the set of all goal poses~$\Gfull$, we need to detail our system setup, depicted in Fig.~\ref{fig:pe}.
The conveyor belt $\calB$ moves along the $x$-axis from left to right.
We pick a fixed $x$-value termed~\Xexec, such that when the incoming $o$ reaches \Xexec as per the perception information, at that point we start execution.

% We assume that there exists a specific $x$-value, termed~\Xexec, such that when the robot start's executing for the first time, the object's  $x$-value will be in the range $[\Xexec-\varepsilon_\calP, \Xexec]$.
%any forward propagation to $t_{\rm exec}$ time an initial pose estimate is given for an object, its $x$-value is in the range $[\Xexec-\varepsilon_\calP, \Xexec]$.
%
Recall that a pose of an object~$o$ is a three dimensional point~$(x,y,\theta)$ corresponding to the $(x,y)$ location of~$o$ and to its orientation (yaw angle) along $\calB$.
%
$\Gfull$ contains a fine discretization of all possible $y$ and $\theta$ values and $x$ values in  $[\Xexec-\varepsilon_\calP, \Xexec+\varepsilon_\calP]$.
We select $\Gfull$ such that $\varepsilon_\calP = 0.5$ cm, dimension along $y$-axis is 20 cm. The discretization in $x,y$ and $\theta$ is 1.0 cm and 10 degrees respectively.

In the example depicted in Fig.~\ref{fig:pe}, the thick and the thin solid rectangles show the ground truth and estimated poses, respectively at two time instances in the life time of the object.
%
The first plan is generated for the pose shown at $x_{\textrm{exec}}$. During execution, the robot receives an improved estimate and has to replan for it. At this point we back project this new estimate in time using the known speed of the conveyor and the time duration between the two estimates. This back-projected pose (shown as the dotted rectangle) is then picked as the new goal for replanning. Recall that under the assumption ~\ref{assum:2:1} the back projected pose will always lie inside \Gfull.
%
%Recall that we assume that the new estimate will be within the $\pm \varepsilon_\calP$ window around \Xexec, it will always lie within \Gfull. 
%As \Gfull contains all $y$ and $\theta$ values (up to our discretization), our system can handle the errors along those dimensions.



\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/pose_error2.pdf}
    \caption{\CaptionTextSize A depiction of \Gfull-specification on a conveyor belt (overhead view) and perception noise handling. 
    }
    \label{fig:pe}
%        \vspace{-5mm}
\end{figure}

\subsubsection{Results}

Before we report on our system-level results comparing our work with alternative implementation (namely, results from the query stage), we mention that the preprocessing stage took roughly 3.5 hours and the memory footprint following this stage is less than 20 MB.
%
This backs up our intuition that the domain allows for efficient compression of a massive amount of paths in a reasonable amount of preprocessing time. In all experiments, we used $\Trc =3.5$ seconds




\paragraph{Real-robot experiments}
\label{sec:robot_results}
To show the necessity of real-time replanning we performed three types of experiments, 
(E1)~replanning with multiple pose estimates, 
(E2)~first-pose planning from the first object pose estimate 
%(received at approximately 1.5m from the robot's base) 
(E3)~best-pose planning from the late (more accurate) pose estimate. 
%
For each set of experiments, we determined the pickup success rate to grasp the moving object (sugar box) off the conveyor belt. In addition, we report on the perception system's success rate by observing the overlap between the point cloud of the object's 3D model transformed by the predicted pose (that was used for planning) and the filtered input point cloud containing points belonging to the object. 
A high (resp. low) overlap results in an accurate (resp. inaccurate) pose estimate. 
%
We use the same strategy to determine a range for which the perception system's estimates are accurate and use it for best-pose planning. 
%This range is found to be 1.05m to 0.95m from the robot's base on the conveyor. 
Further, for each method, we determine the pickup success rate given that the perception was or wasn't accurate. 

The experimental results are shown in Table \ref{tab:robot_results}. Our method achieves the highest overall pickup success rate on the robot, indicating the importance of continuous replanning with multiple pose estimates. 
First-pose planning has the least overall success rate due to inaccuracy of pose estimates when the object is far from the robot's camera. 
Best-pose planning performs better overall than the first pose strategy, since it uses accurate pose estimates, received when the object is close to the robot. However it often fails even when perception is accurate, due to the limited time remaining to grasp object when it's closer to the robot. 
%We can observe this from the method's lowest pickup success rate among the 3 methods when perception = 1 in \ref{tab:robot_results}.






\begin{table}[t]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
                    & \begin{tabular}[c]{@{}c@{}}Overall \\ Pickup\end{tabular}
                    & Perception 
                    & \begin{tabular}[c]{@{}c@{}}Pickup \\ (Perception = 1*)\end{tabular} 
                    & \begin{tabular}[c]{@{}c@{}}Pickup \\ (Perception = 0*)\end{tabular} \\ \hline
E1          & \textbf{69.23}                                                                      & \textbf{42.31}                   & \textbf{83.33}                                                                             & \textbf{57.14}                                                                             \\ \hline
E2 & 16.00                                                                      & 24.00                   & 66.67                                                                             & 0.00                                                                              \\ \hline
E3   & 34.61                                                                      & 34.62                   & 55.56                                                                             & 23.53                                                                             \\ \hline
\end{tabular}
\caption{\CaptionTextSize Real-robot experiments. Success rate for the three experiments (E1---our method, E2---First-pose planning and E3---Best-pose planning). Perception = 1 is accurate estimate, Perception = 0 is inaccurate}
\label{tab:robot_results}
\vspace{-5mm}
\end{table}

\ignore{
\begin{table*}[t]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
                    & \begin{tabular}[c]{@{}c@{}}Pickup success rate [\%]\\ (Overall)\end{tabular} & Perception success rate [\%]& \begin{tabular}[c]{@{}c@{}}Pickup success rate [\%]\\ (Perception = 1*)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pickup success rate [\%]\\ (Perception = 0*)\end{tabular} \\ \hline
Our method (E1)          & \textbf{9.23}                                                                      & \textbf{42.31}                   & \textbf{83.33}                                                                             & \textbf{57.14}                                                                             \\ \hline
First-pose planning (E2) & 16.00                                                                      & 24.00                   & 66.67                                                                             & 0.00                                                                              \\ \hline
Best-pose planing (E3)   & 34.61                                                                      & 34.62                   & 55.56                                                                             & 23.53                                                                             \\ \hline
\end{tabular}
\caption{Real-robot Experiments}
\label{tab:robot_results}
\end{table*}
}

\paragraph{Simulation experiments}






We simulated the real world scenario to evaluate our method against other baselines. We compared our method with \wastar, \textsc{E-graphs} and \rrt. 
For \wastar and \textsc{E-graphs} we use the same graph representation as our method. 
For \textsc{E-graphs} we precompute five paths to randomly-selected  goals in \Gfull. 
We adapt the \rrt algorithm to account for the under-defined goals. To do so, we sample pre-grasp poses along the conveyor 
%in the reachable workspace with a fine discretization 
and compute IK solutions for them to get a set of goal configurations for goal biasing. 
When a newly-added node falls within a threshold distance from the object, we use the same dynamic primitive that we use in the search-based methods to add the final grasping maneuver. If the primitive succeeds, we return success. We also allow wait actions at the pre-grasp locations.

For any planner to be used in our system, we need to endow it with a (possibly arbitrary) planning time bound to compute the future location of the object from which the new execution will start (see Fig.~\ref{fig:tl}). 
%
If the planner fails to generate the plan within this time, then the robot misses the object for that cycle and such cases are recorded as failures. 
%
We label a run as a ``Pickup success" if the planner successfully replans once after the object crosses the 1.0m mark. The mark is the mean of accurate perception range that was determined experimentally and used in the robot experiments as described in Section \ref{sec:robot_results}.
%
The key takeaway from our experiments (Table~\ref{tab:sim_results}) is that having a known time bound on the query time is vital to the success of the conveyor-pickup task.

Our method shows the highest pickup success rate, planning success rate (success rate over all planning queries) and an order of magnitude lower planning times compared to the other methods. 
The planning success rate being lower than 100\% can be attributed to the fact that some goals were unreachable during the runs. 
%
We tested the other methods with several different time bounds. Besides our approach \textsc{E-graphs} perform significantly well. \rrt suffers from the fact that the goal is under-defined and sampling based planners typically require a goal bias in the configuration space. Another important highlight of the experiments is the number of planning cycles over the lifetime of an object. While the other approaches could replan at most twice, our method was able to replan thrice due to fast planning times.


\ignore{
\begin{table*}[t]
\centering
\begin{tabular}{|c||c||c|c|c||c|c|c||c|c|c|}
\hline
   & \textbf{Our Method} 
   & \multicolumn{3}{c|}{\wastar}
   & \multicolumn{3}{c|}{\textsf{E-Graph}}
   & \multicolumn{3}{c|}{\rrt}
   \\ \hline
   & $T_{b}$ = 0.2 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   \\ \hline
Pickup success rate [\%]                   & 92.00     & 0.00      & 0.00     & 18.00      & 0.00        & 0.00       & 80.00       & 0.00       & 0.00       & 18.00      \\ \hline
Planning success rate [\%]                  & 94.67     & 4.00       & 17.00      & 19.00       & 31.00    & 80.00       & 90.00      & 12.00      & 9.00       & 13.00       \\ \hline
Planning time [s]                    & 0.0689       & 0.4329        & 0.6284        & 0.8241        & 0.2830        & 0.4194        & 0.3112        & 0.2718        & 0.2518        & 0.1966        \\ \hline
Planning episodes per pickup & 3            & 2             & 2             & 2             & 2             & 2             & 2             & 2             & 2             & 2             \\ \hline
Path cost [s]                         & 10.11        & 8.19          & 8.28          & 7.60          & 8.54          & 8.22          & 7.90          & 9.68          & 8.96          & 8.04          \\ \hline
%Expansions                        & 17.57        & 315.50        & 392.81        & 495.78        & 188.24        & 202.63        & 137.02        & 146.54        & 104.75        & 78.64         \\ \hline
\end{tabular}
\caption{Simulation results. Here $T_b$ denotes the (possibly arbitrary) timbound that the algorithm uses. Note that for our method $T_b = \Tbound$ is the time bound that the algorithm is ensured to compute a plan.}
\label{tab:sim_results}
\end{table*}
}


% \begin{table}[t]
% % \centering
%     \resizebox{0.5\textwidth}{!}{ 
%         \begin{tabular}{ l | c c c c}
%           & \algname{WA*} & \algname{E-graphs} & \algname{\rrt} & \textbf{Our method} \\
%          \hline
%          Planning time [ms]& - (-) & - (-) & - (-) & \textbf{- (-)}\\
%          Success rate [$\%$]& - & - & - & \textbf{-}\\
%          No. of replans& - & - & - & \textbf{-}\\
%         \end{tabular}
%     }
%     \caption{Planning times, success rate and number of replanning queries averaged over 100 simulated runs of an object moving on $\calB$.}
%     \label{tab:stats_query}
% \end{table}


\ignore{
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figs/preprocess.png}
    \caption{Preprocessing}
    \label{fig:preprocessing}
\end{figure}
}

%\subsubsection{Algorithm Building Blocks}
%\subsubsection{Preprocessing Phase}
%\subsubsection{Query Phase}
%\subsection{Theoretical Analysis}
%\subsubsection{Completeness}
%\subsubsection{Time Complexity of Query Phase}
%\subsection{Experimental Results}
%\subsubsection{Experimental Setup}
%\subsubsection{Real Robot Experiments}
%\subsubsection{Simulation Experiments}

\newpage
\section{Proposed Work}
In the remainder of this thesis we aim to broaden the applicability of CTMP to semi-structured environments. So far we have developed algorithms for fully static environments with known obstacles and a dynamic environment with a known dynamics model i.e the geometric and motion models of the object in the scene were known. Next, we want to consider environments in which the geometric models will be known but the location or motion model of objects in the scene could be unknown apriori. At the same time, we aim to provide strong guarantees on the planning times. Lastly, we also propose to scale up the capability of the conveyor-pickup task to use multiple arms simultaneously picking up different objects.

\subsection{CTMP with Movable Objects in Static Environments}
Environments like homes are considered semi-structured; Consider a kitchen scenario where most of the obstacles are static like kitchen counters, cabinets, stove etc. However, the movable (non-static) items e.g the daily consumbles like a sugar box, a jam jar or the kitchen utensils may be found at random locations specially when the kitchen is used by humans alongside. An example task could be reaching out for a sugar box in a cabinet while avoiding the jam jar.
The constraint of the environments being static is limiting for the robots to work in such semi-structured environments. We assume that the geometric models of all movable objects are known apriori and we have access to a perfect perception system that will give us accurate poses of all the objects in the robot's workspace.

\subsubsection{Problem Definition}
We build on the problem definition from section~\ref{sec:pdef}. Given a start state~$\sStart$, a goal region~$G \in \calC$, a 2D workspace~$\calW$ and a (small) set of known objects~$\calO = \{o_1, o_2, ..., o_n\}$ that can occupy any location~($x,y,yaw$) in the workspace i.e. $loc(o_i) \in \calW$ where~$o_i \in \calO$, for any query goal~$\sGoal$, we need to find a path from~$\sStart$ to a~$g \in G$ in bounded time.

\subsubsection{Increased Complexity of the Problem}
Let us consider a naive approach to analyse the complexity of this problem. Say each object~$o_i$, can occupy~$n$ different discrete locations in~$\calW$. For a single object, if we were to utilise our proposed algorithm in chapter~\ref{chap:icaps}, it will require preprocessing data for all~$n$ locations and then at the query time, a hash table can be used to lookup to map a given object location to the corresponding preprocessed data. As the number of objects increase, the number of possible joint locations for all the objects will grow exponentially with the number of objects. This approach will quickly make the problem intractable memory and computation-wise.

\subsubsection{Key Idea}
For simplicity, consider a single movable obstacle~$o_m \in \calW$. If our algorithm, in the preprocessing phase, can compute two distinct paths~$\pi^1$ and~$\pi^2$ from~$\sStart$ to a goal~$\sGoal \in G$ satisfying the condition that if one path intersects with~$o$, the other path is guaranteed to not intersect with~$o_m$, then in the query phase, for the same start and goal, for any location of~$o_m$ we have the guarantee that either of the two paths~($\pi^1$ or~$\pi^2$) will be valid and and can be executed. This way we can bound the planning time. A simple 2D illustration of the idea is given in Fig.~\ref{fig:movable}.

\begin{figure*}[t]
    \centering
    \begin{subfigure}{.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{query.pdf}
        \caption{}
        \label{fig:tun1}
    \end{subfigure}
    \hspace{4mm}
    \begin{subfigure}{0.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{path1.pdf}
        \caption{}
        \label{fig:tun2}
    \end{subfigure}
    \hspace{4mm}
    \begin{subfigure}{0.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{tunnel.pdf}
        \caption{}
        \label{fig:tun3}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{path2.pdf}
        \caption{}
        \label{fig:tun4}
    \end{subfigure}
    \caption{\CaptionTextSize
    Steps of computating of pair of paths $(\pi^1,\pi^2)$.
    (\subref{fig:tun1})~A planning problem with a static obstacle $o_s$ and a movable obstacle $o_m$.
    %
    (\subref{fig:tun2})~First, plan $\pi^1$ only avoid $o_s$
    %
    (\subref{fig:tun3})~Second, construct an obstacle tunnel around $\pi^1$ considering $o_m$'s geometry
        (\subref{fig:tun4})~Third, plan $\pi^2$ while avoiding both $o_s$ and the tunnel
    }
    \label{fig:movable}
    \vspace{-5mm}
\end{figure*}

\subsubsection{Sketch of the Approach}
Utilizing the aforementioned idea, we can solve the problem by storing a pair of paths for each query but then again it also becomes intractable memory-wise as the number of queries increase. Using the same insight from our previous algorithms our method will subdivide~$G$ into subregions~$\{G_1,G_2,...,G_n\}$. A subregion $G_i$ will contain goals that can use a pair of root paths ($\Pi_i^1,\Pi_i^2$) for planning using an experience-based planner within the time budget~$\Tbound$. The union of these subregions will completely cover~$G$. In the query phase, given the query goal $g$, our method will lookup for the corresponding pair of root paths and use it to plan for it.

\vspace{2mm}
\textbf{Computing Pair of Root Paths:}
One method of compute the pair of root paths is to compute the first path~$\pi^1$ from~$\sStart$ to~$g$ by not considering the movable obstacle~$o$ in~$\calW$ and only considering the static obstacles. Once~$\pi_1$ is computed, we construct an obstacle tunnel around it such that all locations of~$o$ that can intersect with~$\pi^1$ are contained within the tunnel. We then compute the second path~$\pi^2$ while avoiding this obstacle tunnel. In this manner, the two paths will be sufficiently distant apart such that if the~$o$ intersects with one path, it will surely not interesect the other path and hence one of the paths will be valid and can be traversed safely.

Searching for the pair of root paths that satisfy the aforementioned condition in the full space of all possible paths in~$\calC$ is a challenging problem on its own and will open interesting research questions.

\vspace{2mm}
\textbf{Scaling to~$n$ obstacles:}
Building up on our key idea, one obstacle requires a pair of root paths for a query;~$n$ obstacles would require~$n+1$-tuple of root paths. Hence the number of paths required scale linearly with the number of obstacles.

\subsubsection{Challenges and Caveats}
Following are the research questions that we will have to investigate in this work.
\begin{itemize}
\item How to search in the full space of all possible paths to find~$n+1$ paths that satisfy the aforementioned criteria?
\item How to handle the cases in which~$n+1$ paths do not exist?
\item How to make the entire approach feasible memory and precomputation time wise?
\end{itemize}

\subsection{Conveyor Pickup Task with Multiple Robot Arms}
The algorithm proposed in chapter~\ref{sec:rss} considered one object at a time in the workspace of the robot. Consider a scenario where there are multiple robots working at a conveyor and performing a sorting task. Namely, each robot is looking for a specific object and dropping off in a separate bin. Now because all the robots are working at the same conveyor belt, they have to pick their assigned objects while avoiding objects assigned to other robots. For each robot, these other objects would be considered as obstacles. Since these obstacles are moving, it makes the planning problem even more challenging.

\subsection{Schedule of Proposed Work}
\begin{itemize}
\item Summer 2020
	\begin{itemize}
	\item Formulate and implement an algorithm for semi-static environments
	\item Investigate the assumptions needed to provide CTMP guarantees
	\end{itemize}
\item Fall 2020
	\begin{itemize}
	\item Extend the capability of the conveyor-pickup task to use multiple arms simultaneously grasping multiple objects
	\item Demonstrate the framework on the PR2 robot with two arms
	\end{itemize}
\item Spring 2021
	\begin{itemize}
	\item Do a more rigorous empirical analysis of the planning time bounds
	\item Write and defend thesis
	\end{itemize}
\end{itemize}



\newpage

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
