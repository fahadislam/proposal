\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{2010}{-}% support older LaTeX versions
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{tikz-qtree}
%\usepackage{subfigure}
\usepackage{graphicx}
\usepackage[numbers]{natbib}

% paper1
\usepackage{amsthm,amssymb,amsfonts}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{algpseudocode}
\usepackage{parskip}
\usepackage{subcaption}
%\usepackage{subfig}

%% paragraph
\usepackage{titlesec}
\setcounter{secnumdepth}{4}


\graphicspath{
	{./figs/}
}

\hypersetup{letterpaper,bookmarksopen,bookmarksnumbered,
pdfpagemode=UseOutlines,
colorlinks=true,
linkcolor=blue,
anchorcolor=blue,
citecolor=blue,
filecolor=blue,
menucolor=blue,
urlcolor=blue
}

\DeclareMathOperator*{\argmax}{arg\,max}

%opening
\title{Provably Constant-time Motion Planning}
\author{Fahad Islam\\
The Robotics Institute\\
Carnegie Mellon University\\
\texttt{fi@andrew.cmu.edu}}

\input{macros1.tex}
\input{macros2.tex}
\begin{document}

\maketitle

\begin{center}
\Large{Thesis Proposal}
\vspace{40mm}

\large{
\textbf{Thesis Committee:}

Maxim Likhachev (Chair)\\
Chris Atkeson\\
Oliver Kroemer\\
Siddhartha Srinivasa (UW)\\
Oren Salzman (Technion)\\
}
\end{center}
\newpage

\begin{abstract}
In manufacturing and warehouse scenarios, robots often perform recurring manipulation tasks in structured environments. Fast and reliable motion planning is one of the key elements that ensure efficient operations in such environments. A very common example scenario is of manipulators working at conveyor belts, where they have limited time to pick moving objects and if the planner exceeds a certain time threshold, they would fail to pick the objects up. Similar scenarios are encountered in automated assembly lines. Such time-critical applications spur the need for planners which are \emph{guaranteed} to be fast. To this end we introduce the concept of Constant-time Motion Planning (CTMP); namely, the ability to provably guarantee to generate a full motion plan within a (small) constant time. We then develop several algorithms that fall into the class of constant-time motion planning algorithms.

Specifically, up to now we have developed constant-time motion planning algorithms for two domains; (1) Manipulation for repetitive tasks in static environments (2) Manipulation for the task of picking up moving objects (of known models) off a conveyor belt. For the latter, the robot typically perceives a rough pose estimate viewing from a distance, but it must start moving early on (relying on that rough estimate) to be able to reach the object in time and then adjust its motion in real time as it gets improved estimates.
Our key insight is that since these domains are fairly repetitive, the space in which the robot operates is a very small subset of its configuration space, which allows us to preprocess it exhaustively. The preprocessing step generates a representative set of paths that can be used by search at query time in a way that assures small constant-time planning.
%
For the former domain, we evaluate our algorithm for a mail sorting task in simulation on PR2 and also tested the algorithm on a real truck-unloading robot. For the latter domain, we perform real robot experiments on PR2 working at a conveyor belt.

For the remainder of this thesis, we propose to further study and formally define what constant-time planning implies in practice and what underlying assumptions it entails. We also propose to boost the capability of the conveyor pick-up task by having multiple robot arms simultaneously picking up objects, while still maintaining strong theoretical guarantees on the planning side. This introduces new algorithmic challenges including decision making about which object to assign to which arm and motion synchronization between the arms.
\end{abstract}
\newpage

\tableofcontents
\newpage

\section{Introduction}
\subsection{Motivation}
In industrial settings such as manufacturing and warehouse environments, the robots typically operate in structured environments and perform repetitive tasks. Despite significant advancements in the field of motion planning in the past several decades, a large percentage of warehousing and manufacturing industry still uses robots that run hardcoded routines to perform very specific tasks. The lack of penetration of modern motion planning algorithms at scale can be attributed to the fact that the industry needs systems which are \emph{guaranteed} to be fast and reliable, even at the cost of flexibility that these systems could potentially provide.

In warehouses, robots are widely deployed at fast moving conveyor belts to perform repetitive pick and place or sorting tasks, which gives them a very short time buffer to plan their motion. Failing to plan within that time, the robot would skip objects and affect the throughput of the system. Similarly in assembly lines where multiple robots operate in their respective work stations, an overhead caused by the motion planner at one station can slow down the entire chain. This thesis focuses on such time critical applications and introduces the notion of (small) Constant-time Motion Planning (CTMP). CTMP implies generating full motion plans in provably-bounded short planning times.

This thesis is motivated by the following key observations from the aforementioned domains; (1) The tasks are highly repetitious (2) The  environments are fairly structured. (1) gives us an insight that the operational space of the robot is a very small as compared to its full C-space and (2) implies that the environment model is known in advance. These two aspects allow us to fully preprocess the part of the C-space that the robot operates in while accounting for the known objects that exists in its surroundings.

A naive way of providing constant-time guarantees would be to precompute paths for all possible start and goal states that the planner could be queried for and use a simple hash lookup at query time (assuming the lookup time is constant). However, as the set of start or goal states increase, this approach quickly becomes intractable memory and precomputation time wise. Our method provides a compression scheme and precomputes only a small set of representative paths which can still ``cover" the robot's operational space. Namely, using this set of paths, at query time, our method can solve any query within the robot's operational space in bounded time.

\subsection{Approach}
\label{sec:approach}
So far, we have developed CTMP algorithms for two domains.
\subsubsection{Manipulation for repetitive tasks in static environments}
In this work, we consider the specific case where the start state is fixed and there exists a ``goal region" which contains all possible goals. Consider for example, a typical mailroom scenario where the robot has to pick up mail from a fixed location and sort them in cubby shelves. The start corresponds to the pickup location for the packages and the goal region corresponds to all possible goal poses within the cubby shelves. Note that the cubbies reduce the robot's operational space drastically which makes the preprocessing tractable for our method.

Our key insight is that given an ``attractor state''~$s$ in the goal region, typically there is a large region of states around it for which a greedy search (a search following a potential function) towards~$s$ is collision free. We show that such so called attractor regions can be efficiently computed by using dynamic programming.
Importantly, the runtime-complexity of such a greedy search is bounded and there is no need to perform computationally-complex collision-detection operations. 
This insight allows us to generate in an offline phase a small set of these attractor regions together with a path between each attractor state and the start, ensuring that together these attractor regions completely cover the full goal region.
In the query phase, a path is generated by performing a greedy search from the goal to an attractor state (the one which contains the goal) followed by the precomputed path from the start.

\subsubsection{Manipulation for picking up moving objects off a conveyor belt}
We will refer to this problem as ``conveyor pickup task". For this domain we assume that the geometric models of the target objects are known in advance. The success of manipulation tasks relies heavily on the accuracy of the perception system which often is noisy, especially if the target objects are perceived from a distance. For fast moving conveyor belts, the robot cannot wait for a perfect estimate before it starts execution. In order to be able to reach the object in time it must start moving early on (relying on the initial noisy estimates) and adjust its motion on-the-fly in response to the pose updates from perception. We developed an approach that meets these requirements by providing provable constant-time planning and replanning guarantees.

Again, with the same insight, in the first step we precompute a small set of paths from a fixed start (say drop off location) that can cover the goal region which in this domain is defined in the space of object poses. The goal can be any arbitrary object pose~$(x,y,yaw)$ of the objects. In the second step, we uniformly discretize these paths in time to get a set of states that we call ``replannable'' states. To handle pose updates online, we may need to replan from any of these states to all the goals in the goal region. The algorithm then goes back to the first step treating all the replannable states as new starts and this recursive process continues until all replannable states are taken care of. While the approach has exponential complexity in the number of timesteps from the start to the goal, we drastically save on computation and memory by reusing the first set of paths (from the drop off location to the goal region). Our algorithm guarantees that for any replannable state and a goal, a plan will be generated within a bounded time. Experimentally we observe that the robot fails most of the time without replanning i.e. in case it relies on the first estimate or if it waits for an accurate estimate before starting planning.

\subsection{Expected Contribution}
We expect to make the following contributions in this thesis.
\begin{itemize}
	\item Introduce and formalize the concept of Constant-time Motion Planning (CTMP)
	\item Develop CTMP algorithm for domains with repetitive manipulation tasks in static environments
	\item Develop CTMP algorithm for the conveyor pickup task
	\item Enhance the conveyor pick task using multiple robot arms, simultaneously picking up multiple objects
\end{itemize}

\section{Background}
\subsection{Configuration Space and Motion Planning}
Motion planning algorithms operate in the state space often also referred to as the \emph{configuration space} (C-space) or~$\calC$~\cite{lozano1990spatial}. In this space a state or a configuration of the robot can be uniquely represented as a point which makes it convenient for the motion planners to operate. It can be the~($x,y$) position of the robot or~($x,y,yaw$) if the orientation also matters. For a robot arm the configuration can be composed of all the joint positions. If planning with dynamics, the velocities and times may also be added to the configuration. Similarly higher order derivatives can also be added as per the requirements of the domain. The number of variables in the configuration represents the \emph{dimension} of the C-space. For example, for a 7DoF robot arm, for only considering the joint positions, the dimension of the C-space is seven. The free space~$\Cfree$ contains all the configurations which are valid with respect to some state validity criterion such as violation of obstacle collision constraints or kinematic constraints etc. The obstacle space~$\Cobs$ contains all the configurations which are invalid and so~$\Cobs = \calC \backslash \Cfree$

For the motion planning problem we define a start configuration~$\Sstart$ and a goal set~$\Sgoal$. The goal can be under-specified as the pose of the robot end-effector or the pose of the target object. For instance, given a target object that the robot needs to grasp, the goal can be the grasp pose (i.e. position and orientation of the end-effector) or it can be even more under-specified as the pose of the object, giving the flexiblity to select different grasp poses. The motion planning problem is defined as finding a continuous path~$\tau : [0:1] \rightarrow \Cfree$ s.t. $\tau(0) = \Sstart$ and $\tau(1) \in \Sgoal $. Typically in practice most motion planning algorithms find a path of the form~$[s_0, s_1, s_2,...,s_n] \in \Cfree$ s.t. $s_0 = \Sstart$ and $s_n \in \Sgoal$ assuming that the consecutive states i.e $s_i, s_{i+1}$ can be connected via some simple interpolation scheme.

Computing~$\Cfree$ is extremely hard, specially in higher dimensions. Instead of doing that, most motion planners generally sample in~$\Cfree$ and use a collision checker to identify the validity of a configuration. These planners typically sample states in~$\Cfree$ using rejection sampling, connect those samples to construct a graph (or a tree) and then use graph search to find the path from a start to goal. These algorithms are called sampling-based algorithms. A motion planner is referred to as \emph{complete} if it guarantees to find a path if one exists, and otherwise returns failure. Sampling-based planners are \emph{probablisitically complete}, meaning that in the limit of the number of samples they are guaranteed to find a solution if one exists.

\subsection{C-space representation for Search-based Planning}
\label{sec:sbp}
Search-based planning methods descritize the C-space into cells. This descritization relies generally relies on a grid-based or lattice-based structure. A cell is the smallest unit of this discrete space and represents a small volume of C-space states that lie within it. A representative state within a cell, commonly its geometric center is picked to denote a vertex for that cell. All the vertices~$\calV$ connect to their neighboring vertices through edges~$\calE$ to construct a graph~$\calG = (\calV, \calE)$. These edges may have associated costs. Only those vertices and edges are added to~$\calG$ that are valid. The motion planning problem is thus turned into a graph search problem which can be solved using any graph search algorithim. The search is done on an implicit graph as opposed to explicit graph. Namely, instead of precomputing the entire graph which is infeasible for large domains, the graph is constructed and evaluated on-the-fly as the search progresses.

The C-space discretization is done at a certain resolution which is a domain dependent parameter. The resolution determines how course or fine the discretization is. Search-based algorithms are attributed as \emph{resolution complete} which means that they will return a path if one exists for the given resolution. Note that it is possible that a solution exists in the C-space and the search-based planner fails to find it because the resolution is not fine enough for that problem.

\subsection{Computational Complexity in Motion Planning}
The general motion planning problem was shown by Reif to be PSPACE-hard~\cite{reif1979complexity}. Later it was shown to be PSPACE-complete by Canny~\cite{canny1988complexity}. Their analysis was based on finding the \emph{exact} solutions. The class of algorithms that go for finding the exact solutions are called combinatorial algorithms or \emph{exact} algorithms~\cite{lavalle2006planning}. These algorithms do not rely on any approximation of the C-space and operate in the continuous C-space. They are also complete with respect to the continuous C-space which is a stronger notion of completeness compared to sampling-based planners or search-based planners which are probablisitically complete and resolution complete respectively.

The dimension~$d$ of the C-space is the most crucial element in determining the complexity of the motion planning problem. It has been shown that for the general motion planning problem the computational complexity grows exponentially with~$d$~\cite{chazelle1991singly}.

The exact motion planning algorithms do not scale to more complex problems as they make certain limiting assumptions about the geometry of the robot and the obstacles. Since the exact methods are no more used in modern days, the computational complexity analysis for the general motion planning problem is of less relevance and the community focusses more on the analysis of the specific algorithms.



\section{Related Work}
\subsection{Preprocessing-based Methods}
Preprocessing-based motion planners often prove beneficial for real-time planning. They analyse the configuration space offline to generate some auxiliary information that can be used online to speed up planning. 

The most reknowned example is the Probablistic Roadmap Method (PRM)~\cite{kavraki1996probabilistic} methods. They have a preprocessing and a query phase. In the preprocessing phase a roadmap is constructed in $\Cfree$ by randomly sampling valid states in the C-space and connecting them to their neighboring states if the edges making the connections are valid. In the query phase the start and goal states are connected to their neighboring states in the roadmap (if valid connections are possible) and then any search algorithm like A* search is used to find the path. PRM methods are categorized as multi-query methods meaning that once the roadmap is constructed it can be used to answer multiple queries for the same environment. Query times can be significantly sped up by further preprocessing the roadmaps using landmarks~\cite{paden2017landmark}. Recently,the repetition roadmap~\cite{LA18} was suggested as a way to extend the \textsf{PRM} for the case of multiple highly-similar scenarios. Some of these methods have been integrated with sparse motion-planning roadmaps (see e.g.,~\cite{SSAH14,DB14}) to reduce the memory footprint of the algorithm. All these methods only provide probablistic completeness guarantees.

Our work bares resemblance to previous work on 
subgoal graphs~\cite{UK17,UK18}.
Subgoal graphs is a search-based method that preprocesses the entire C-space to generate a sparse overlay graph comprised of vertices called sub-goals. The pair of sub-goals connected by an edge are \emph{reachable} and thus searching on this overlay graph is significantly faster than searching on the original graph. The method assures that the path found on this subgoal graph has a path on the original graph and can be quickly computed at query time. As this method requires preprocessing the full C-space it is expensive and only is applicable to lower dimensional planning problems.

While all these preprocessing-based approaches speedup planning times compared to planning from scratch by using the preprocessed information, they do not provide bounds on the planning times.


\subsection{Motion Planning with Reuse}
An alternative approach to address our problem is to precompute a set of complete paths into a library and given a query, attempt to match complete paths from the library to the new query~\cite{berenson2012robot,jetchev2013fast}.
Using paths from previous search episodes (also known as using experience) has also been an active line of work~\cite{PCCL12,PDCL13,BAG12,CSMOC15}. E-graphs method~\cite{PCCL12} computes a heuristic function to guide the search in such a way as to reuse search efforts from previously generated paths. The Lightning framework~\cite{BAG12} runs two modules in parallel, a module that runs the planner from scratch and a module that retrieves and repairs path stored in a library. The Thunder framework~\cite{CSMOC15} also leverages previous experiences and the experiences are generated using probabilistic sampling. The experiences as stored as sparse roadmap spanner~(SPARS). 

These methods also provide significant speedups in planning times, but they still do not provide constant-time planning guarantees which our method aims to provide.

\subsection{Real-time Motion Planning}
In practice, the only class of algorithms that provide constant-time planning guarantees are real-time algorithms~\cite{KL06,KS09,K90}.
To provide guarantees on planning time the search only looks at a finite horizon, generating a partial plan, and interleaves planning and execution. These algorithms run repeated A* searches and update the heuristic values in the local search spaces while maintaining the admissibility of the heuristics. The heuristis get more informed as the search is run repeatedly. By doing so they provide completeness guarantees, meaning that the robot will eventually arrive at the goal.

As opposed to these finite horizon planning algorithms, our method plans for indefinite horizons i.e. all the way from the start to the goal while providing constant-time planning guarantees

\subsection{Online Replanning: Moving Target Search}
The conveyor-planning problem can be modelled as a Moving Target Search problem (MTS) which is a widely-studied topic in the graph search-based planning literature~\cite{ishida1991moving,ishida1995moving,koenig2007speeding,sun2010moving}. 
These approaches interleave planning and execution incrementally and update the heuristic values of the state space to improve the distance estimates to the moving target. Unfortunately, in high-dimensional planning problems, this process is computationally expensive which is why these approaches are typically used for two-dimensional grid problem such as those encountered in video games.

\subsection{Global Control using Local Potential Functions}
Finally, our notion of attractor states is similar (in spirit) to control-based methods that  ensure safe operation over local regions of the free configuration space~\cite{CRC03,CCR06,tedrake2010lqr}. Metaphorically they name these regions as ``funnels" that collapse a large set of input conditions to a single policy,
These regions are then used within a high-level motion planner to compute collision-free paths. LQR-trees~\cite{tedrake2010lqr} computes local stability regions to build a sparse tree of LQR-stabilized trajectories. The union of these local regions covers the entire state space.

\subsection{Motion Planning for Conveyor Pickup Task}
Existing work on picking moving objects has focused on different aspects of the problem ranging from closed-loop controls to object perception and pose estimation, motion planning and others~\cite{allen1993automated, han2019toward, stogl2017tracking, zhang2018gilbreth}. 
%
Here, we focus on motion-planning related work. Graph-searched based approaches have been used for the motion-planning problem~\cite{cowley2013perception, menon2014motion}. The former uses a kinodynamic motion planner to smoothly pick up moving objects i.e., without an impactful contact. A heuristic search-based motion planner that plans with dynamics and could generate optimal trajectories with respect to the time of execution was used. While this planner provides strong optimality guarantees, it is not real-time and thus cannot be used online.
%
The latter work was demonstrated in the real world showing real-time planning capability. While the planner is fast, it does  pure kinematic planning and does not require collision checking with the target object. The approach plans to a pregrasp pose and relies on Cartesian-space controllers to perform the pick up. The usage of the Cartesian controller limits the types of objects that the robot can grasp.

\section{Constant-time Motion Planning (CTMP)}
\label{sec:ctmp}
In this thesis we introduce the concept of Constant-time Motion Planning (CTMP). We develop algorithms that will provably guarantee constant query-time complexity. Constant-time complexity implies that regardless of the problem size or the input size, the algorithm demands the same computation time.

\subsection{Input size of the Algorithm}
As described in section~\ref{sec:conveyor}, our approach operates in discretized search spaces. The size of the input to the query phase of our algorithm is determined by the size of the discretized operational space of the robot. 
To prove that the algorithm has a constant time complexity, we need to show that its computation time is bounded and the bound is independent of the input size.
%In our problem setting our input to the algorithm is the full configuration space of the robot~$\calC$. The size of~$\calC$ by definition is unbounded since it is a continuous space. We will also consider the case in section~\ref{sec:conveyor} where we consider a discretized search space as highlighted in section~\ref{sec:sbp}. In that case the input is finite is defined by the size of the underlying implicit graph~$\calG$. 

\subsection{(Small) Constant-time Algorithm}
More formally an algorithm with input size~$n$ has a constant time complexity if~$T(n)$ is bounded by a value that does not depend on~$n$.
Note that the running time does not have to be independent of~$n$, but an upper bound for the running time has to be bounded independently of~$n$. In Big O notation, a constant-time algorithm has an asymptotic complexity of~$O(1)$. In the proposed constant-time algorithms we also aim that~$T(n)$ of the algorithm is small as to have real-time performance.

\subsubsection{A Naive Constant-time Algorithm}
Refering to the problem setting described in section~\ref{sec:approach}, given a goal region~$G$ with a finite set of goals, the naive approach would be to precompute and store paths to each goal in~$G$ in a preprocessing stage and use a hash table mapping goals to paths and use it to answer any query goal~$g \in G$ in the query stage. Under the assumption that there are no hash collisions, the hash lookup is considered a constant-time operation and hence the algorithm will have a constant-time complexity. However, this approach becomes intractable when the number of queries becomes large. This is because the memory requirement or the precomputation time becomes very large for computational resources.

\subsubsection{Proposed Approach}
The naive approach puts all the computation burden on the proprocessing phase and requires minimum computation time but a large memory during the query phase. On the other end of the spectrum we can just plan from scratch online every time and eliminate the preprocessing stage, shifting all the burden to the query phase. This would require minimal memory but the high computational complexity. We propose a middle ground where we can balance the offline-online burden in such a way that we can provide provable (small) constant time planning guarantees. Instead of having to precompute paths to all the goals in~$G$, we propose compression schemes to precompute and store only a small set of representative paths that can be used in query phase in such a way that assures (small) constant time planning. In the following two sections we will theoretically show that our algorithms run in constant-time and experimentally show that the planning times are small.


\section{CTMP for Repetitive Tasks in Static Environments}
\subsection{Algorithm Framework}
\label{sec:alg}
In this section we describe our algorithmic framework. We start (Sec.~\ref{sec:pdef}) by formally defining our problem and continue (Sec.~\ref{sec:key}) by describing the key idea that enables our approach.
We then proceed (Sec.~\ref{subsec:alg}) to detail our algorithm and conclude with implementation details (Sec.~\ref{subsec:impl}).

\subsubsection{Problem formulation and assumptions}
\label{sec:pdef}
Let $\calX$ be the configuration space of a robot operating in a static environment containing obstacles.
We say that a configuration is valid (invalid) if the robot, placed in that configuration does not (does) collide with obstacles, respectively.
We are given in advance a start configuration~$\Sstart \in \calX$ and some goal region~$G \subset \calX$.
We emphasize that the goal region may contain invalid configurations.
In the query phase we are given multiple queries $(\Sstart, s_{\text{goal}})$ where $s_{\rm goal} \in G$ is a valid configuration and for each query, we need to compute a collision-free path connecting $\Sstart$ to $s_{\text{goal}}$.

Coming back to our motivating example of mail sorting---the start configuration would be some predefined configuration above the cart where the robot can pick up the envelopes from and the goal region would comprise of all possible placements of the robot's end effector in the cubbies. The environment is static as the only obstacles in the environment are the shelves and the cart which remain stationary in between queries.

We discretize $\calX$ into a state lattice $\calS$ such that any state~$s \in \calS$ is connected to a set of successors and predecessors via a mapping Succs/Preds: $\calS \rightarrow 2^\calS$.
Define $G_\calS := \calS \cap G$ to be the states that reside in the goal region. Note that although our approach is applicable to general graphs (directed or undirected), to be able to reuse the planned path in the reverse direction (e.g. in our motivating example for the motion from the shelve to the start configuration) the graph needs to be undirected.
We make the following assumptions:

\begin{enumerate}[label={\textbf{A\arabic*}}]
  \item \label{assum:1} $G_\calS$ is a relatively small subset of $S$. Namely, it is feasible to exhaustively iterate over all states in $G_\calS$.
However, storing a path from $\Sstart$ to each state in $G_\calS$ is infeasible.
  
  \item \label{assum:2} The planner has access to a heuristic function $h: \calS \times \calS \rightarrow \mathbb{R}$ which can estimate the distance between any two states in $G_\calS$. Moreover, 
 \begin{itemize}
  \item The heuristic function should be \textit{weakly-monotone} with respect to $G_\calS$, meaning that $\forall s_1, s_2  \in G_\calS$ where $s_1 \neq s_2 $, it holds that,
  \begin{center}
    $h(s_1, s_2) \geq \min\limits_{s_1' \in \text{Preds}(s_1)} h(s_1', s_2)$.
  \end{center}

  \item The heuristic function $h$ should induce that the goal region is \emph{convex} with respect to $h$, meaning that $\forall s_1, s_2  \in G_\calS$ where $s_1 \neq s_2 $, it holds that,
  \begin{center}
     $\argmin\limits_{s_1' \in \text{Preds}(s_1)} h(s_1', s_2) \in G_\calS$.
  \end{center}

 \end{itemize}
 Namely, for any distinct pair of states ($s_1, s_2$) in $G_\calS$, at least one of $s_1$'s predecessors has a heuristic value less than or equal to its heuristic value.
 Moreover, the predecessor with minimal heuristic value lies in the goal region.


  \item \label{assum:3} The planner has access to a tie-breaking rule that can be used to define a total order \footnote{A total order is a binary relation on some set which is anti-symmetric, transitive, and a convex relation.} over all states with the same heuristic value.
  % to a given state.
  \end{enumerate}

These assumptions allow us to establish strong theoretical properties regarding the efficiency of our planner. Namely, that
within a known bounded time, we can compute a collision-free path from $\Sstart$ to any state in $G_\calS$. 


Assumption~\ref{assum:2} may seem too restrictive (especially convexity), imposing that the goal region cannot be of arbitrary structure. However, after we detail our algorithm (Sec.~\ref{subsec:alg}) and analyze its theoretical properties (Sec.~\ref{sec:analysis}), we sketch how we can relax this assumption to be less restrictive.


 
\subsubsection{Key idea}
\label{sec:key}
Our algorithm relies heavily on the notion of a greedy search.  Thus, before we describe of our algorithm, we formally define the terms greedy predecessor and greedy search.

\vspace{2mm}
\begin{definition}
\label{def:greedy-suc}
  Let $s$ be some state and $h(\cdot)$ be some heuristic function.
  A state $s' \in \text{Preds}(s)$ is said to be a \emph{greedy} predecessor of $s$ according to $h$ if it has the minimal $h$-value among all of $s$'s predecessors.
\end{definition}
Note that 
if $h$ is weakly monotone with respect to $G_\calS$
(Assumption~\ref{assum:2}) 
and we have some tie-breaking rule
(Assumption~\ref{assum:3}), then every state has a greedy predecessor in the $G_\calS$ and it is unique.
In the rest of the text, when we use the term greedy predecessor, we assume that it is unique and in $G_\calS$.

\vspace{2mm}
\begin{definition}
  Given a heuristic function $h(\cdot)$,
  an algorithm is said to be a \emph{greedy search}  with respect to $h$ if for every state it returns its greedy predecessor according to $h$.
\end{definition}
Note that we define the greedy search in terms of the predecessors and not the successors to account for the directionality of the graph. This will become more clear in Sec.~\ref{subsec:alg}.

\textbf{Remark:} Now that we have the notion of greedy search, we can better explain our definition of convexity with respect to~$h$ (Assumption~\ref{assum:2});
this assumption ensures that a greedy search between any pair of states lies within the goal region, analogously to the standard notion of a convex region where for every pair of points within the region, every point on the straight line segment that joins the pair of points is also within the region.

Our key insight is to precompute in an offline phase subregions within the goal region where a greedy search to a certain (``attractor'') state is guaranteed to be collision free and use these subregions in the query phase.
Specifically, in the preprocessing phase, $G_\calS$ is decomposed into a finite  set of (possibly overlapping) subregions $\calR$.
Each subregion $R_i \in \calR$ is a hyper-ball defined using a center which we refer to as the ``attractor state''~
\sAttract and a radius $r_i$.
These subregions, which may contain invalid states, are constructed in such a way that the following two properties hold
\begin{enumerate}[label={\textbf{P\arabic*}}]
  \item \label{property:1} For any valid goal state $s_{\text{goal}} \in R_i \cap G_\calS$, a greedy search with respect to $h(s, \sAttract)$ over $\calS$ starting at $\sGoal$ will result in a collision-free path to \sAttract.
  \item \label{property:2} The union of all the subregions completely cover the valid states in $G_\calS$. 
      Namely, $\forall s \in G_\calS~\text{s.t.}~s~\text{is valid}, \exists R \in \calR \ s.t. \ s \in R$.
\end{enumerate}

In the preprocessing stage, we precompute a library of collision-free paths $\calL$ which includes a path from $\Sstart$ to each attractor state. 
In the query phase, given a query~\sGoal, we 
(i)~identify a subregion $R_i$ such that $\sGoal \in R_i$ (using the precomputed radii~$r_i$),
(ii)~run a greedy search towards~\sAttract by greedily choosing at every point the predecessor that minimizes~$h$ and
(iii)~append this path with the precomputed path in $\calL$ to $\Sstart$ to obtain the complete plan.
For a visualization of our algorithm, see Fig.~\ref{fig:approach}.


\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{approach.pdf}
    % \vspace{-2mm}
  \caption{
  Visualization of approach. Subregions are depicted in green, 
  goal region~$G$ is depicted in tiled gray  containing obstacles (red).
  Precomputed paths from $s_{\rm{start}}$ to attractor states are depicted in blue.
 Given a query $(s_{\rm{start}}, s_{\rm{goal}})$, the returned path is given by $\pi_i$ appended with a greedy path ~$\pi_g$ from $s_{\rm{goal}}$ to \sAttract (reversed), which is depicted in purple.
}
    \label{fig:approach}
% \vspace{-6mm}
\end{figure}

\subsection {Algorithm}
\label{subsec:alg}
\subsubsection{Preprocessing Phase}
The preprocessing phase of our algorithm, detailed in Alg.~\ref{alg:1}, takes as input the start state~$\Sstart$, the goal region~$G_\calS$ and a conventional motion planner $\calP$, and outputs a set of subregions~$\calR$ and the corresponding library of paths~$\calL$ from~\Sstart to each~\sAttract. 

The algorithm covers~$G_\calS$ by iteratively finding a state $s$ not covered\footnote{Here, a state $s$ is said to be covered if there exists some subregion $R \in \calR$ such that $s \in R$.} by any subregion and computing a new subregion centered at $s$.
To ensure that $G_\calS$ is completely covered (Property~\ref{property:2}) we maintain a set~$V$ of valid (collision free) and a set $I$ of invalid (in collision) states called \emph{frontier states} (lines~\ref{alg:1:v} and~\ref{alg:1:i}, respectively).
We also construct subregions centered around invalid states~$\hat{\calR}$ to efficiently store which invalid states have been considered.
We start by initializing~$V$ with some random state in $G_\calS$ and iterate until both~$V$ and~$I$ are empty, which will ensure that $G_\calS$ is indeed covered even if $G_\calS$ is not fully connected.

At every iteration, we pop a state from $V$ (line~\ref{alg:1:pop}), and if there is no subregion covering it, we add it as a new attractor state and compute a path $\pi_i$ from $\Sstart$ (line~\ref{alg:1:path}) using the planner $\calP$.
We then compute the corresponding subregion (line~\ref{alg:1:cr} and Alg.~\ref{alg:2}).

As we will see shortly, computing a subregion corresponds to a Dijkstra-like search centered at the attractor state.
The search terminates with the subregion's radius $r_i$ and a list of frontier states that comprise of the subregion's boundary.
The valid and invalid frontier states are then added to $V$ and $I$, respectively (lines~\ref{alg:1:insert_v} and~\ref{alg:1:insert_i}).


Once $V$ gets empty the algorithm starts to search for states which are valid and yet uncovered by growing subregions around invalid states popped from~$I$ (lines~\ref{alg:1:iv_loop}-\ref{alg:1:iv_region} and detailed in Alg.~\ref{alg:3b}). If a valid and uncovered state is found, it is added to $V$ and the algorithm goes back to computing subregions centered at valid states (lines~\ref{alg:1:x_states}-\ref{alg:1:break}), otherwise if $I$ also gets empty, the algorithm terminates and it is guaranteed that each valid state contained in $G_\calS$ is covered by at least one subregion.

\begin{algorithm}[t]
%\footnotesize
\hspace*{\algorithmicindent} \textbf{Inputs:} $G_\calS$, $\Sstart$, $\calP$
\Comment{goal region, start state and a planner} 

\hspace*{\algorithmicindent} \textbf{Outputs:} 
$\calR, \calL$
%$R_i = (\sAttract, r_i)$, $\pi_i$,
\Comment{subregions and corresponding paths to $\Sstart$}

%\hspace*{\algorithmicindent}
% \textbf{Parameters:} $r_{\text{max}}$   \Comment{maximum radius of a region}
\caption{Goal Region Preprocessing}\label{alg:1}
\begin{algorithmic}[1]
\Procedure{PreprocessRegion}{$G_\calS$}
%\Procedure{PreprocessRegion}{$G, r_{\text{max}}$}
  \State $s \leftarrow$\textsc{ SampleValidState}($G_\calS$)
  \State $V \leftarrow \{ s \}$   \Comment{valid frontier states initialized to random {state}} \label{alg:1:v}
  \State $I$ = $\emptyset$   \Comment{invalid frontier states} \label{alg:1:i}
  \State $ i \leftarrow 0$
       \hspace{2mm} 
       $\calL = \emptyset$
       \hspace{2mm} 
       $\calR = \emptyset$
       \hspace{2mm} 
       $\hat{\calR} = \emptyset$
       
  \vspace{2mm}
    \While {$V$ and $I$ are not empty}
        \While {$V$ is not empty}
          \State $s \leftarrow V.\text{pop}()$ \label{alg:1:pop}
            \If {$\nexists R \in \calR$  s.t. $s \in R$ }  \label{alg:1:discard}
            \Comment{$s$ is not covered}
        \State $\sAttract \leftarrow s$               
        \label{alg:1:attract} 
                \State $\pi_i$ = $\calP.$\textsc{PlanPath}($\Sstart, \sAttract$); \label{alg:1:pp}
                \hspace{2mm }
                $\calL \leftarrow \calL \cup \{ \pi_i \}$  \label{alg:1:path}
                \State $(\text{OPEN}, r_i) \leftarrow$ \textsc{ComputeReachability}($\sAttract$) \label{alg:1:cr}
%                \State $(\text{OPEN}, r_i) \leftarrow$ \textsc{ComputeReachability}($\sAttract, r_{\text{max}}$) \label{alg:1:cr}
                \State insert Valid(OPEN) in $V$  \label{alg:1:insert_v}
                \State insert Invalid(OPEN) in $I$   \label{alg:1:insert_i}
                \State $R_i$ $\leftarrow$ $(\sAttract, r_i)$; 
                \hspace{2mm} $ i \leftarrow i+1$        \hspace{2mm }
                $\calR \leftarrow \calR \cup \{ R_i \}$
                            \EndIf
        \EndWhile

\vspace{2mm}        
        
        \While {$I$ is not empty} \label{alg:1:iv_loop}
            \State $s$ $\leftarrow$ $I.pop()$
      \If {$\nexists R \in \calR \cup \hat{\calR}$ s.t. $s \in R$ }      \Comment{$s$ is not covered}
\State $(X, r)$ $\leftarrow$ \textsc{FindValidUncoveredState}($s$)
%                \State $(X, r)$ $\leftarrow$ \textsc{FindValidUncoveredState}($s, r_{\text{max}}$)
                \State $\hat{R}$ $\leftarrow$ $(s,r)$;
        \hspace{2mm}
        $\hat{\calR} \leftarrow \hat{\calR} \cup \{ \hat{R} \}$ \Comment{invalid subregion}  \label{alg:1:iv_region}
                \If {$X$ is not empty}  \Comment{valid state found}  \label{alg:1:x_states}
                    \State insert $X$ in $V$
                    \State \textbf{break} \label{alg:1:break}
                \EndIf
            \EndIf
        \EndWhile
    \EndWhile

  \vspace{2mm}

  \State \Return $\calR, \calL$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{figure}[tb]
  \centering
%    \includegraphics[width=0.3\textwidth]{alg2.pdf}
    % \vspace{-2mm}
  \caption{
  Visualization of Alg~\ref{alg:2}. Subregion $R_i$ (green) grown from $\sAttract$ in a goal region~$G$ (tiled grey) containing an obstacle (red).
  Frontier states and first state not in $R_i$ are depicted by circles and a cross, respectively.
}
    \label{fig:alg2}
% \vspace{-6mm}
\end{figure}

\subsubsection{Reachability Search}
The core of our planner lies in the way we compute the subregions (Alg.~\ref{alg:2} and Fig.~\ref{fig:alg2}) which we call a ``Reachability Search''. The algorithm maintains a set of \emph{reachable} states $S_{\text{reachable}}$ for which Property~\ref{property:1} holds.
As we will see, this will ensure that in the query phase, we can run a greedy search from any reachable state $s \in S_{\text{reachable}}$ and it will terminate in the attractor state. 
%
The following recursive definition formally captures the notion of a reachable state.

\vspace{2mm}
\begin{definition}
  Given some attractor state \sAttract, we say that a state $s \in G_\calS$ is reachable under some function $h(\cdot)$ with respect to \sAttract if either
  (i)~$s = \sAttract$ or
  (ii)~the greedy predecessor of $s$ with respect to $h(s,\sAttract)$ is reachable.
\end{definition}

The algorithm computes a subregion that covers the maximum number of reachable states that can fit into a hyper-ball defined by $h(s,\sAttract)$. 
The search maintains a priority queue OPEN ordered according to $h(s,\sAttract)$. Initially, the successors of $\sAttract$ are inserted in the OPEN (line~\ref{alg:2:OPEN}). For each expanded successor, if its valid greedy predecessor is in $S_{\text{reachable}}$, then the successor is also labeled as reachable (lines~\ref{alg:2:crit} and~\ref{alg:2:set}). 

The algorithm terminates when the search pops a state which is valid but does not have a greedy predecessor state in $S_{\text{reachable}}$ (line~\ref{alg:2:terminate}). Intuitively, this corresponds to  the condition when the reachability search exits an obstacle (see Fig.~\ref{fig:alg2}).
At termination, all the states within the boundary of radius $r_i$ (excluding the boundary) are reachable.


\begin{algorithm}[t]
%\footnotesize
\caption{Reachability Search}\label{alg:2}

\begin{algorithmic}[1]
\Procedure{ComputeReachability}{$\sAttract$}
%\Procedure{ComputeReachability}{$\sAttract, r_{\text{max}}$}
\State $S_{\text{reachable}} \leftarrow \{\sAttract\}$ \Comment{reachable set} \label{alg:2:reachable}
\State OPEN $\leftarrow \{$Succs($\sAttract$)$\}$  \Comment{key: $h(s,\sAttract)$} \label{alg:2:OPEN}
\State CLOSED $\leftarrow \emptyset$
\State $r_i \leftarrow 0$

%\While {$r_i \leq r_{\text{max}}$}
\While{OPEN$\neq \emptyset$}
    \State $s \leftarrow$ OPEN.pop()
    \State insert $s$ in CLOSED
    \State $s'_g \leftarrow \argmin\limits_{s' \in \text{Preds}(s)} h(s', \sAttract)$ 
%    \State $s'_g \leftarrow$ GreedySucc($s$)
  \label{alg:2:greedy} 
 \Comment{greedy predecessor}
 % acc. to some tie-breaking criteria}
    \If {$s'_g$ $\in$ $S_{\text{reachable}}$ and Valid(edge(s,$s'_g$))}  \label{alg:2:crit}
        \State $S_{\text{reachable}} \leftarrow S_{\text{reachable}} \cup \{s\}$  \Comment{$s$ is greedy} \label{alg:2:set}
    \ElsIf {Valid($s$)} \label{alg:2:terminate}
        \State $r_i \leftarrow h(s, \sAttract)$ \label{alg:2:rad}
        \State \Return (OPEN, $r_i$)
%        \State break
    \EndIf
    \For {each $s' \in \text{Succs}(s) \cap G_\calS$} \label{alg:2:prun}
        \If {$s' \notin$ CLOSED}
            \State insert $s'$ in OPEN with priority $h(s', \sAttract)$
        \EndIf
    \EndFor
\EndWhile
\State $r_i \leftarrow h(s, \sAttract) + \epsilon$    \Comment{$\epsilon$ is a small positive constant}
\State \Return (OPEN, $r_i$)

\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
%\footnotesize
\caption{Find valid uncovered state}\label{alg:3b}

\begin{algorithmic}[1]
\Procedure{FindValidUncoveredState}{$\hat{s}$}
\State OPEN $\leftarrow \{\hat{s}\}$
\While{OPEN$\neq \emptyset$}
  \State $s \leftarrow$ OPEN.pop()
  \State insert $s$ in CLOSED
  \If {$\nexists R \in \calR$  s.t. $s \in R$ and Valid(s)}
  % \Comment{$s$ is not covered and $s$ is valid}
    \State \Return ($\{s\}$, $h$($s$, $\hat{s}$))
  \EndIf
  \For {each $s' \in \{\text{Succs}(s) \cup \text{Preds}(s)\} \cap G_\calS$}
    \If {$s' \notin$ CLOSED}
        \State insert $s'$ in OPEN with priority $h(s', \sAttract)$
    \EndIf
  \EndFor
\EndWhile
\State $r = h$($s$, $\hat{s})) + \epsilon$    \Comment{$\epsilon$ is a small positive constant}
\State \Return ($\emptyset$, $r$))
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsubsection{Query Phase}
Given a query goal state $s_{\text{goal}} \in G_\calS$ 
our algorithm, detailed in Alg.~\ref{alg:3}, starts by finding a subregion $R_i \in \calR$ which covers it (line.~\ref{alg:3:covers}). Namely, a subregion~$R_i$ for which 
$h(s_{\text{goal}}, \sAttract) < r_i$.
We then run a greedy search starting from $s_{\text{goal}}$ by iteratively finding for each state $s$ the predecessor with the minimum heuristic $h(s, \sAttract)$ value until the search reaches \sAttract (lines~\ref{alg:3:greedy-call} and~\ref{alg:3:greedy-call-start}-~\ref{alg:3:greedy-call-end}). 
The greedy path~$\pi_g$ is then appended to the corresponding precomputed path~$\pi_i \in \calL$ (line~\ref{alg:3:return}). 
Note that at no point  do we need to perform collision checking in the query phase (given the fact that the environment is static).

\begin{algorithm}[t]
\hspace*{\algorithmicindent} \textbf{Inputs:} $G_\calS, \calL, \calR$

\hspace*{\algorithmicindent} \textbf{Output:} $\pi$	\Comment{full path from \sStart to \sGoal}
%\footnotesize
\caption{Query}\label{alg:3}

\begin{algorithmic}[1]
\Procedure{FindGreedyPath}{$s_1, s_2$}
  \label{alg:3:greedy-call-start}
  \State $s_{\text{curr}} \leftarrow s_2$; \hspace{2mm} $\pi \leftarrow \emptyset$
  \While{$s_{\text{curr}} \neq s_1$}
    \State $\pi \leftarrow \pi \cdot s_{\text{curr}}$
    \Comment{append current state to path}
      \State $s_{\text{curr}} \leftarrow \argmin\limits_{s \in \text{Preds}(s_{\text{curr}})} h(s, s_1)$  \Comment{greedy predecessor}
    \EndWhile
    \State \Return \textsc{Reverse} ($\pi$)   \Comment{reverse to get a path from $s_1$ to $s_2$}
\EndProcedure
\label{alg:3:greedy-call-end}

\vspace{2mm}

\Procedure{ComputePath}{$\sGoal$}
  \For {each $R_i \in \calR$}
    \If {$h(s_{\text{goal}}, \sAttract) < r_i$}
    \Comment{$R_i$ covers $\sGoal$}
    \label{alg:3:covers}
      \State $\pi_g \leftarrow$ \textsc{FindGreedyPath} ($\sAttract, \sGoal$)
      \label{alg:3:greedy-call}
      \State \Return $\pi_i \cdot \pi_g$  \Comment{append $\pi_g$ to~$\pi_i \in \calL$}
      \label{alg:3:return}
    \EndIf
  \EndFor

\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Implementation details}
\label{subsec:impl}

\subsubsection{Ordering subregions for faster queries}
Recall that in the query phase we iterate over all subregions to find one that covers \sGoal. 
In the worst case we will have to go over all subregions.
However,  our algorithm typically covers most of the goal region $G_\calS$ using a few very large subregions (namely, with a large radii~$r_i$) and the rest of $G_\calS$ is covered by a number of very small subregions.

Thus, if we order our subregions (offline) according to their corresponding radii, there is a higher chance of finding a covering subregion faster. While this optimization does not change our worst-case analysis (Sec.~\ref{sec:analysis}), it speeds up the query time in case the number of subregions is very large.

\subsubsection{Efficiently constructing $\calL$}
Constructing paths to the attractor states (Alg.~\ref{alg:1}, line~\ref{alg:1:pp}) can be done using any motion planning algorithm $\calP$.
In our implementation we chose to use RRT-Connect~\cite{KL00}.
Interestingly, this step dominates the running time of the preprocessing step.
%
To improve the preprocessing time, we initially set a small timeout for RRT-Connect to compute a path from an attractor state $\Sstart$ to $\sAttract$.
Attractor states for which RRT-Connect fails to find a path to $\sAttract$ are marked as \textit{bad} attractors and we do not grow the subregions from them. 
These, so-called \textit{bad} attractors are discarded when other subregions cover them.

When Alg.~\ref{alg:1} terminates, we reload the valid list $V$ with the remaining bad attractors and rerun Alg.~\ref{alg:1} but this time with a large timeout for RRT-Connect. 
%
We can also increase the timeout with smaller increments and run Alg.~\ref{alg:1} iteratively until there are no more bad attractors (assuming that there exists a solution for each goal state $\in$ $G_\calS$).


\subsubsection{Pruning redundant subregions}
To reduce the number of precomputed subregions we remove redundant ones after the Alg.~\ref{alg:1} terminates. 
In order to do that, we iterate through all the subregions and remove the ones which are fully contained within any other subregion. 
%Note that this is possible the way Alg.~\ref{alg:1} is designed. 
This step reduces both the query complexity (see Sec.~\ref{sec:analysis}) as well as the memory consumption.

\subsection{Analysis}
\label{sec:analysis}
In this section we formally prove that 
our algorithm is correct (Sec.~\ref{subsec:correct}) and 
analyze its computational complexity, completeness and bound on solution quality (Sec.~\ref{subsec:complexity},~\ref{subsec:completeness} and ~\ref{subsec:quality} respectively).

\subsubsection{Correctness}
\label{subsec:correct}
To prove that our algorithm is correct, we show that indeed all states of every subregion are reachable and we can identify if a state belongs to a subregion using its associated radius.
Furthermore, we show that a path obtained by a greedy search within any subregion is valid and that all states in $G_\calS$ are covered by some subregion.
These notions are captured by the following set of lemmas.

\vspace{2mm}
\begin{lemma}
\label{lemma:reachable-1}
Let $S_{\text{reachable}}$ be the set of states computed by Alg.~\ref{alg:2} for some attractor vertex \sAttract.
%
Every state $s \in S_{\text{reachable}}$ is reachable with respect to \sAttract.
\end{lemma}
%
\begin{proof}
The proof is constructed by an induction over the states added to $S_{\text{reachable}}$.
The base of the induction is trivial as the first state added to $S_{\text{reachable}}$  is \sAttract (line~\ref{alg:2:reachable}) which, by definition, is reachable with respect to \sAttract.
%
A state $s$ is added to $S_{\text{reachable}}$ only if its greedy predecessor is in $S_{\text{reachable}}$ (line~\ref{alg:2:greedy}) which by the induction hypothesis is reachable with respect to \sAttract.
This implies by definition that $s$ is reachable with respect to \sAttract.
%
Note that this argument is true because the greedy predecessor of every state is unique (Assumption~\ref{assum:3}).
\end{proof}

\begin{lemma}
\label{lemma:reachable-2}
Let $S_{\text{reachable}}$ be the set of states computed by Alg.~\ref{alg:2} for some attractor vertex \sAttract.
%
A state $s$ is in $S_{\text{reachable}}$ iff $h(s, \sAttract) < r_i$.
\end{lemma}

\begin{proof}
Alg.~\ref{alg:2} orders the nodes to be inserted to $S_{\text{reachable}}$ according to $h(s, \sAttract)$ (line~\ref{alg:2:OPEN}).
As our heuristic function is weakly monotonic (Assumption~\ref{assum:2}), the value of $r_i$ monotonically increases as the algorithm adds states to $S_{\text{reachable}}$ (line~\ref{alg:2:rad}).
Thus, for every state $s \in S_{\text{reachable}}$, we have that $h(s, \sAttract) < r_i$.

For the opposite direction, assume that there exists a state $s \notin S_{\text{reachable}}$ such that $h(s, \sAttract) < r_i$.
This may be because  Alg.~\ref{alg:2} terminated due to a node $s'$ that was popped from the open list with 
$h(s', \sAttract) \leq h(s, \sAttract)$.
However, using the fact that our heuristic function is weakly monotonic (Assumption~\ref{assum:2}) we get a contradiction to the fact that $h(s, \sAttract) < r_i$.
Alternatively, this may be because we prune away states that are in $G_\calS$  (Alg.~\ref{alg:2} line~\ref{alg:2:prun}).
However, using the fact that our goal region is convex with respect to $h$ (Assumption~~\ref{assum:2}), this cannot hold.
\end{proof}

\begin{lemma}
\label{lemma:greedy}
Let $R_i \in \calR$ be a subregion computed by Alg.~\ref{alg:2}.
for some attractor vertex \sAttract.
% 
A greedy search with respect to $h(s, \sAttract)$  starting from any valid state $s \in R_i$ is complete and valid.
\end{lemma}

\begin{proof}
Given a state $s \in R_i$, we know that $s \in S_{\text{reachable}}$ (Lemma~\ref{lemma:reachable-2})
and that it is reachable with respect to \sAttract (Lemma~\ref{lemma:reachable-1}).
%
It is easy to show (by induction) that any greedy search starting at a state $S_{\text{reachable}}$ will only output states in $S_{\text{reachable}}$.
Furthermore, a state is added to $S_{\text{reachable}}$ only if the edge connecting to its greedy predecessor is valid (line~\ref{alg:2:crit}).
Thus, if $s\in S_{\text{reachable}}$ is valid, the greedy search with respect to $h(s, \sAttract)$  starting from $s$ is complete and valid.
\end{proof}

\begin{lemma}
\label{lemma:coverage}
At the end of Alg.~\ref{alg:1}, every state $s \in G_\calS$ is covered by some subregion $R \in \calR$.
\end{lemma}
\begin{proof}
Assume that this does not hold and let $s \in G_\calS$ be a state that is not covered by any subregion but has a neighbor (valid or invalid) that is covered.
%
If $s$ is valid, then it would have been in the valid frontier states $V$ and either been picked to be an attractor state (line~\ref{alg:1:attract}) or covered by an existing subregion (Alg.~\ref{alg:2}).
%
A similar argument holds if $s$ is not valid.
\end{proof}

From the above we can immediately deduce the following corollary:

\vspace{2mm}

\begin{cor}
  After preprocessing the goal region~$G_\calS$ (Alg.~\ref{alg:1} and~\ref{alg:2}), in the query phase we can compute a valid path for any valid state $s \in G_\calS$ using Alg.~\ref{alg:3}.
\end{cor}


\textbf{Remark}
We can relax Assumption~\ref{assum:2} in two ways.
The first is by explicitly tracking states not in the goal region instead of pruning them away (Alg.~\ref{alg:2} line~\ref{alg:2:prun}).
Unfortunately, this may require the algorithm to store many states not in $G_\calS$  which may be impractical (recall that Assumption~\ref{assum:1} only states that we can exhaustively store in memory the states in the goal region).
An alternative, more practical way, to relax Assumption~\ref{assum:2}  is by terminating the search when we  encounter a state not in the goal region.
This may cause the algorithm to generate much more subregions (with smaller radii) which may increase the memory footprint and the preprocessing times.


\subsubsection{Time Complexity of Query Phase}
\label{subsec:complexity}
The query time comprises of 
(i)~finding the containing subregion $R_i$
(ii)~running the greedy search to $\sAttract$
and
(iii)~lookup operations from preprocessed data.
Step~(i) requires iterating over all subregions (in the worst case) which takes $O(|\calR|)$ steps while 
step~(ii) requires expanding the states along the path from $\sGoal$ to $\sAttract$ which requires~$O(\calD)$ expansions where $\calD$ is the depth (maximal number of expansions of a greedy search) of the deepest subregion. 
For each expansion we need to find the greedy predecessor, considering at most $b$ predecessors, where $b$ is the maximal branching factor of our graph.
We can measure the depth of each subregion in Alg.~\ref{alg:2} by keeping track of the depth of each expanded state from the root i.e., $\sAttract$. We assume that step~(iii) is constant-time operation. Hence, overall the query phase takes $O(|\calR| + \calD \cdot b)$ operations. The maximal query time can also be empirically profiled after the preprocessing phase.

%
\textbf{Remark:}
Note that we can also bound the number of expansions required for the query phase by bounding the maximum depth of the subregions. We can do that by terminating Alg.~\ref{alg:2} when the $R_i$ reaches the maximum depth or if the existing termination condition (line ~\ref{alg:2:terminate}) is satisfied.
Having said that, this may come at the price of increasing the number of subregions.

\textbf{Constant-time Complexity}
We need to show that $O(|\calR| + \calD \cdot b) \rightarrow O(1)$. While~$\calD$ and~$b$ are constants,~$|\calR|$ is not a constant and does not have a bound and so the complexity of the query phase as is is~$O(|\calR|$ which is linear in~$|\calR|$.
Here we propose a modification or extension of the full algorithm (both preprocessing and query phases) which is detailed in section~\ref{sec:extension} which will make the complexity constant time. The extension allows us to specify a constant upper bound on~$|\calR|$.  
Now recall the definition of the constant-time algorithm from section~\ref{sec:ctmp}. Irrespective of the size of input to the algorithm, since ~$|\calR|$ has a constant upper bound and $\calD$ and~$b$ are constants, the query phase has constant-time complexity i.e.~$O(1)$.

%Note that in~$O(|\calR| + \calD \cdot b)$,~$\calD$ and~$b$ are trivially bounded; both are constants. The third term~$|\calR|$ however depends on goal region~$G_\calS$. Roughly speaking,~$|\calR|$ depends on three things~(1) how cluttered~$G_\calS$ is with obstacles, the more cluttered it is the more subregions we will get~(2) the size of~$G_\calS$; a bigger goal region will render more subregions and~(3) the parameter~$\calD$. We make the assumption that for a choice of~$\calD$~(large enough), for certain degree of obstacle clutter in C-space~(less enough) and by our original problem formulation that the goal region is a small subset of the C-space, the upper bound on~$|\calR|$ is bounded independently of the size of~$G_\calS$. Under this analysis we can conclude that the algorithm has a constant time complexity i.e~$O(1)$.

\subsubsection{Algorithm Completeness}
\label{subsec:completeness}
Our method generates plans by stitching together a path from the library $\calL$ (computed offline using some planner~$\calP$), and a path computed online which is returned by the greedy search on a discretized graph $G_\calS$. For the former, our method simply inherits the completeness properties of the planner $\calP$, whereas for the latter, our method is resolution complete; it follows from the correctness discussion (Section~\ref{subsec:correct}). 

\subsubsection{Bound on Solution Quality}
\label{subsec:quality}
Let the function $c(\cdot)$ denote the cost of a path and $c^*(\cdot)$ denote the cost of an optimal path. Assuming that we precompute each path $\pi_i \in \calL$ with the optimal cost $c^*(\pi_i)$, from the triangle inequality it can be trivially shown that the quality of complete path $\pi$ computed by our method has an additive suboptimality bound; i.e.,
 % \begin{center}
 $c(\pi) - c^*(\pi) < 2 * c(\pi_g)$,
 % \end{center}
where $c(\pi_g)$ is the cost of the greedy path from the attractor to the goal state.

\subsection{Algorithm Extension}
\label{sec:extension}
In section~\ref{sec:analysis}, we enforced a bound on the number of subregions to guarantee~$O(1)$ complexity. If we do so Alg.~\ref{alg:1} may terminate without covering the~$G$ fully.
The key idea of the following algorithm extension is to change the method of finding the attractor given a goal from geometric checks~(using radii of subregions) to a hash lookup which is a constant time operation. For doing so, the algorithm needs to pay an extra price in terms of memory, since now it has to store the goals in~$G_\calS$ and associated attractors in a lookup table. The new method differs both in preprocessing and query phases and is explained in this section.

\subsubsection{Preprocessing Phase and Reachabilty Search}
In case the upper bound on the number of subregions is reached in Alg.~\ref{alg:1} before termination, the algorithm switches to Alg.~\ref{alg:grp2}. The key difference is that instead of computing new geometric subregions~$R_i$, the algorithm maintains a lookup table~$\calM$ mapping the goals to their corresponding attractors computed during the reachability search. The modified reachability search is algorithm is detailed in Alg.~\ref{alg:reach2}. The two main differences in the reachability search are that OPEN need not be a priority queue; it can be an unordered queue and the termination condition that was there in Alg.~\ref{alg:2} is removed.

\subsubsection{Query phase}

%% Main Alg

\begin{algorithm}[t]
%\footnotesize
\hspace*{\algorithmicindent} \textbf{Inputs:} $G_\calS$, $\Sstart$, $\calP$
\Comment{goal region, start state and a planner} 

\hspace*{\algorithmicindent} \textbf{Outputs:} 
$\calL$,
$\calM$
%$R_i = (\sAttract, r_i)$, $\pi_i$,
\Comment{Library of paths to $\Sstart$ and a lookup table}
\caption{Goal Region Preprocessing}\label{alg:grp2}
\begin{algorithmic}[1]
\Procedure{PreprocessRegion}{$G_\calS$}
  \State $V \leftarrow$\textsc{ GetAllValidStates}($G_\calS$)
  \State $ i \leftarrow 0$
       \hspace{2mm} 
       $\calL = \emptyset$
       \hspace{2mm} 
       $\calM = \emptyset$
       
  \vspace{2mm}
    %\While {$V$ and $I$ are not empty}
        \While {$V$ is not empty}
          \State $s \leftarrow V.\text{pop}()$ %\label{alg:1:pop}
            %\If {$s$ is not covered}  %\label{alg:1:discard}
           % \Comment{$s$ is not covered}
        \State $\sAttract \leftarrow s$               
        %\label{alg:1:attract} 
                \State $\pi_i$ = $\calP.$\textsc{PlanPath}($\Sstart, \sAttract$); %\label{alg:1:pp}
                \hspace{2mm }
                $\calL \leftarrow \calL \cup \{ \pi_i \}$  %\label{alg:1:path}
                \State $S_{\text{reachable}} \leftarrow$ \textsc{ComputeReachability}($\sAttract$) %\label{alg:1:cr}
                \State insert lookup enteries $S_{\text{reachable}} \rightarrow \sAttract$ in~$\calM$
                \State $V \leftarrow V \backslash S_{\text{reachable}}$;
                \hspace{2mm} $ i \leftarrow i+1$

        \EndWhile

  \vspace{2mm}

  \State \Return $\calL, \calM$
\EndProcedure
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
%\footnotesize
\caption{Reachability Search}\label{alg:reach2}

\begin{algorithmic}[1]
\Procedure{ComputeReachability}{$\sAttract$}
%\Procedure{ComputeReachability}{$\sAttract, r_{\text{max}}$}
\State $S_{\text{reachable}} \leftarrow \{\sAttract\}$ \Comment{reachable set} %\label{alg:2:reachable}
\State OPEN $\leftarrow \{$Succs($\sAttract$)$\}$  %\Comment{key: $h(s,\sAttract)$}% \label{alg:2:OPEN}
\State CLOSED $\leftarrow \emptyset$
\State $r_i \leftarrow 0$

%\While {$r_i \leq r_{\text{max}}$}
\While{OPEN$\neq \emptyset$}
    \State $s \leftarrow$ OPEN.pop()
    \State insert $s$ in CLOSED
    \State $s'_g \leftarrow \argmin\limits_{s' \in \text{Preds}(s)} h(s', \sAttract)$ 
%    \State $s'_g \leftarrow$ GreedySucc($s$)
  %\label{alg:2:greedy} 
 \Comment{greedy predecessor}
 % acc. to some tie-breaking criteria}
    \If {$s'_g$ $\in$ $S_{\text{reachable}}$ and Valid(edge(s,$s'_g$))} % \label{alg:2:crit}
        \State $S_{\text{reachable}} \leftarrow S_{\text{reachable}} \cup \{s\}$  \Comment{$s$ is greedy}% \label{alg:2:set}
    \EndIf
    \For {each $s' \in \text{Succs}(s) \cap G_\calS$}% \label{alg:2:prun}
        \If {$s' \notin$ CLOSED}
            \State insert $s'$ in OPEN with priority $h(s', \sAttract)$
        \EndIf
    \EndFor
\EndWhile
%\State $r_i \leftarrow h(s, \sAttract) + \epsilon$    \Comment{$\epsilon$ is a small positive constant}
\State \Return $S_{\text{reachable}}$

\EndProcedure
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
\hspace*{\algorithmicindent} \textbf{Inputs:} $G_\calS$, $\calM$, $\calL$

\hspace*{\algorithmicindent} \textbf{Output:} $\pi$	\Comment{full path from \sStart to \sGoal}
%\footnotesize
\caption{Query}\label{alg:3}

\begin{algorithmic}[1]

%\Procedure{FindGreedyPath}{$s_1, s_2$}
%  \label{alg:3:greedy-call-start}
%  \State $s_{\text{curr}} \leftarrow s_2$; \hspace{2mm} $\pi \leftarrow \emptyset$
%  \While{$s_{\text{curr}} \neq s_1$}
%    \State $\pi \leftarrow \pi \cdot s_{\text{curr}}$
%    \Comment{append current state to path}
%      \State $s_{\text{curr}} \leftarrow \argmin\limits_{s \in \text{Preds}(s_{\text{curr}})} h(s, s_1)$  \Comment{greedy predecessor}
%    \EndWhile
%    \State \Return \textsc{Reverse} ($\pi$)   \Comment{reverse to get a path from $s_1$ to $s_2$}
%\EndProcedure
%\label{alg:3:greedy-call-end}
%
%\vspace{2mm}

\Procedure{ComputePath}{$\sGoal$}
 % \For {each $R_i \in \calR$}
    %\If {$h(s_{\text{goal}}, \sAttract) < r_i$}
    %\Comment{$R_i$ covers $\sGoal$}
    \State $\sAttract \leftarrow \calM$.\textsc{LookupAttractor}($\sGoal$)
    %\label{alg:3:covers}
      \State $\pi_g \leftarrow$ \textsc{FindGreedyPath} ($\sAttract, \sGoal$)
     % \label{alg:3:greedy-call}
      \State \Return $\pi_i \cdot \pi_g$  \Comment{append $\pi_g$ to~$\pi_i \in \calL$}
     % \label{alg:3:return}
    %\EndIf
  %\EndFor

\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Experimental Results}
\label{sec:eval}

\subsubsection{Mail Sorting Task}

We evaluated our algorithm on the PR2 robot for a single-arm (7-DOF) motion-planning problem. The illustrated task here is to pick up envelopes from a cart and put them in the cubby shelves (see Fig.~\ref{fig:PR2}). Such settings are common in mailroom environments where the robot may have to encounter the same scenario over and over again. The start state is a fixed state corresponding to the pickup location, whereas the task-relevant goal region~$G$ is specified by bounding the position and orientation of the end effector. For this domain, we define~$G$ as a bounding box covering all possible positions and orientations of the end effector within the cubby shelf.

\begin{figure}[tb]
  \centering
    \includegraphics[width=0.6\textwidth]{pr2.png}
    % \vspace{-2mm}
  \caption{
  Motivating scenario---a robot (PR2) picking up objects from shelves and placing them into a bin.
}
    \label{fig:PR2}
% \vspace{-6mm}
\end{figure}

The search is done on an implicit graph $G_\calS$ constructed using $\textit{motion primitives}$ which are small kinematically feasible motions. We define the primitives in the task-space respresentation as small motions for the robot end effector in position axes ($\textit{x, y, z}$) and orientation axes of Euler angles ($\textit{roll, pitch, yaw}$), and a joint-angle motion for the redundant joint of the 7-DOF arm. 
The heurstic function is the Euclidean distance in these seven dimensions. The discretization we use for the graph $G_\calS$ is 2~cm for the position axes, 10 degrees for the Euler axes and 5 degrees for the redundant joint. 
For this domain, we keep the $\textit{pitch}$ and $\textit{roll}$ of the end effector fixed and allow motion primitives along the remaining five dimensions i.e. $\textit{x, y, z, yaw}$ and the reduntant joint angle. We also limit the $\textit{yaw}$ to be between -30 and 30 degrees. Note that the specification of the $G_\calS$ is purely task specific and we exploit the task constraints to limit the size of $G_\calS$ which makes our preprocessing step tractable (Assumption~\ref{assum:1}).

We compared our approach with different single- and multi-query planners in terms of planning times, success rates and memory consumption (see Table~\ref{tab:stats}) for 200 uniformly sampled goal states from $G$. Among the multi-query planners, we implemented PRM, a multi-query version of RRT which we name MQ-RRT and the E-graph planner. 
For MQ-RRT, we precompute an RRT tree rooted at  $\Sstart$ offline (similar to PRM) and query it by trying to connect $\sGoal$ to the nearest nodes of the precomputed tree. We use the same connection strategy for MQ-RRT as the one that the asymptotically-optimal version of PRM uses\footnote{In order for the quality of paths obtained the PRM to converge to the quality of the optimal solution, a query should be connected to its $k$ nearest neighbors where 
$k = e(1+1/d)\log(n)$.
Here $n$ is the number of nodes in the tree/roadmap and $d$ is the dimensionality of the configuration space~\cite{karaman2011sampling,SSH16}. 
}.
For PRM, we precomputed the paths from all the nodes in~$G$ to~$s_{\text{start}}$ (this is analogous to our library~$\calL$). 
The query stage thus only required the connect operation (i.e. attempting to connect to $k$ nearest neighbors of $\sGoal$). For both of these planners we also added a goal-region bias by directly sampling from~$G$, five percent of the time\footnote{We used OMPL~\cite{SMK12} for comparisons with the sampling-based planners and modified the implementations as per needed.}. 

For single-query planning, we only report the results for RRT-Connect as it has the fastest run times from our experience. For PRM and MQ-RRT, if the connect operation fails for a query, we considered that case as a failure. For RRT-Connect, we set a timeout of 10 seconds.

For our method, preprocessing (Alg.~\ref{alg:1}) took 1,445 seconds and returned 1,390 subregions. For precomputing paths (Alg.~\ref{alg:1}, line~\ref{alg:1:pp}), we use RRT-Connect. For the first run of Alg.~\ref{alg:1}, we set the timeout to be 10 seconds and for the second run (after reloading $V$ with the bad attractors), we set the timeout to be 60 seconds. By doing so the algorithm finishes successfully with having no remaining bad attractors.

Table~\ref{tab:stats} shows the numerical results of our experiments. Our method being provably guaranteed to find a plan in bounded time, shows a success rate of 100 percent. For the two preprocessing-based planners PRM and MQ-RRT, we report the results for a preprocessing time of 4T (T being the time consumed by our method in preprocessing). The E-graph planner is bootstrapped with a hundred paths precomputed for uniformly sampled goals in $G_S$. For each of these three multi-query planners while running the experiments, the newly-created edges were appended to the auxilary data (tree, roadmap or E-graph) to be used for the subsequent queries.

Among other planners, PRM shows the highest success rate but at the cost of a large memory footprint. The E-graph planner has a small memory footprint but it shows significantly longer planning times. RRT-Connect being a single-query planner happens to be the slowest. Our method shows a speedup of over tenfold in query time as compared to PRM and MQ-RRT and about three orders of magnitude speedup over the E-graph planner and RRT-Connect. The plots in Fig.~\ref{fig:plots} show how the success rate and the memory footprints of PRM and MQ-RRT vary as a function of the preprocessing time. For our domain the PRM seems to saturate in terms of the success rate after T time whereas MQ-RRT continues to improve provided more preprocessing time. In terms of memory usage, PRM's memory footprint grows more rapidly than MQ-RRT.

\begin{table*}[t]
\centering
     \resizebox{\columnwidth}{!}{%
    % \resizebox{1.4\textwidth}{!}{\begin{minipage}{\textwidth}
        \begin{tabular}{ l | c c c c c}
           & PRM (4T) & MQ-RRT (4T) & E-graph & RRT-Connect & \textbf{Our method} \\
         \hline
         Planning time [ms]& 21.7 (59.6) & 21.2 (35.5) & 497.8 (9678.5) & 1960 (9652) & \textbf{1.0 (1.6)}\\
         Success rate [$\%$]& 86 & 69.75 & 76.5 & 83.8 & \textbf{100}\\
         Memory usage [Mb] & 1,828 & 225.75 & \textbf{2.0} & - & {7.8}
        \end{tabular}
    }
    \caption{Experimental results comparing our method with other single- and multi-query planners tested on Intel® Core i7‐5600U (2.6GHz) machine with 16GB RAM. The table shows the mean/worst-case planning times, success rates and memory usage for our method and for other multi-query planners preprocessed with quadruple the time that our method takes in precomputation (T = 1,445 seconds). Note that the worst-case time for our method shown in these results ($\sim$1.6 millisecond) is the empirical one and not the computed provable time bound which is 3 milliseconds (on our machine) for this environment.
    Results of sampling-based planners are averaged over 200 uniformly sampled queries. For the sampling-based planners the results were averaged over 4 trials (for the same set of 200 queries) with different random number generation seeds.}
    \label{tab:stats}
  % \end{center}
% \vspace{-6mm}
\end{table*}


\begin{figure}
\begin{subfigure}{0.5\textwidth}
\centering
  \includegraphics[width=0.9\linewidth]{success.png}
  \caption{}
  \label{fig:success}
\end{subfigure}
\hfill
\begin{subfigure}{0.5\textwidth}
\centering
  \includegraphics[width=0.9\linewidth]{memory.png}
  \caption{}
  \label{fig:memory}
\end{subfigure}
    \caption{Preprocessing time vs~(\subref{fig:success}) the success rates and~(\subref{fig:memory}) memory useage for the 200 queries averaged over 4 trials with different random number generation seeds for the PRM and the MQ-RRT algorithms. The results were computed at the intervals which are multiple of the time T = 1,455 seconds, that our method takes for precomputation. The green cross shows our method for reference.}
    \label{fig:plots}
\end{figure}

\subsubsection{Motion Planning for Truck-unloading Robot}
We tested our algorithm on an industrial truck-unloading robot~(see Fig.~\ref{fig:gmt}). The robot has a mobile omnidirectional base referred to as \textit{base} and two articulated mechanisms--a manipulator-like tool with suction grippers and a  scooper-like tool with conveyor belts, referred to as \textit{arm} and \textit{nose}, respectively. The general task of this robot is to unload boxes from delivery trucks as quickly and efficiently as possible and that is why it requires fast and reliable motion planning.

The robot's arm is 4-Dof and the nose has 3-Dof. We restrict the base motion to be one-dimensional and only allow the base to move either forward or backward. The robot can be controlled in different modes of operation and we tested our planner for each of these modes~(see table~\ref{tab:gmt}). The start state of the robot for each of these mode is a fixed configuration where the robot drops off the boxes or resets. Similar to the mail-sorting domain, the goal region is defined in the task space of the robot. For this robot we define two goal regions~$G_{\textrm{arm}}$ and~$G_{\textrm{nose}}$ for the arm and nose respectively. The discretization of~$G$ along position axes~($x,y,z$) is~0.05m and rotations axes~($\textit{roll, pitch, yaw}$) is 5 degrees. The sizes of the goal regions are selected as per the requirement of the task.
We compare our results with Weighted A* (WA*)algorithm. The results are summarized in Table~\ref{tab:gmt}. We observe our algorithm shows over an order of magnitude improvements in terms of planning times over the WA*. For a timeout of 5 seconds both the planners showed 100 percent success.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{gmt.JPG}
    % \vspace{-2mm}
  \caption{
  Truck-unloader robot with its manipulator-like (arm) and scooper-like (nose) end effectors
}
    \label{fig:gmt}
% \vspace{-6mm}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{gmt_sim.png}
    % \vspace{-2mm}
  \caption{
  A simulated real world truck-unloading scenario
}
    \label{fig:gmt_sim}
% \vspace{-6mm}
\end{figure}

\begin{table*}[t]
\centering
     \resizebox{0.7\columnwidth}{!}{%
    % \resizebox{1.4\textwidth}{!}{\begin{minipage}{\textwidth}
        \begin{tabular}{ l | c c c c}  
           & arm & nose & arm+base & nose+base\\
         \hline
         WA* 		& 12.8 & 7.2 & 7.5 & 4.4 \\
         Our Method & 0.69 & 0.29 & 0.5 & 0.47 \\
        \end{tabular}
    }
    \caption{Comparison of mean planning times~[ms] averaged over 100 randomized queries for different modes of operation of the truck-unloading robot. The number of subregions preprocessed for each of these modes~(left to right) are~101,~7,~85 and~308.}
    \label{tab:gmt}
  % \end{center}
% \vspace{-6mm}
\end{table*}

As this algorithm is only applicable to static (known) environments, we only preprocessed it with collision checking against the walls of the truck and with self collision checking and not with the boxes inside the truck. We also tested our planner in a simulated real world scenario~(see Fig.\ref{fig:gmt_sim}) in conjunction with the WA* planner. For each query, we first ran our planner and validated the computed path against non-static obstacles i.e the boxes. We employed WA* as a fallback planner in case the first path is invalid. We observed that for all the queries ran for this scene, the fallback planner was never invoked. This experiment gave us an interesting use case for our planner to be employed in conjunction with conventional planners in the environments that are partially static.

\section{CTMP to Pickup Moving Objects off a Conveyor}
\label{sec:conveyor}
\subsection{Overview}
%% motivation
Conveyor belts are widely used in automated distribution, warehousing, as well as for manufacturing and production facilities. In the modern times robotic manipulators are being deployed extensively at the conveyor belts for automation and faster operations~\cite{zhang2018gilbreth}. In order to maintain a high-distribution throughput, manipulators must pick up moving objects without having to stop the conveyor for every grasp. In this work, we consider the problem of motion planning for grasping moving objects off a conveyor. An object in motion imposes a requirement that it should be picked up in a short window of time. The motion planner for the arm, therefore, must compute a path within a bounded time frame to be able to successfully perform this task.


%% why replanning?
    Manipulation relies on high quality detection and localization of moving objects. When the object first enters the field of view of the robot, the initial perception estimates of the object's pose are often inaccurate. Consider the example of an object (sugar box) moving along the conveyor towards the robot in Fig.~\ref{fig:intro_pic}, shown through an image sequence as captured by the robot's Kinect camera in Fig.~\ref{fig:pose_sequence}. In order to understand how pose estimation error varies as the object gets closer, we filter the input point cloud and remove all points lying outside the conveyor. We then compute the pair-wise minimum distance between the input point cloud and a predicted point cloud that corresponds to the object's 3D model transformed with the predicted pose through ICP~\cite{besl1992method} refinement. The plot in Fig.~\ref{fig:pose_sequence} shows the variation of the average pairwise minimum distance between the two point clouds as the object gets closer. We can observe that the distance decreases as the object moves closer, indicating that the point clouds overlap more closely due to more accurate pose estimates closer to the camera.
    
% The black color denotes points corresponding to the object in the observed scene after filtering out all points lying outside the conveyor. The white point cloud shows the object's 3D model transformed with the predicted 3-Degree of Freedom pose obtained by using Iterative Closest Point (ICP)~\cite{besl1992method} refinement. Upon observation of the overlap between the point clouds, we see that they begin to overlap more closely as the object moves towards the robot, indicating that the pose estimate is becoming increasingly accurate. 
However, if the robot waits too long to get an accurate estimate of the object pose, the delay in starting plan execution could cause the robot to miss the object. The likelihood of this occurring increases proportionately with the speed of the conveyor. Therefore, the robot should start executing a plan computed for the initial pose and as it gets better estimates, it should repeatedly replan for the new goals. However, for every replanning query, the time window for the pickup shrinks. This makes the planner's job difficult to support real-time planning.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.25\textwidth]{conveyor_pr2}
    \caption{
    \CaptionTextSize
    A scene demonstrating the PR2 robot picking up a moving object (sugar box) off a conveyor belt.}
    \label{fig:intro_pic}
    \vspace{-7.5mm}
\end{figure}


%% why problem is hard
Furthermore, the planning problem is challenging because the motion planner has to account for the dynamic object and thus plan with time as one of the planning dimension. It should generate a valid trajectory that avoids collision with the environment around it and also with the target object to ensure that it does not damage or topple it during the grasp. Avoiding collisions with the object requires precise geometric collision checking between the object geometry and the geometry of the manipulator. The resulting complexity of the planning problem makes it infeasible to plan online for this task.

%%
Motivated by these challenges, we propose a planning framework that leverages offline preprocessing to provide bounds on the planning time when the planner is invoked online. Our key insight is that in our domain the manipulation task is highly repetitive---even for different object poses, the computed paths are quite similar and can be efficiently reused to speed up online planning. Based on this insight, we derive a method that precomputes a representative set of paths offline with some auxiliary datastructures and uses them online to plan in constant time. Here, we assume that the models of the target objects are known. Namely, the planner is provided with the geometric model of the target object apriori. To the best of our knowledge, our approach is the first to provide provable constant-time planning guarantees on generating motions all the way to the goal for dynamic environments.

%
We experimentally show that constant-time planning and replanning capability is necessary for a successful conveyor pickup task. Specifically if we only perform one-time planning, (namely, either following the plan for the initial noisy pose estimate or from a delayed but accurate pose estimate) we frequently fail to pick the object.
\begin{figure}[t]
    \centering
    \begin{subfigure}{.225\textwidth}
    %   \centering
        \includegraphics[width=0.9\textwidth]{object_blur}
        \caption{}
        \label{fig:obj1}
    \end{subfigure}
    \begin{subfigure}{0.225\textwidth}
    %   \centering
        \includegraphics[width=0.9\textwidth]{pose_error_f}
        \caption{}
        \label{fig:obj2}
    \end{subfigure}
    \caption{
    \CaptionTextSize
    (\subref{fig:obj1})~Depiction of an object moving along a conveyor towards the robot.
    %
    (\subref{fig:obj2})~Pose error as a function of the distance from the conveyor's start.
    %, calculated as the average pair wise minimum distance between the input point cloud and the point cloud corresponding to the predicted pose.
    }
    \label{fig:pose_sequence}
    \vspace{-7.5mm}
\end{figure}

\subsection{Problem definition}
Our system is comprised of 
a robot manipulator~$\calR$,
a conveyor belt~$\calB$ moving at some known velocity,
a set of known objects~$\calO$ that need to be grasped and 
a perception system~$\calP$ that is able to estimate the type of object and its location on~$\calB$.

Given a pose $g$ of an object $o \in \calO$, our task is to plan the motion of $\calR$ such that it will be able to pick~$o$ from~$\calB$ at some future time.
%
Unfortunately, the perception system $\calP$ may give inaccurate object poses.
Thus, the pose $g$ will be updated by~$\calP$ as $\calR$ is executing its motion. 
To allow for $\calR$ to move towards the updated pose in real time, we introduce the additional requirement that planning should be done within a predefined time bound~\Tbound.
%
For ease of exposition, when we say that we plan to a pose $g$ of $o$ that is given by $\calP$, 
we mean that we plan the motion of $\calR$ such that it will be able to pick~$o$ from~$\calB$ at some future time. 
This is explained in detail in Sec.~\ref{sec:eval} and in Fig.~\ref{fig:pe}.

%
We denote by $\Gfull$ the discrete set of initial object poses on $\calB$ that $\calP$ can perceive.
%
Finally, we assume that $\calR$ has an initial configuration \Shome from which it will start its motion to grasp any object.


Roughly speaking, the objective, following the set of assumptions we will shortly state, is to enable planning to any goal pose $ g \in \Gfull$ in bounded time~\Tbound regardless of $\calR$'s current configuration.
%More specifically, the perception system is setup such that it sends updated pose estimates on the fly as the object moves along the conveyor belt and as the robot approaches the object.   
To formalize this idea, let us introduce the notion of a \emph{reachable} state:

\begin{definition}
    A goal pose $g \in \Gfull$ is said to be \emph{reachable} from a state $s$ if there exists a path from $s$ to $g$ and it can be computed in finite time.
\end{definition}

Given a state $s$ we denote the set of all goals that are reachable from $s$ as $G^{\rm reach}(s)$ and we say that $G^{\rm reach}(s)$ is \emph{reachable} from $s$.

\begin{definition}
    A reachable pose $g \in \Gfull$ is said to be \emph{covered} by a state $s$ if 
    the system can plan a path from $s$ to $g$ within time~\Tbound.
\end{definition}

Thus, we wish to build a system such that 
for any state $s$ that the system can be in 
and every reachable goal pose $g \in \Gfull$ updated by~$\calP$,
$g$ is covered by $s$.


We are now ready to state the assumptions for which we can solve the problem defined.
%\begin{definition}
%    A goal region $G \subseteq \Gfull$ is said to be \emph{reachable} from a state $s$ if any state $g \in G$ is reachable from $s$.
%\end{definition}

We make the following assumptions about the system.
\begin{enumerate}[label={\textbf{A\arabic*}},leftmargin=0.75cm]
    % \item \label{assum:1} The goal set \Gfull is reachable from the start state~\Shome. Namely,  $G^{\rm reach}(\Shome) = \Gfull$.
    
\ignore{
    \item \label{assum:2} Given a path 
    $\Pi = \{s_0, \ldots, s_k \}$ 
    s.t. $s_0 = \Shome$ and $s_k \in \Gfull$, 
    we have that $G^{\rm reach}(s_{i+1}) \subset G^{\rm reach}(s_{i})$.
    Namely, the reachable set of goals for a state on the path is a subset of the reachable set of every other state on that path that exists before it.
    \os{Oren - Didn't we agree that this is not an assumption but a property?}
}

%    \item \label{assum:3} \Gfull would accommodate for an error $\epsilon$ in the initial pose estimate $g_{\textrm{init}}$ of the perception system. Each subsequent estimate $g_{\textrm{next}}$ will be within the $\epsilon$ window around $g_{\textrm{init}}$ (in retrospect because the conveyor is moving).
 
    \item \label{assum:4} There exists a replan cutoff time \Trc from when the robot starts moving, after which the planner does not replan and continues to execute the last planned path.

    \item \label{assum:5} If the robot starts moving at $t = 0$ then for any time $t < \Trc$, the environment is static. Namely, objects on~$\calB$ cannot collide with $\calR$ during that time.
    
    \item \label{assum:3} Given an initial goal pose $g_{\textrm{init}} \in \Gfull$ by $\calP$, any subsequent pose $g_{\textrm{new}} \in \Gfull$ is at most $\varepsilon_\calP$ distance away from $g_{\textrm{init}}$ (after accounting for the fact that the object moved along $\calB$).
    

\end{enumerate}

Assumptions~\ref{assum:4}-\ref{assum:5} enforce a requirement that $\calP$ must provide an accurate estimate $g$ while $o$ is at a safe distance from $\calR$.
%
Assumption~\ref{assum:3} corresponds to the error tolerance of the perception system. This is explained in detail in Sec.~\ref{sec:eval} and in Fig.~\ref{fig:pe}
% Assumption~\ref{assum:1} corresponds to properties of the environment \emph{without} taking the objects on the conveyor belt into account 
% while


% Assumptions~\ref{assum:3}-\ref{assum:5} correspond to properties of the environment that account for the perception system and the objects on the conveyor belt.


\subsection{Algorithmic framework}
\label{subsec:strawman}
Our approach for bounded-time planning relies on a \emph{preprocessing} stage that allows to efficiently compute paths in a \emph{query} stage to any goal state (under Assumptions~\ref{assum:4}-\ref{assum:3}). 
%
Before we describe our approach, we start by describing a \naive method that solves the aforementioned problem but requires a prohibitive amount of memory.
%
This can be seen as a warmup before describing our algorithm which exhibits the same traits but doing so in a memory-efficient manner.

\subsubsection{Straw man approach}
\begin{figure}[t]
    \centering
    \begin{subfigure}{.225\textwidth}
    %   \centering
        \includegraphics[width=0.9\textwidth]{naive1}
        \caption{}
        \label{fig:naive1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.225\textwidth}
    %   \centering
        \includegraphics[width=0.9\textwidth]{naive2}
        \caption{}
        \label{fig:naive2}
    \end{subfigure}
    \caption{
    \CaptionTextSize
    The two steps of the preprocessing stage of the straw man algorithm. In both figures, paths that are very similar (and their corresponding goal states) are depicted using the same color.
    (\subref{fig:naive1})~First step---the algorithm computes a path from \Shome to every state in \Gfull.
    %
    (\subref{fig:naive2})~Second step---the algorithm computes for each path a new path to each goal state for every state along the path. Here, one representative path is depicted together with three different goal states. 
    This is repeated recursively for the new paths but not visualized.}
    \label{fig:naive}
    \vspace{-5mm}
\end{figure}

We divide the preprocessing stage into two steps.
In the first step, we compute from \Shome a path $\Pi_g$ to every reachable $g \in \Gfull$. 
%(such a path exists following \ref{assum:1}).
. Thus, all goal states are covered by \Shome and this allows us to start executing a path once $\calP$ gives its initial pose estimate.
However, we need to allow for updated pose estimations while executing~$\Pi_g$. 

%
Following \ref{assum:4} and \ref{assum:5}, this only needs to be done up until time~\Trc.
Thus, we discretize each path such that consecutive states along the path are no more than $\delta _t$ time apart. As we will see, this will allow the system to start executing a new path within $\Tbound + \delta_t$ after a new pose estimation is obtained from $\calP$.
%
We call all states that are less than \Trc time from \Shome \emph{replanable states}.


In the second step of the preprocessing stage, for every replanable state along each path $\Pi_g$, we compute a new path to all goal states. 
%\bound{that are $\varepsilon_\calP$ away from $g$}.
See Fig.~\ref{fig:naive}.
%
This will ensure that all goal states are covered by the replanabale states that were introduced in the first step of the preprocessing stage. Namely, it will allow to immediately start executing a new path once the goal location is updated by~$\calP$.
%
Unfortunately,~$\calP$ may update the goal location more than once. Thus, this process needs to be performed recursively for the new paths as well.


The outcome of the preprocessing stage is a set of precomputed collision-free paths starting at states that are at most $\Trc$ from \Shome and end at goal states.
The paths are stored in a lookup table that can be queried in $O(1) < \Tbound$ time.

In the query stage we obtain an estimation $g_1$ of the goal pose by $\calP$. 
The algorithm then retrieves the path~$\Pi_1(\Shome,g_1)$ from~\Shome to~$g_1$ and the robot starts executing~$\Pi_1(\Shome,g_1)$.
%
For every new estimation $g_i$ of the goal pose obtained from~$\calP$  while the system is executing path $\Pi_{i-1}(s,g_{i-1})$, the algorithm retrieves from the lookup table the path $\Pi_i(s',g_i)$ from the first state~$s'$ along $\Pi_{i-1}(s,g_{i-1})$ that is $\Tbound + \delta_t$ time from~$s$. The robot~$\calR$ will then start executing~$\Pi_i(s',g_i)$ once it reaches~$s'$.

Clearly, every state is covered by this brute-force approach, however it requires a massive amount of memory.
Let $n_{\rm goal} = \vert \Gfull \vert$ be the number of goal states and
$\ell$ be the number of states between \Shome and the state that is \Trc time away.
This approach requires precomputing and storing $O(n_{\rm goal}^\ell)$ paths which is clearly infeasible.
In the next sections, we show how we can dramatically reduce the memory footprint of the approach without compromising on the system's capabilities.

\subsubsection{Algorithmic approach}
While the straw man algorithm presented  allows for planning to any goal pose $ g \in \Gfull$ in bounded time~\Tbound, its memory footprint is prohibitively large.
%
We suggest to reduce the memory footprint by building on the observation that many paths to close-by goals traverse very similar parts of the configurations space (see Fig.~\ref{fig:naive}).

Similar to the straw man algorithm, our preprocessing stage runs in two steps.
In the first step we  compute from \Shome a path $\Pi_g$ to every goal state $ g \in \Gfull$. However, this is done by computing a set of so-called ``root paths'' $\{\Pi_1, \ldots, \Pi_k \}$ from \Shome to a subset of goals in \Gfull (here we will have that $k \ll n_{\rm goal})$. 
As we will see, these paths will allow to efficiently compute paths to every goal state in the query stage and the system only needs to explicitly store these root paths in memory (and not a path to every goal state as in the straw man algorithm).
%
In the second step of the preprocessing stage, the algorithm recursively computes for all replanabale states along all root paths a new path to each goal state. However, this is done by attempting to re-use previously-computed root paths which, in turn, allows for a very low memory footprint.
%
It is important to note that replanable states are not only the ones that lie on the root paths from \Shome---whenever the recursion happens we compute a new set of root paths and generate new replanable states
%
The remainder of this section formalizes these ideas.

\subsubsection{Algorithmic building blocks}
We start by introducing the algorithmic building blocks that we use.
Specifically, we start by describing the motion planner that is used to compute the root paths 
and then continue to describe how they can be used as \emph{experiences} to efficiently compute paths to near-by goals.

\paragraph{Motion planner}
We use a heuristic search-based planning approach with motion primitives (see, e.g,~\cite{CCL10,CSCL11,LF09})
as it allows for deterministic planning time which is key in our domain.
Moreover, such planners can easily handle under-defined goals as we have in our setting---we define a goal as a six DoF grasp pose for the goal object while our robot has seven DoFs and we need to acount for the grasping time.

\textbf{State space and graph construction.}
We define a state $s$ as a pair $(q,t)$ where $q = (\theta_1, ..., \theta_n)$ is a configuration represented by the joint angles for an $n$-DOF robot arm (in our setting $n=7$) and $t$ is the time associated with $q$.
%
Given a state $s$ we define two types of motion primitives which are short kinodynamically feasible motions that the robot~$\calR$ can execute. The first, which we term \emph{predefined} primitives are used to move to a pre-grasp position while the second, termed \emph{dynamic} primitives are used to perform grasping after the robot reached a pre-grasp position.
%

The predefined primitives are small individual joint movements in either direction as well as wait actions.
For each motion primitive, we compute its duration by using a nominal constant velocity profile for the  joint that is moved.
%\footnote{Computing a plan using these motion primitives is not guaranteed to respect the acceleration limits of $\calR$. However, in practice this seldom occurs and we elaborate in Sec.~\ref{sec:eval} how we handle these execution errors.}.
%

The dynamic primitives are generated by using a Jacobian pseudo inverse-based control law similar to what~\cite{menon2014motion} uses. 
The velocity of the end effector is computed such that the end-effector minimizes the distance to the grasp pose. Once the gripper encloses the object, it moves along with the object until the gripper is closed.
Additional details omitted due to lack of space. 

\textbf{Motion planner.}
The states and the transitions implicitly define a graph $\calG = (S,E)$ where $S$ is the set of all states and~$E$ is the set of all transitions defined by the motion primitives. We use Weighted \astar (\wastar)~\cite{pohl1970heuristic} to find a path in $\calG$ from a given state~$s$ to a goal state $g$. 
\wastar is a suboptimal heursitic search algorithm that allows a tradeoff between optimality and greediness by inflating the heuristic function by a given weight~$w$. 
The search is guided by an efficient and fast-to-compute heuristic function which in our case has two components.
The first component drives the search to intercept the object at the right time and 
the second component guides the search to correct the orientation of the end effector as it approaches the object. 
More formally, our heuristic function is given by
\vspace{-3mm}
$$
 h(s,g) = \max (w \cdot t(s,g), \textsc{AngleDiff}(s,g)).
$$
Here, $t(s,g)$ is the expected time to intercept the object which can be analytically computed from the velocities and positions of the target object and the end-effector and \textsc{AngleDiff}($s,g$) gives the magnitude of angular difference between the end-effector's current pose and target pose.

\paragraph{Planning using experiences}
We now show how previously computed paths which we call ``experiences" can be reused in our framework. Given a heuristic function $h$ we define for each root path $\Pi$ and each goal state $g \in \Gfull$ the \emph{shortcut} state $\Ssc (\Pi,g)$ as the  state that is closest to~$g$ with respect~$h$.
Namely,
\vspace{-3mm}
$$
\Ssc (\Pi,g) := \argmin\limits_{s_i \in \Pi} h(s_i, g).
$$
Now, when searching for a path to a goal state $g \in \Gfull$ we add $\Ssc (\Pi,g)$ as a successor for any state along $\Pi$
(subject to the constraint that the path along $\Pi$ to \Ssc is collision free). In this manner we reuse previous experience to quickly reach a state close to the $g$.

\ignore{
\paragraph{Planning using experiences}
We now show how \emph{experience graphs}~\cite{PCCL12} can be used in our framework.
%
Roughly speaking, experience graphs allow a planner  to accelerate its planning efforts whenever possible by using previously-computed paths. The planner gracefully degenerates to planning from scratch if no previous planning experiences can be reused.
%
The key idea is to bias the search efforts, using a specially-constructed heuristic function (called the ``e-graph heuristic''), towards finding a way to get onto the previously-computed paths and to remain on them rather than explore new regions as much as possible. 

In our setting, we use a simplified version of the aforementioned approach which is faster to compute.
%
The key insight is that in our setting, we always start at \Shome which is the first state on all root paths. Thus, we only need to bias the search to stay on a root path (and we don't need to bias the search efforts to get onto the previously-computed paths).
%
To this end, given a heuristic function $h$ we define for each root path $\Pi$ and each goal state $g \in \Gfull$ the \emph{shortcut} state $\Ssc (\Pi,g)$ as the   state that is closest to~$g$ with respect~$h$.
Namely,
$$
\Ssc (\Pi,g) := \argmin\limits_{s_i \in \Pi} h(s_i, g).
$$
Now, when searching for a path to a goal state $g \in \Gfull$ we
(i)~add $\Ssc (\Pi,g)$ as a successor for any state along $\Pi$
(subject to the constraint that the path along $\Pi$ to \Ssc is collision free)
and
(ii)~update our heuristic function to bias the search to use root paths. Specifically, for any state $s$ on the root path $\Pi$ the heuristic is given by
$$
h(s,g) = \min(h(s,\Ssc (\Pi,g)) + h(\Ssc (\Pi,g),g), \varepsilon \cdot h(s,g)).
$$
Here, $\varepsilon>1$ is a penalty term that biases the search to find a path via \Ssc.
}

\subsubsection{Algorithmic details}
We are finally ready to describe our algorithm describing first the preprocessing phase and then  the query phase.

\paragraph{Preprocessing}
\begin{figure*}[t]
    \centering
    \begin{subfigure}{.225\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{2_compute_root_paths_1}
        \caption{}
        \label{fig:crp1}
    \end{subfigure}
    \hspace{4mm}
    \begin{subfigure}{0.225\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{2_compute_root_paths_2}
        \caption{}
        \label{fig:crp2}
    \end{subfigure}
    \hspace{4mm}
    \begin{subfigure}{0.225\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{2_compute_root_paths_3}
        \caption{}
        \label{fig:crp3}
    \end{subfigure}
    \caption{\CaptionTextSize
    First step of the preprocessing stage.
    (\subref{fig:crp1})~A goal state $g_1$ is sampled and the root path $\Pi_1$ is computed between \Shome and $g_1$.
    %
    (\subref{fig:crp2})~The set $G_1 \subset \Gfull$ of all states that can use $\Pi_1$ as an experience is computed and associated with $\Pi_1$.
    %
    (\subref{fig:crp3})~The goal region covered by four root paths from \Shome after the first step of the preprocessing stage terminates.
    }
    \label{fig:crp}
    \vspace{-5mm}
\end{figure*}
Our preproccesing stage starts by sampling a goal state~$g_1 \in \Gfull$ and computing a root path~$\Pi_1$ from~$\Shome$ to~$g_1$. We then associate with~$\Pi_1$ all goal states~$G_1 \subset \Gfull$ such that~$\Pi_1$ can be used as an experience in reaching any state in $g_1' \in G_1$ within~\Tbound
Thus, all goal states in~$G_1$ are covered by~\Shome.
%
We then repeat this process but instead of sampling  a goal state from \Gfull, we sample from $\Gfull \setminus \Gcov$, where~\Gcov is the set of goal states already covered by \Shome.
At the end of this step, we obtain a set of root paths. 
Each root path~$\Pi_i$ is associated with a goal set $G_i \subseteq \Gfull$ such that 
(i)~$\Pi_i$ can be used as an experience in reaching any state in $g_i' \in G_i$ in~\Tbound and 
(ii)~$\bigcup_i G_i = \Gfull$.
%
See Fig.~\ref{fig:crp}.and  Alg.~\ref{alg:step1}, respectively.




\begin{algorithm}[t]
\caption{\textsc{ComputeRootPaths}($s_{\textrm{start}}, \Guncov$)}
\label{alg:step1}
    \AlgFontSize
\begin{algorithmic}[1]
\State $\Psi \leftarrow \emptyset$   \Comment{A list of pairs ($\Pi_i, G_i)$}
\State $\Guncovp \leftarrow \emptyset$; \hspace{3mm}
       $\Gcovp \leftarrow \emptyset$; \hspace{3mm}
       $i = 0$
\While{$\Guncov \neq \emptyset$}
        \Comment{Runs until all uncovered points are considered}
    \State $g_i \leftarrow$\textsc{Sample}($\Guncov$)
    \State $\Guncov \leftarrow \Gcov \setminus \{g_i\}$
    
    \State $\Pi_i \leftarrow$ \textsc{Plan}($s_{\textrm{start}}, g_i$)
        \Comment{Plans root path}
    \If {$\Pi_i = \emptyset$}  \Comment{Path does not exist}
        \State $\Guncovp \leftarrow \Guncovp \cup \{g_i\}$
    \Else
        \State $G_i \leftarrow \{ g_i \}$
        \For {\textbf{each} $g_j \in \Guncov$}
            \State $\pi_j \leftarrow$\textsc{Plan}($s_{\textrm{start}},g_j,\Pi_i$)
                \Comment{Plans using root path as experience}
            \If {$\pi_j \neq \emptyset$} \Comment{Planner succeeded}
                \State $G_i \leftarrow G_i \cup \{g_j\}$
                \State $\Guncov \leftarrow \Guncov \setminus \{g_j\}$
            \EndIf
        \EndFor
        \State $\Psi \leftarrow \Psi \cup \{ (\Pi_i, G_i)\}$
        \Comment{Insert $(\Pi_i, G_i)$ to $\Psi$}
        \State $i \leftarrow i + 1$
    \EndIf
\EndWhile
\State \textbf{return} $\Psi, \Guncovp$
\end{algorithmic}
\end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
\caption{\textsc{PreprocessMain(\Shome, \Gfull)}}\label{alg:all}
    \AlgFontSize
\begin{algorithmic}[1]
    \State \textsc{Preprocess}($\Shome, \Gfull$)
\end{algorithmic}
\end{algorithm}
%
\begin{algorithm}[t]
\caption{\textsc{Preprocess}($\Sstart,\Guncov,\Gcov $)}\label{alg:preprocess}
    \AlgFontSize
\begin{algorithmic}[1]
\State $\Psi_{\textrm{work}}, G'^{\textrm{uncov}}_{\textrm{work}} \leftarrow$ \textsc{ComputeRootPaths}($\Sstart,\Guncov$)

\If {$\Sstart = \Shome$}
    \State $\Psi_{\textrm{home}} = \Psi_{\textrm{work}}$
\EndIf

\State $G'^{\textrm{cov}} \leftarrow 
    \Gcov \cup (\Guncov \setminus G'^{\textrm{uncov}}_{\textrm{work}})$
\If{$t(s_{\textrm{start}}) \geq \Trc$}
    \State \textbf{return} $G'^{\textrm{uncov}}_{\textrm{work}}, G'^{\textrm{cov}}$
\EndIf
\For {\textbf{each} $(\Pi_i, G_i) \in \Psi_{\textrm{work}}$}
    \State $G_i^{\textrm{cov}} \leftarrow G_i$;
            \hspace{2mm}
           $G_i^{\textrm{uncov}} \leftarrow G'^{\textrm{cov}} \setminus G_i$;
            \hspace{2mm}
           $t = \Trc$
%    \State $t = \Trc$
%    \State $G_i^{\textrm{UNCOV}} \leftarrow G'^{\textrm{COV}} \setminus G_i$
%    \State $G_i^{\textrm{COV}} \leftarrow G_i$

    \While{$t \geq t(\Sstart)$}
        \State $s \leftarrow$ \textsc{GetState($\Pi_i, t$)}
        \For {\textbf{each} $(\Pi_j, G_j) \in \Psi_{\textrm{home}}$}
        \label{alg:preprocess:line:latch1a}
            \If{\textsc{CanLatch}($s,\Pi_j$)}
                \State $G_i^{\textrm{uncov}} \leftarrow G_i^{\textrm{uncov}} \setminus G_j$
                \State $G_i^{\textrm{cov}} \leftarrow G_i^{\textrm{cov}} \cup G_j$
                \label{alg:preprocess:line:latch1b}
            \EndIf
        \EndFor
        % }
        %%%%%%%%% LATCHING END
        \If{$G_i^{\textrm{uncov}} = \emptyset$}
            \State \textbf{break}
        \EndIf
        \State $G_i^{\textrm{uncov}},G_i^{\textrm{cov}} \leftarrow$ \textsc{Preprocess}($s,G_i^{\textrm{uncov}},G_i^{\textrm{cov}}$)
        \If{$G_i^{\textrm{uncov}} = \emptyset$}
            \State \textbf{break}
        \EndIf
        \State $t \leftarrow t - \delta_t$
    \EndWhile
    % \State $(\Pi_i,\Pi_j).s = s$ \Comment{replan state}
\EndFor
\State \textbf{return} $G'^{\textrm{uncov}}_{\textrm{work}}, G'^{\textrm{cov}}$

\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Equipped with a set of root paths and the corresponding goal regions they cover, we now need to allow for efficient replanning. This is done by using the following two observations:
\begin{enumerate}[label={\textbf{O\arabic*}},leftmargin=0.75cm]
    \item \label{obs:1} 
    Assume that $\calR$ is executing a path $\Pi_i$ to $g_i \in G_i$ and that $\Pi_{s, i \rightarrow j}$ is a path starting at state $s \in \Pi_i$ and ending at goal $g_j \in \Gfull \setminus G_i$.
    If the last update given by $\calP$ happens $\Tbound + \delta_t$ before $s$ is reached,
    then the system can execute path $\Pi_i$ until $s$ and then continue executing $\Pi_{s, i \rightarrow j}$ to reach  $g_j$.

    \item \label{obs:2} 
    Assume that $\calR$ is executing a path $\Pi_i$ to $g_i \in G_i$ and that $\pi_{s,s',i \rightarrow j}$ is a path connecting  $s \in \Pi_i$ to $s' \in \Pi_j$ (with~$\Pi_j$ the root path to $g_j \in G_j$).
    If the last update given by~$\calP$ happens $\Tbound + \delta_t$ before~$s$ is reached,
    and the new goal is in $G_j$,
    then the system can execute path~$\Pi_i$ until~$s$, execute $\pi_{s,s',i \rightarrow j}$ and then continue executing~$\Pi_{j}$ from~$s'$ until the goal is reached.

    %\os{do we need to add a figure???}
\end{enumerate}

%\subsubsection{Procrastination is a bliss}
%Recall that in the straw man algorithm presented in Sec.~\ref{subsec:strawman}, given a path $\Pi$ connecting \Shome to some goal $g$, we computed a path to every goal state $\Gfull \setminus \{g\}$ from all replanable states.
%
\ref{obs:1} implies that if we can cover a goal state $g'$  by some state~$s$ along $\Pi_g$ (with $g \neq g'$), then we cover $g'$ by all states $\Pi$ that occur before~$s$.
%
\ref{obs:2} implies that if we can compute a path connecting one root path to some other root path, a process we term as ``latching'' on to the new path, then the new root path can be used to reach all its associated goal states.

We are now ready to describe the second step of our preprocessing stage.
%
For every root path $\Pi_i$ we look at the last replanning state $s_{\Pi_i, \Trc}$ (namely, the state that is $t=\Trc$ time from \Shome). For every other root path $\Pi_j$, we test if the motion (a dynamically generated primitive) connecting $s_{\Pi_i, \Trc}$ to $s_{\Pi_j, \Trc + \delta_t}$ (the state on $\Pi_j$ that is $\Trc+\delta_t$ away from \Shome) is valid (i.e. is collision free and reachable in time). 
%
If this is the case then all goal states in $G_j$ are covered by all replanning states along~$\Pi_i$
See Alg.~\ref{alg:preprocess} lines~\ref{alg:preprocess:line:latch1a}-\ref{alg:preprocess:line:latch1a}.
%

Let $\Guncov(\Trc)$ be all the states that are still uncovered after the above process. We recursively apply our algorithm to the setting where the start state is $s_{\Pi_i, \Trc}$ and the goal region is~$\Guncov(\Trc)$.
If all states are covered after this step, we terminate. 
If not, let $\Guncovp(\Trc)$ be all the states still uncovered.
We consider the state $s_{\Pi_i, \Trc-\delta_t}$ (the state on $\Pi_j$ that is $\Trc-\delta_t$ away from \Shome) and recursively run our algorithm where the start state is $s_{\Pi_i, \Trc-\delta_t}$ and the goal region is $\Guncovp(\Trc)$.
This process is repeated until either all states are covered or we backtracked to $s_{\Pi_i, 0}$ i.e the first state on $s_{\Pi_i}$.
See Fig.~\ref{fig:pl} and  Alg.~\ref{alg:all} and~\ref{alg:preprocess}, respectively.

Thus, the outcome of the preprocessing stage is map $\calM: S \times \Gfull \rightarrow \{ \Pi_1, \Pi_2, \ldots \}$ mapping which root path can be as an experience to reach a goal $g$ from a state $s$.






\begin{figure*}[t]
    \centering
    \begin{subfigure}{.225\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_1}
        \caption{}
        \label{fig:pl1}
    \end{subfigure}
    \hspace{1mm}
    \begin{subfigure}{0.225\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_2}
        \caption{}
        \label{fig:pl2}
    \end{subfigure} 
    \hspace{1mm}
    \begin{subfigure}{0.225\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_3}
        \caption{}
        \label{fig:pl3}
    \end{subfigure}
    %
    \hspace{1mm}
    \begin{subfigure}{.225\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_4}
        \caption{}
        \label{fig:pl4}
    \end{subfigure}
    %\hspace{2mm}
    \begin{subfigure}{0.225\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_5}
        \caption{}
        \label{fig:pl5}
    \end{subfigure}
    \hspace{1mm}
    \begin{subfigure}{0.225\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_6}
        \caption{}
        \label{fig:pl6}
    \end{subfigure}
    \hspace{1mm}
    \begin{subfigure}{0.225\textwidth}
    %   \centering
        \includegraphics[width=\textwidth]{3_preprocess_loop_7}
        \caption{}
        \label{fig:pl7}
    \end{subfigure}
    \caption{\CaptionTextSize
    Preprocess loop.
    (\subref{fig:pl1})~We start by trying to latch on to every other root path. 
    %
    (\subref{fig:pl2})~For successful latches, we add the corresponding goal states to our covered region.
    %
    (\subref{fig:pl3})~We then try to compute new root path with its corresponding goal states (see also Fig.~\ref{fig:crp2}).
    %
    (\subref{fig:pl4})-(\subref{fig:pl6})~This process is repeated by backtracking along the root path.
    %
    (\subref{fig:pl7})~Outcome of a preprocessing step for one path. \Gfull is covered either by using $\Pi_1$ as an experience, 
    latching on to $\Pi_2$ or  $\Pi_3$ (at different time steps)
    or by 
    using newly-computed root paths. 
    }
    \label{fig:pl}
    \vspace{-5mm}
\end{figure*}

\paragraph{Query}
In the query stage, given an initial estimation $g_{\rm init}$ of the goal pose from $\calP$, our algorithm queries $\calM$ to obtain the root path $\Pi$ from $\Shome$ to $g_{\rm init}$.
It then plans a path using $\Pi$ as an experience and starts executing this path.
%
Now, assume that the~$\calR$ is executing path $\Pi_{\rm curr}$ and a new estimation $g_{\rm new}$ of the goal pose is provided by $\calP$.
%
We consider the state $s_{\Pi_{\rm curr}, \Trc}$ along the path $\Pi_{\rm curr}$ at time $\Trc$ and test if
(i)~we can reuse the root path that $\calR$ is currently executing as an experience to reach the new goal
(Alg.~\ref{alg:query}, lines~\ref{alg:query:line:c1a}-\ref{alg:query:line:c1b}),
(ii)~there is a root path that can be used as an experience to reach $g_{\rm new}$ starting at $s_{\Pi_{\rm curr}, \Trc}$ 
(Alg.~\ref{alg:query}, lines~\ref{alg:query:line:c2a}-\ref{alg:query:line:c2b})
or if 
(iii)~there is a root path starting at \Shome associated with~$g_{\rm new}$ that we can latch on to.
If the former holds, we run our experience-based planner to obtain a path $\Pi(s_{\Pi_{\rm curr}}, \Trc, g_{\rm new})$ starting at $s_{\Pi_{\rm curr}}$ and ending at $g_{\rm new}$. We then execute $\Pi_{\rm curr}$ until $s_{\Pi_{\rm curr}, \Trc}$ and then continue by executing $\Pi(s_{\Pi_{\rm curr}}, \Trc, g_{\rm new})$.
%
If the latter holds, we run our experience-based planner to obtain a path $\Pi(s', \Trc + \delta_t), g_{\rm new})$ starting at $s'$, the state on the path we latched on to and ending at $g_{\rm new}$. We then execute $\Pi_{\rm curr}$ until $s_{\Pi_{\rm curr}, \Trc}$ and then continue by executing the motion that latches on to $s'$ and finally executing  $\Pi(s', \Trc + \delta_t, g_{\rm new})$.
See Alg.~\ref{alg:query}, lines~\ref{alg:query:line:c3a}-\ref{alg:query:line:c3b}.
%
If neither hold, we consider the state $s_{\Pi_{\rm curr}, \Trc-\delta_t}$ and repeat this process.
%
For pseudocode describing our approach see~\ref{alg:query}.

\begin{algorithm}[t]
\caption{\textsc{Query}($g, \pi_{\textrm{curr}},s_{\textrm{start}}$)}\label{alg:query}  
    \AlgFontSize
\begin{algorithmic}[1]
    \State $s_{\rm first} \leftarrow \textsc{Head}  \pi_{\textrm{curr}}$ 
        \Comment{First state on $ \pi_{\textrm{curr}}$}
        \label{alg:query:line:c1a}
    \State $\Pi_{\textrm{curr}} \leftarrow$ $\calM(s_{\textrm{start}},g)$ 
         \Comment{Lookup root path from $s_{\textrm{start}}$ to $g$}
    \State $\pi \leftarrow$ \textsc{Plan}($s_{\textrm{start}},g,\Pi_{\textrm{curr}}$)
        \Comment{Plan using root path as experience}
    \If{$\pi \neq  = \emptyset$}
        \State \textbf{return} $\pi$
        \label{alg:query:line:c1b}
    \EndIf
\vspace{2mm}

\State $t \leftarrow \Trc$; \hspace{3mm} $t_{\textrm{curr}}\leftarrow \textsc{CurrentTime} + \Tbound$
\While{$t \geq t_{\textrm{curr}}$}
    \State $s \leftarrow$ \textsc{GetState}($\pi_{\textrm{curr}}, t$)
        \Comment{Get state on path at time $t$}
        \label{alg:query:line:c2a}
    \State $\Pi_{\textrm{next}} \leftarrow$ $\calM(s,g)$ 
         \Comment{Lookup root path from $s$ to $g$}
    \If{$\Pi_{\textrm{next}} \neq \emptyset$}
        \State $\pi_{\textrm{next}} \leftarrow$\textsc{Plan}($s_{\textrm{start}},g,\Pi_{\textrm{next}}$)
         \Comment{Plan using root path as experience}
        \State $\pi \leftarrow$ \textsc{MergePaths}($\pi_{\textrm{curr}},\pi_{\textrm{next}},t$)
        \State \textbf{return} $\pi$
        \label{alg:query:line:c2b}
    \EndIf
%%%%%%%%%%%%%%%%LATCHING 

\vspace{2mm}

%    {\color{blue}
    \State $\Pi_{\textrm{home}} \leftarrow \calM (\Shome,g)$
     \Comment{Lookup root path from  to $g$}
     \label{alg:query:line:c3a}
    \If{$\Pi_{\textrm{home}} \neq \emptyset$}
        \If{\textsc{CanLatch}($s,\Pi_{\textrm{home}}$)}
            \State $\pi_{\textrm{home}} \leftarrow$\textsc{Plan}($s_{\textrm{start}},g,\Pi_{\textrm{home}}$)
            \Comment{Plan using root path}
            \State $\pi \leftarrow$ \textsc{MergePathsByLatching}($\pi_{\textrm{curr}},\pi_{\textrm{home}}, t$)
            \State \textbf{return} $\pi$
            \label{alg:query:line:c3b}
        \EndIf
    \EndIf
 %  }
%%%%%%%%%%%
    \State $t \leftarrow t - \delta_t$
\EndWhile
\State \textbf{return failure}
\end{algorithmic}
\end{algorithm}




\subsubsection{Theoretical guarantees}

\begin{lemma}[Completeness]
\label{lemma:completeness}
Let $s$ be a state that $\calR$ is at in the query phase and $g\in \Gfull$ is a goal state.
If~$g$ is reachable from a state $s'$ that is (i)~reachable from $s$ and (ii)~is \Tbound away from $s$ then the planner will compute a path to $g$.
\end{lemma}
We omit the proof due to lack of space.

\begin{lemma}[Constant-time planning]
\label{lemma:bounded_time}
Let $s$ be a replanable state and $g$ a goal state provided by $\calP$.
If~$g$ is reachable, the planner is guaranteed to provide a solution in constant time.
\end{lemma}

\begin{proof}
    We have to show that the query stage (Alg.~\ref{alg:query}) has a constant-time complexity. 
    The number of times the algorithm queries for a root path (namely, computing $\calM()$  which is a hash look-up that is a constant-time operation) is bounded by $n_{\textrm{steps}} = \Trc/\delta_t$ which is the maximum number of time steps from $t = 0$ to $\Trc$. 
    The number of times the algorithm will attempt to latch on to a root path (namely, a call to \textsc{CanLatch}  which is a constant-time operation) is also bounded by $n_{\textrm{steps}}$. Finally, Alg.~\ref{alg:query} calls the \textsc{Plan} method only once.
    Since we are using a deterministic planner, the computation time is constant. 
    Hence the overall complexity of Alg.~\ref{alg:query} is $O(1)$.
\end{proof}

The analysis provided in Lemma~\ref{lemma:bounded_time} highlights the tradeoff our algorithm takes.
If we use a fine discretization of states along the path (namely, we choose a small value for $\delta_t$) then the guaranteed  planning time \Tbound is going to be high but there is a higher chance that any goal $g$ will be reachable.
On the other hand, if we use a course discretization (namely, we choose a large value for $\delta_t$) then we reduce~\Tbound at the price that some goal states may not be reachable.
%




\ignore{
Before describing how this is done, consider two root paths $\Pi_i$ and $\Pi_j$ with associated goal regions $G_i$ and~$G_j$, respectively.
Now, let $s_i$ and $s_j$ be states on $\Pi_i$ and $\Pi_j$, respectively, such that the timestamp associated with $s_j$ is $\delta_t$ time after the one associated with $s_i$. Furthermore, assume that the path between $s_i$ and $s_j$ is collision free. 
%
Now, assume that the robot is executing path $\Pi_i$ (targeting a goal state in $G_i$) and the perception system updates the goal state to be reached as $g_j \in G_j$. 
If the robot did not yet reach $s_i$ then it can reach $g_j$ by
(i)~continuing to follow $\Pi_i$ until $s_i$ is reached, 
(ii)~move to $s_j$ on $\Pi_j$ and 
(iii)~use $\Pi_j$ to reach $g_j$.
%
We term the process we just described of moving from one root path to another as ``latching'' on to a new root path.

Let $s_{\Pi_i, t}$ be the state that is $t$ time from \Shome on path $\Pi_i$.
If a collision-free path existed from $s_{\Pi_i, \Trc}$ to $s_{\Pi_j, \Trc+\delta_t}$ for every $i,j$ then we could latch on from any root path to any other root path. Moreover, following Assumption~\ref{assum:4}, $\Trc$ is the last time that the perception could update the goal location so no other replanning would be required.
%
Unfortunately, this may not be the case.
%
Thus, for every root path $\Pi_i$, we consider $s_{\Pi_i, \Trc}$ and check if we can latch on to all other root paths. 
If this can't be done, then we can tr

considering the last replanning state $s_{\Pi_i, \Trc}$ (namely, the state that is $t=\Trc$ time from \Shome). For every other root path $\Pi_j$, we test if the path connecting $s_{\Pi_i, \Trc}$ to $s_{\Pi_j, \Trc + \delta_t}$ (the state on $\Pi_j$ that is $\Trc+\delta_t$ away from \Shome) is collision free. 

In the straw man algorithm this was obtained by recursively computing a path for the replanning states along all the previously-computed paths.
%
Here, we attempt to re-use the previously-computed paths as much as possible by ``latching'' onto them.




More formally, for each root path~$\Pi_i$, we start by considering the last replanning state $s_{\Pi_i, \Trc}$ (namely, the state that is $t=\Trc$ time from \Shome). For every other root path $\Pi_j$, we test if the path connecting $s_{\Pi_i, \Trc}$ to $s_{\Pi_j, \Trc + \delta_t}$ (the state on $\Pi_j$ that is $\Trc+\delta_t$ away from \Shome) is collision free. 
If this is the case we know that any goal state in $G_j$ can be
}


\subsection{Evaluation}
\label{sec:eval}

We evaluated our algorithm in simulation and on a real robot. The conveyor speed that we used for all of our results is $0.2m/s$. We used Willow Garage's PR2 robot in our experiments using its 7-DOF arm. The additional time dimension makes the planning problem eight dimensional.


\begin{table*}[t]
\centering
\begin{tabular}{|c||c||c|c|c||c|c|c||c|c|c|}
\hline
   & \textbf{Our Method} 
   & \multicolumn{3}{c|}{\wastar}
   & \multicolumn{3}{c|}{\textsf{E-Graph}}
   & \multicolumn{3}{c|}{\rrt}
   \\ \hline
   & $T_{b}$ = 0.2 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   \\ \hline
Pickup success rate [\%]                   
& 92.0 & 0.0 & 0.0 & 18.0 & 0.0 & 0.0 & 80.0 & 0.0 & 0.0 & 18.0 \\ \hline
Planning success rate [\%]                  
& 94.7 & 4.0 & 17.0 & 19.0 & 31.0 & 80.0 & 90.0 & 12.0 & 9.0 & 13.0 \\ \hline
Planning time [s]
& 0.069 & 0.433 & 0.628 & 0.824 & 0.283 & 0.419 & 0.311 & 0.279 & 0.252& 0.197\\ \hline
Planning episodes per pickup 
& 3 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 \\ \hline
Path cost [s]                         & 10.11        & 8.19          & 8.28          & 7.60          & 8.54          & 8.22          & 7.90          & 9.68          & 8.96          & 8.04          \\ \hline
%Expansions                        & 17.57        & 315.50        & 392.81        & 495.78        & 188.24        & 202.63        & 137.02        & 146.54        & 104.75        & 78.64         \\ \hline
\end{tabular}
\caption{\CaptionTextSize Simulation results. Here $T_b$ denotes the (possibly arbitrary) timebound that the algorithm uses. Note that for our method $T_b = \Tbound$ is the time bound that the algorithm is ensured to compute a plan.}
\label{tab:sim_results}
\end{table*}



\subsubsection{Experimental setup}
\paragraph{Perception system~$\calP$}
In order to pick a known object $o$ moving object along the conveyor~$\calB$, we need a method to estimate its 3-DoF pose at various locations across~$\calB$. Thus, we created an ICP-based pose estimation strategy that uses the known 3D model of~$o$ and the initial pose estimate obtained after pre-processing the input point cloud. 
%
The following process is performed at every frame obtained by $\calP$. 
We start by removing all points that lie outside $\calB$ which yields only points corresponding to the object of interest. 
This is followed by statistical outlier removal in order to remove any points that have been left in scene after the first step. 
In the final step, we compute the mean of the points left and use that as an initial translation estimate for ICP. 
The rotation estimate is applied in a way that it places the object parallel to its direction of motion along the conveyor. The object's 3D model is transformed with this initial estimate, creating a point cloud and ICP refinement is performed between this point and the pre-processed observed point cloud. The resulting transform is concatenated with the initial estimate to return the final predicted pose. For speeding up ICP, we downsample all point clouds to a leaf size of 15mm.

\paragraph{Sense-plan-act cycle}
We follow the classical sense-plan-act cycle as depicted in Fig.~\ref{fig:tl}.
%
Specifically, 
$\calP$ captures an image (point cloud) of the object~$o$ at time~$t_{\textrm{img}}$
followed by a period of duration $T_{\textrm{perception}}$ in which the $\calP$ estimates the pose of $o$.
At time $t_{\textrm{msg}} = t_{\textrm{img}} + T_{\textrm{perception}}$, planning starts for a period of $T_{\textrm{planning}}$ which is guaranteed to be less than~$\Tbound$.
Thus, at $t_{\textrm{plan}} = t_{\textrm{msg}} + T_{\rm planning}$ the planner waits for an additional duration of $T_{\rm wait} = \Tbound - T_{\rm planning}$.
Finally, at~$t_{\textrm{exec}} = t_{\textrm{plan}} + T_{\rm wait}$, the robot starts executing the plan. Note that the goal $g$ that the planner plans for is not for the object pose at $t_{\textrm{img}}$ but its forward projection in time to $t_{\textrm{exec}}$ to account for $T_{\textrm{perception}}$ and $T_{\textrm{bound}}$.
While executing the plan, if we obtain an updated pose estimate, the execution is preempted and the cycle repeats.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/timeline2.pdf}
    \caption{\CaptionTextSize Timeline of the sense-plan-act cycle.}
    \label{fig:tl}
    \vspace{-5mm}
\end{figure}

\paragraph{Goal region specification}
To define the set of all goal poses~$\Gfull$, we need to detail our system setup, depicted in Fig.~\ref{fig:pe}.
The conveyor belt $\calB$ moves along the $x$-axis from left to right.
We pick a fixed $x$-value termed~\Xexec, such that when the incoming $o$ reaches \Xexec as per the perception information, at that point we start execution.

% We assume that there exists a specific $x$-value, termed~\Xexec, such that when the robot start's executing for the first time, the object's  $x$-value will be in the range $[\Xexec-\varepsilon_\calP, \Xexec]$.
%any forward propagation to $t_{\rm exec}$ time an initial pose estimate is given for an object, its $x$-value is in the range $[\Xexec-\varepsilon_\calP, \Xexec]$.
%
Recall that a pose of an object~$o$ is a three dimensional point~$(x,y,\theta)$ corresponding to the $(x,y)$ location of~$o$ and to its orientation (yaw angle) along $\calB$.
%
$\Gfull$ contains a fine discretization of all possible $y$ and $\theta$ values and $x$ values in  $[\Xexec-\varepsilon_\calP, \Xexec+\varepsilon_\calP]$.
We select $\Gfull$ such that $\varepsilon_\calP = 0.5$ cm, dimension along $y$-axis is 20 cm. The discretization in $x,y$ and $\theta$ is 1.0 cm and 10 degrees respectively.

In the example depicted in Fig.~\ref{fig:pe}, the thick and the thin solid rectangles show the ground truth and estimated poses, respectively at two time instances in the life time of the object.
%
The first plan is generated for the pose shown at $x_{\textrm{exec}}$. During execution, the robot receives an improved estimate and has to replan for it. At this point we back project this new estimate in time using the known speed of the conveyor and the time duration between the two estimates. This back-projected pose (shown as the dotted rectangle) is then picked as the new goal for replanning. Recall that under the assumption ~\ref{assum:3} the back projected pose will always lie inside \Gfull.
%
%Recall that we assume that the new estimate will be within the $\pm \varepsilon_\calP$ window around \Xexec, it will always lie within \Gfull. 
%As \Gfull contains all $y$ and $\theta$ values (up to our discretization), our system can handle the errors along those dimensions.



\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figs/pose_error2.pdf}
    \caption{\CaptionTextSize A depiction of \Gfull-specification on a conveyor belt (overhead view) and perception noise handling. 
    }
    \label{fig:pe}
%        \vspace{-5mm}
\end{figure}

\subsubsection{Results}

Before we report on our system-level results comparing our work with alternative implementation (namely, results from the query stage), we mention that the preprocessing stage took roughly 3.5 hours and the memory footprint following this stage is less than 20 MB.
%
This backs up our intuition that the domain allows for efficient compression of a massive amount of paths in a reasonable amount of preprocessing time. In all experiments, we used $\Trc =3.5$ seconds




\paragraph{Real-robot experiments}
\label{sec:robot_results}
To show the necessity of real-time replanning we performed three types of experiments, 
(E1)~replanning with multiple pose estimates, 
(E2)~first-pose planning from the first object pose estimate 
%(received at approximately 1.5m from the robot's base) 
(E3)~best-pose planning from the late (more accurate) pose estimate. 
%
For each set of experiments, we determined the pickup success rate to grasp the moving object (sugar box) off~$\calB$. In addition, we report on $\calP$'s success rate by observing the overlap between the point cloud of the object's 3D model transformed by the predicted pose (that was used for planning) and the filtered input point cloud containing points belonging to the object. 
A high (resp. low) overlap results in an accurate (resp. inaccurate) pose estimate. 
%
We use the same strategy to determine a range for which $\calP$'s estimates are accurate and use it for best-pose planning. 
%This range is found to be 1.05m to 0.95m from the robot's base on the conveyor. 
Further, for each method, we determine the pickup success rate given that the perception was or wasn't accurate. 

The experimental results are shown in Table \ref{tab:robot_results}. Our method achieves the highest overall pickup success rate on the robot, indicating the importance of continuous replanning with multiple pose estimates. 
First-pose planning has the least overall success rate due to inaccuracy of pose estimates when the object is far from the robot's camera. 
Best-pose planning performs better overall than the first pose strategy, since it uses accurate pose estimates, received when the object is close to the robot. However it often fails even when perception is accurate, due to the limited time remaining to grasp object when it's closer to the robot. 
%We can observe this from the method's lowest pickup success rate among the 3 methods when perception = 1 in \ref{tab:robot_results}.






\begin{table}[t]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
                    & \begin{tabular}[c]{@{}c@{}}Overall \\ Pickup\end{tabular}
                    & Perception 
                    & \begin{tabular}[c]{@{}c@{}}Pickup \\ (Perception = 1*)\end{tabular} 
                    & \begin{tabular}[c]{@{}c@{}}Pickup \\ (Perception = 0*)\end{tabular} \\ \hline
E1          & \textbf{69.23}                                                                      & \textbf{42.31}                   & \textbf{83.33}                                                                             & \textbf{57.14}                                                                             \\ \hline
E2 & 16.00                                                                      & 24.00                   & 66.67                                                                             & 0.00                                                                              \\ \hline
E3   & 34.61                                                                      & 34.62                   & 55.56                                                                             & 23.53                                                                             \\ \hline
\end{tabular}
\caption{\CaptionTextSize Real-robot experiments. Success rate for the three experiments (E1---our method, E2---First-pose planning and E3---Best-pose planning). Perception = 1 is accurate estimate, Perception = 0 is inaccurate}
\label{tab:robot_results}
\vspace{-5mm}
\end{table}

\ignore{
\begin{table*}[t]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
                    & \begin{tabular}[c]{@{}c@{}}Pickup success rate [\%]\\ (Overall)\end{tabular} & Perception success rate [\%]& \begin{tabular}[c]{@{}c@{}}Pickup success rate [\%]\\ (Perception = 1*)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pickup success rate [\%]\\ (Perception = 0*)\end{tabular} \\ \hline
Our method (E1)          & \textbf{9.23}                                                                      & \textbf{42.31}                   & \textbf{83.33}                                                                             & \textbf{57.14}                                                                             \\ \hline
First-pose planning (E2) & 16.00                                                                      & 24.00                   & 66.67                                                                             & 0.00                                                                              \\ \hline
Best-pose planing (E3)   & 34.61                                                                      & 34.62                   & 55.56                                                                             & 23.53                                                                             \\ \hline
\end{tabular}
\caption{Real-robot Experiments}
\label{tab:robot_results}
\end{table*}
}

\paragraph{Simulation experiments}






We simulated the real world scenario to evaluate our method against other baselines. We compared our method with \wastar, \textsc{E-graphs} and \rrt. 
For \wastar and \textsc{E-graphs} we use the same graph representation as our method. 
For \textsc{E-graphs} we precompute five paths to randomly-selected  goals in \Gfull. 
We adapt the \rrt algorithm to account for the under-defined goals. To do so, we sample pre-grasp poses along the conveyor 
%in the reachable workspace with a fine discretization 
and compute IK solutions for them to get a set of goal configurations for goal biasing. 
When a newly-added node falls within a threshold distance from the object, we use the same dynamic primitive that we use in the search-based methods to add the final grasping maneuver. If the primitive succeeds, we return success. We also allow wait actions at the pre-grasp locations.

For any planner to be used in our system, we need to endow it with a (possibly arbitrary) planning time bound to compute the future location of the object from which the new execution will start (see Fig.~\ref{fig:tl}). 
%
If the planner fails to generate the plan within this time, then the robot misses the object for that cycle and such cases are recorded as failures. 
%
We label a run as a ``Pickup success" if the planner successfully replans once after the object crosses the 1.0m mark. The mark is the mean of accurate perception range that was determined experimentally and used in the robot experiments as described in Section \ref{sec:robot_results}.
%
The key takeaway from our experiments (Table~\ref{tab:sim_results}) is that having a known time bound on the query time is vital to the success of the conveyor pickup task.

Our method shows the highest pickup success rate, planning success rate (success rate over all planning queries) and an order of magnitude lower planning times compared to the other methods. 
The planning success rate being lower than 100\% can be attributed to the fact that some goals were unreachable during the runs. 
%
We tested the other methods with several different time bounds. Besides our approach \textsc{E-graphs} perform significantly well. \rrt suffers from the fact that the goal is under-defined and sampling based planners typically require a goal bias in the configuration space. Another important highlight of the experiments is the number of planning cycles over the lifetime of an object. While the other approaches could replan at most twice, our method was able to replan thrice due to fast planning times.


\ignore{
\begin{table*}[t]
\centering
\begin{tabular}{|c||c||c|c|c||c|c|c||c|c|c|}
\hline
   & \textbf{Our Method} 
   & \multicolumn{3}{c|}{\wastar}
   & \multicolumn{3}{c|}{\textsf{E-Graph}}
   & \multicolumn{3}{c|}{\rrt}
   \\ \hline
   & $T_{b}$ = 0.2 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   & $T_{b}$ = 0.5 & $T_{b}$ = 1.0 & $T_{b}$ = 2.0 
   \\ \hline
Pickup success rate [\%]                   & 92.00     & 0.00      & 0.00     & 18.00      & 0.00        & 0.00       & 80.00       & 0.00       & 0.00       & 18.00      \\ \hline
Planning success rate [\%]                  & 94.67     & 4.00       & 17.00      & 19.00       & 31.00    & 80.00       & 90.00      & 12.00      & 9.00       & 13.00       \\ \hline
Planning time [s]                    & 0.0689       & 0.4329        & 0.6284        & 0.8241        & 0.2830        & 0.4194        & 0.3112        & 0.2718        & 0.2518        & 0.1966        \\ \hline
Planning episodes per pickup & 3            & 2             & 2             & 2             & 2             & 2             & 2             & 2             & 2             & 2             \\ \hline
Path cost [s]                         & 10.11        & 8.19          & 8.28          & 7.60          & 8.54          & 8.22          & 7.90          & 9.68          & 8.96          & 8.04          \\ \hline
%Expansions                        & 17.57        & 315.50        & 392.81        & 495.78        & 188.24        & 202.63        & 137.02        & 146.54        & 104.75        & 78.64         \\ \hline
\end{tabular}
\caption{Simulation results. Here $T_b$ denotes the (possibly arbitrary) timbound that the algorithm uses. Note that for our method $T_b = \Tbound$ is the time bound that the algorithm is ensured to compute a plan.}
\label{tab:sim_results}
\end{table*}
}


% \begin{table}[t]
% % \centering
%     \resizebox{0.5\textwidth}{!}{ 
%         \begin{tabular}{ l | c c c c}
%           & \algname{WA*} & \algname{E-graphs} & \algname{\rrt} & \textbf{Our method} \\
%          \hline
%          Planning time [ms]& - (-) & - (-) & - (-) & \textbf{- (-)}\\
%          Success rate [$\%$]& - & - & - & \textbf{-}\\
%          No. of replans& - & - & - & \textbf{-}\\
%         \end{tabular}
%     }
%     \caption{Planning times, success rate and number of replanning queries averaged over 100 simulated runs of an object moving on $\calB$.}
%     \label{tab:stats_query}
% \end{table}


\ignore{
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figs/preprocess.png}
    \caption{Preprocessing}
    \label{fig:preprocessing}
\end{figure}
}
%\subsubsection{Algorithm Building Blocks}
%\subsubsection{Preprocessing Phase}
%\subsubsection{Query Phase}
%\subsection{Theoretical Analysis}
%\subsubsection{Completeness}
%\subsubsection{Time Complexity of Query Phase}
%\subsection{Experimental Results}
%\subsubsection{Experimental Setup}
%\subsubsection{Real Robot Experiments}
%\subsubsection{Simulation Experiments}

\section{Proposed Work}
In the remainder of this thesis we aim to broaden the applicability of CTMP to semi-structured environments. So far we have developed algorithms for fully static environments with known obstacles and a dynamic environment with a known dynamics model i.e the geometric and motion models of the object in the scene were known. Next, we want to consider environments in which the geometric models will be known but the location or motion model of objects in the scene could be unknown apriori. At the same time, we aim to provide strong guarantees on the planning times.

\subsection{CTMP with Movable Objects in Static Environments}
Environments like homes are considered semi-structured; Consider a kitchen scenario where most of the obstacles are static like kitchen counters, cabinets, stove etc. However, the movable (non-static) items e.g the daily consumbles like a sugar box, a jam jar or the kitchen utensils may be found at random locations specially when the kitchen is used by humans alongside.
The constraint of the environments being static is limiting for the robots to work in such semi-structured environments. We assume that the geometric models of all movable objects are known apriori and we have access to a perfect perception system that will give us accurate poses of all the objects in the robot's workspace.

\subsubsection{Problem Definition}
We build on the problem definition from section~\ref{sec:pdef}. Given a start state~$\sStart$, a goal region~$G \in \calC$, a 2D workspace~$\calW$ and a (small) set of known objects~$\calO = \{o_1, o_2, ..., o_n\}$ that can occupy any location~($x,y,yaw$) in the workspace i.e. $loc(o_i) \in \calW$ where~$o_i \in \calO$, for any query goal~$\sGoal$, we need to find a path from~$\sStart$ to~$\sGoal$ in bounded time.

\subsubsection{Increased Complexity of the Problem}
Let us consider a naive approach to analyse the complexity of this problem. Say each object~$o_i$, can occupy~$n$ different locations in~$\calW$. For a single object, if we were to utilise our proposed algorithm in chapter~\ref{ctmp1}, it will require preprocessing data for all~$n$ locations and then at the query time, we can possibly use a hash table lookup to map a given object location to the corresponding preprocessed data. As the number of objects increase, the number of possible joint locations for all the objects will grow exponentially with the number of objects. This approach will quickly make the problem intractable memory and computation-wise.

\subsubsection{Key Idea}
Our key idea is that 

\subsection{CTMP for Conveyor Task with Multiple Arms}
\subsection{CTMP for Motion Planning on Constrained Manifolds}
\subsection{Timeline}




\newpage

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
